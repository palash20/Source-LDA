A submarine (or simply sub) is a watercraft capable of independent operation underwater. It differs from a submersible, which has more limited underwater capability. The term most commonly refers to a large, crewed vessel. It is also sometimes used historically or colloquially to refer to remotely operated vehicles and robots, as well as medium-sized or smaller vessels, such as the midget submarine and the wet sub. The noun submarine evolved as a shortened form of submarine boat; by naval tradition, submarines are usually referred to as "boats" rather than as "ships", regardless of their size.Although experimental submarines had been built before, submarine design took off during the 19th century, and they were adopted by several navies. Submarines were first widely used during World War I (1914–1918), and now figure in many navies large and small. Military usage includes attacking enemy surface ships (merchant and military), attacking other submarines, aircraft carrier protection, blockade running, ballistic missile submarines as part of a nuclear strike force, reconnaissance, conventional land attack (for example using a cruise missile), and covert insertion of special forces. Civilian uses for submarines include marine science, salvage, exploration and facility inspection and maintenance. Submarines can also be modified to perform more specialized functions such as search-and-rescue missions or undersea cable repair. Submarines are also used in tourism, and for undersea archaeology.Most large submarines consist of a cylindrical body with hemispherical (or conical) ends and a vertical structure, usually located amidships, which houses communications and sensing devices as well as periscopes. In modern submarines, this structure is the "sail" in American usage, and "fin" in European usage. A "conning tower" was a feature of earlier designs: a separate pressure hull above the main body of the boat that allowed the use of shorter periscopes. There is a propeller (or pump jet) at the rear, and various hydrodynamic control fins. Smaller, deep-diving and specialty submarines may deviate significantly from this traditional layout. Submarines use diving planes and also change the amount of water and air in ballast tanks to change buoyancy for submerging and surfacing.Submarines have one of the widest ranges of types and capabilities of any vessel. They range from small autonomous examples and one- or two-person vessels that operate for a few hours, to vessels that can remain submerged for six months—such as the Russian Typhoon class, the biggest submarines ever built. Submarines can work at greater depths than are survivable or practical for human divers. Modern deep-diving submarines derive from the bathyscaphe, which in turn evolved from the diving bell. History  Early submersibles According to a report in Opusculum Taisnieri published in 1562:Two Greeks submerged and surfaced in the river Tagus near the City of Toledo several times in the presence of The Holy Roman Emperor Charles V, without getting wet and with the flame they carried in their hands still alight.In 1578, the English mathematician William Bourne recorded in his book Inventions or Devises one of the first plans for an underwater navigation vehicle. A few years later the Scottish mathematician and theologian John Napier wrote in his Secret Inventions (1596) the following: "These inventions besides devises of sayling under water with divers, other devises and strategems for harming of the enemyes by the Grace of God and worke of expert Craftsmen I hope to perform." It's unclear whether he ever carried out his idea.The first submersible of whose construction there exists reliable information was designed and built in 1620 by Cornelis Drebbel, a Dutchman in the service of James I of England. It was propelled by means of oars. 18th century submarines By the mid-18th century, over a dozen patents for submarines/submersible boats had been granted in England. In 1747, Nathaniel Symons patented and built the first known working example of the use of a ballast tank for submersion. His design used leather bags that could fill with water to submerge the craft. A mechanism was used to twist the water out of the bags and cause the boat to resurface. In 1749, the Gentlemen's Magazine reported that a similar design had initially been proposed by Giovanni Borelli in 1680. By this point of development, further improvement in design necessarily stagnated for over a century, until new industrial technologies for propulsion and stability could be applied.The first military submarine was the Turtle (1775), a hand-powered acorn-shaped device designed by the American David Bushnell to accommodate a single person. It was the first verified submarine capable of independent underwater operation and movement, and the first to use screws for propulsion. 19th century submarines In 1800, France built a human-powered submarine designed by American Robert Fulton, the Nautilus. The French eventually gave up on the experiment in 1804, as did the British when they later considered Fulton's submarine design.In 1864, late in the American Civil War, the Confederate navy's H. L. Hunley became the first military submarine to sink an enemy vessel, the Union sloop-of-war USS Housatonic. In the aftermath of its successful attack against the ship, the Hunley also sank, possibly because it was too close to its own exploding torpedo.In 1866, the first submarine that successfully dived made a controlled underwater cruise and emerged to the surface again all by its own was the Sub Marine Explorer of the German American Julius H. Kroehl (in German, Kröhl), which incorporated many technologies that are still essential to modern submarines. Mechanical power The first submarine not relying on human power for propulsion was the French Plongeur (Diver), launched in 1863, which used compressed air at 180 psi (1241 kPa). The first air–independent and combustion–powered submarine was Ictineo II, designed by the Spanish intellectual, artist and engineer Narcís Monturiol, launched in Barcelona in 1864.The submarine became a potentially viable weapon with the development of the Whitehead torpedo, designed in 1866 by British engineer Robert Whitehead, the first practical self-propelled or 'locomotive' torpedo. The spar torpedo that had been developed earlier by the Confederate navy was considered to be impracticable, as it was believed to have sunk both its intended target, and probably H. L. Hunley, the submarine that deployed it.Discussions between the English clergyman and inventor George Garrett and the Swedish industrialist Thorsten Nordenfelt led to the first practical steam-powered submarines, armed with torpedoes and ready for military use. The first was Nordenfelt I, a 56-tonne, 19.5-metre (64 ft) vessel similar to Garret's ill-fated Resurgam (1879), with a range of 240 kilometres (130 nmi; 150 mi), armed with a single torpedo, in 1885.A reliable means of propulsion for the submerged vessel was only made possible in the 1880s with the advent of the necessary electric battery technology. The first electrically powered boats were built by Isaac Peral y Caballero in Spain, Dupuy de Lôme and Gustave Zédé in France, and James Franklin Waddington in England. 20th century submarines Submarines were not put into service for any widespread or routine use by navies until the early 1900s. This era marked a pivotal time in submarine development, and several important technologies appeared. A number of nations built and used submarines. Diesel electric propulsion became the dominant power system and equipment such as the periscope became standardized. Countries conducted many experiments on effective tactics and weapons for submarines, which led to their large impact in World War I.The Irish inventor John Philip Holland built a model submarine in 1876 and a full-scale version in 1878, which were followed by a number of unsuccessful ones. In 1896 he designed the Holland Type VI submarine, which used internal combustion engine power on the surface and electric battery power underwater. Launched on 17 May 1897 at Navy Lt. Lewis Nixon's Crescent Shipyard in Elizabeth, New Jersey, the Holland VI was purchased by the United States Navy on 11 April 1900, becoming the Navy's first commissioned submarine, christened USS Holland.Commissioned in June 1900, the French steam and electric Narval employed the now typical double-hull design, with a pressure hull inside the outer shell. These 200-ton ships had a range of over 100 miles (160 km) underwater. The French submarine Aigrette in 1904 further improved the concept by using a diesel rather than a gasoline engine for surface power. Large numbers of these submarines were built, with seventy-six completed before 1914.The Royal Navy commissioned five Holland-class submarines from Vickers, Barrow-in-Furness, under licence from the Holland Torpedo Boat Company from 1901 to 1903. Construction of the boats took longer than anticipated, with the first only ready for a diving trial at sea on 6 April 1902. Although the design had been purchased entire from the US company, the actual design used was an untested improvement to the original Holland design using a new 180 hp petrol engine.These types of submarines were first used during the Russo-Japanese War of 1904–05. Due to the blockade at Port Arthur, the Russians sent their submarines to Vladivostok, where by 1 January 1905 there were seven boats, enough to create the world's first "operational submarine fleet". The new submarine fleet began patrols on 14 February, usually lasting for about 24 hours each. The first confrontation with Japanese warships occurred on 29 April 1905 when the Russian submarine Som was fired upon by Japanese torpedo boats, but then withdrew. World War I Military submarines first made a significant impact in World War I. Forces such as the U-boats of Germany saw action in the First Battle of the Atlantic, and were responsible for sinking RMS Lusitania, which was sunk as a result of unrestricted submarine warfare and is often cited among the reasons for the entry of the United States into the war.At the outbreak of war Germany had only 20 submarines immediately available for combat, although these included vessels of the diesel-engined U-19 class with the range (5,000 miles) and speed (eight knots) to operate effectively around the entire British coast. By contrast the Royal Navy had a total of 74 submarines, though of mixed effectiveness. In August 1914, a flotilla of ten U-boats sailed from their base in Heligoland to attack Royal Navy warships in the North Sea in the first submarine war patrol in history.The U-boats' ability to function as practical war machines relied on new tactics, their numbers, and submarine technologies such as combination diesel-electric power system developed in the preceding years. More submersibles than true submarines, U-boats operated primarily on the surface using regular engines, submerging occasionally to attack under battery power. They were roughly triangular in cross-section, with a distinct keel to control rolling while surfaced, and a distinct bow. During World War I more than 5,000 Allied ships were sunk by U-boats. World War II During World War II, Germany used submarines to devastating effect in the Battle of the Atlantic, where it attempted to cut Britain's supply routes by sinking more merchant ships than Britain could replace. (Shipping was vital to supply Britain's population with food, industry with raw material, and armed forces with fuel and armaments.) While U-boats destroyed a significant number of ships, the strategy ultimately failed. Although the U-boats had been updated in the interwar years, the major innovation was improved communications, encrypted using the famous Enigma cipher machine. This allowed for mass-attack naval tactics (Rudeltaktik, commonly known as "wolfpack"), but was also ultimately the U-boats' downfall. By the end of the war, almost 3,000 Allied ships (175 warships, 2,825 merchantmen) had been sunk by U-boats. Although successful early in the war, ultimately the U-boat fleet suffered a casualty rate of 73%, almost all fatalities.The Imperial Japanese Navy operated the most varied fleet of submarines of any navy, including Kaiten crewed torpedoes, midget submarines (Type A Ko-hyoteki and Kairyu classes), medium-range submarines, purpose-built supply submarines and long-range fleet submarines. They also had submarines with the highest submerged speeds during World War II (I-201-class submarines) and submarines that could carry multiple aircraft (I-400-class submarines). They were also equipped with one of the most advanced torpedoes of the conflict, the oxygen-propelled Type 95. Nevertheless, despite their technical prowess, Japan chose to utilize its submarines for fleet warfare, and consequently were relatively unsuccessful, as warships were fast, maneuverable and well-defended compared to merchant ships.The submarine force was the most effective anti-ship weapon in the American arsenal. Submarines, though only about 2 percent of the U.S. Navy, destroyed over 30 percent of the Japanese Navy, including 8 aircraft carriers, 1 battleship and 11 cruisers. US submarines also destroyed over 60 percent of the Japanese merchant fleet, crippling Japan's ability to supply its military forces and industrial war effort. Allied submarines in the Pacific War destroyed more Japanese shipping than all other weapons combined. This feat was considerably aided by the Imperial Japanese Navy's failure to provide adequate escort forces for the nation's merchant fleet.During World War II, 314 submarines served in the US Navy, of which nearly 260 were deployed to the Pacific. When the Japanese attacked Hawaii in December 1941, 111 boats were in commission; 203 submarines from the Gato, Balao, and Tench classes were commissioned during the war. During the war, 52 US submarines were lost to all causes, with 48 directly due to hostilities. US submarines sank 1,560 enemy vessels, a total tonnage of 5.3 million tons (55% of the total sunk).The Royal Navy Submarine Service was used primarily in the classic Axis blockade. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2 million tons of enemy shipping and 57 major warships, the latter including 35 submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when HMS Venturer engaged the U864; the Venturer crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost, the majority, 42, in the Mediterranean. Cold-War military models The first launch of a cruise missile (SSM-N-8 Regulus) from a submarine occurred in July 1953, from the deck of USS Tunny, a World War II fleet boat modified to carry the missile with a nuclear warhead. Tunny and its sister boat, Barbero, were the United States' first nuclear deterrent patrol submarines. In the 1950s, nuclear power partially replaced diesel-electric propulsion. Equipment was also developed to extract oxygen from sea water. These two innovations gave submarines the ability to remain submerged for weeks or months. Most of the naval submarines built since that time in the US, the Soviet Union/Russian Federation, Britain, and France have been powered by nuclear reactors.In 1959–1960, the first ballistic missile submarines were put into service by both the United States (George Washington class) and the Soviet Union (Golf class) as part of the Cold War nuclear deterrent strategy.During the Cold War, the US and the Soviet Union maintained large submarine fleets that engaged in cat-and-mouse games. The Soviet Union lost at least four submarines during this period: K-129 was lost in 1968 (a part of which the CIA retrieved from the ocean floor with the Howard Hughes-designed ship Glomar Explorer), K-8 in 1970, K-219 in 1986, and Komsomolets in 1989 (which held a depth record among military submarines—1000 m). Many other Soviet subs, such as K-19 (the first Soviet nuclear submarine, and the first Soviet sub to reach the North Pole) were badly damaged by fire or radiation leaks. The US lost two nuclear submarines during this time: USS Thresher due to equipment failure during a test dive while at its operational limit, and USS Scorpion due to unknown causes.During the Indo-Pakistani War of 1971, the Pakistan Navy's Hangor sank the Indian frigate INS Khukri. This was the first sinking by a submarine since World War II. During the same war, the Ghazi, a Tench-class submarine on loan to Pakistan from the US, was sunk. It was the first submarine combat loss since World War II. In 1982 during the Falklands War, the Argentine cruiser General Belgrano was sunk by the British submarine HMS Conqueror, the first sinking by a nuclear-powered submarine in war. 21st century submarines  Usage  Military Before and during World War II, the primary role of the submarine was anti-surface ship warfare. Submarines would attack either on the surface, using deck guns or submerged, using torpedoes. They were particularly effective in sinking Allied transatlantic shipping in both World Wars, and in disrupting Japanese supply routes and naval operations in the Pacific in World War II.Mine-laying submarines were developed in the early part of the 20th century. The facility was used in both World Wars. Submarines were also used for inserting and removing covert agents and military forces, for intelligence gathering, and to rescue aircrew during air attacks on islands, where the airmen would be told of safe places to crash-land so the submarines could rescue them. Submarines could carry cargo through hostile waters or act as supply vessels for other submarines.Submarines could usually locate and attack other submarines only on the surface, although HMS Venturer managed to sink U-864 with a four torpedo spread while both were submerged. The British developed a specialized anti-submarine submarine in WWI, the R class. After WWII, with the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to hunt each other effectively.The development of submarine-launched ballistic missile and submarine-launched cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster bombs to nuclear weapons.The primary defense of a submarine lies in its ability to remain concealed in the depths of the ocean. Early submarines could be detected by the sound they made. Water is an excellent conductor of sound (much better than air), and submarines can detect and track comparatively noisy surface ships from long distances. Modern submarines are built with an emphasis on stealth. Advanced propeller designs, extensive sound-reducing insulation, and special machinery help a submarine remain as quiet as ambient ocean noise, making them difficult to detect. It takes specialized technology to find and attack modern submarines.Active sonar uses the reflection of sound emitted from the search equipment to detect submarines. It has been used since WWII by surface ships, submarines and aircraft (via dropped buoys and helicopter "dipping" arrays), but it reveals the emitter's position, and is susceptible to counter-measures.A concealed military submarine is a real threat, and because of its stealth, can force an enemy navy to waste resources searching large areas of ocean and protecting ships against attack. This advantage was vividly demonstrated in the 1982 Falklands War when the British nuclear-powered submarine HMS Conqueror sank the Argentine cruiser General Belgrano. After the sinking the Argentine Navy recognized that they had no effective defense against submarine attack, and the Argentine surface fleet withdrew to port for the remainder of the war, though an Argentine submarine remained at sea. Civilian Although the majority of the world's submarines are military, there are some civilian submarines, which are used for tourism, exploration, oil and gas platform inspections, and pipeline surveys. Some are also used in illegal activities.The Submarine Voyage ride opened at Disneyland in 1959, but although it ran under water it was not a true submarine, as it ran on tracks and was open to the atmosphere. The first tourist submarine was Auguste Piccard, which went into service in 1964 at Expo64. By 1997 there were 45 tourist submarines operating around the world. Submarines with a crush depth in the range of 400–500 feet (120–150 m) are operated in several areas worldwide, typically with bottom depths around 100 to 120 feet (30 to 37 m), with a carrying capacity of 50 to 100 passengers.In a typical operation a surface vessel carries passengers to an offshore operating area and loads them into the submarine. The submarine then visits underwater points of interest such as natural or artificial reef structures. To surface safely without danger of collision the location of the submarine is marked with an air release and movement to the surface is coordinated by an observer in a support craft.A recent development is the deployment of so-called narco submarines by South American drug smugglers to evade law enforcement detection. Although they occasionally deploy true submarines, most are self-propelled semi-submersibles, where a portion of the craft remains above water at all times. In September 2011, Colombian authorities seized a 16-meter-long submersible that could hold a crew of 5, costing about $2 million. The vessel belonged to FARC rebels and had the capacity to carry at least 7 tonnes of drugs. Polar operations 1903 – Simon Lake submarine Protector surfaced through ice off Newport, Rhode Island.1930 – USS O-12 operated under ice near Spitsbergen.1937 – Soviet submarine Krasnogvardeyets operated under ice in the Denmark Strait.1941–45 – German U-boats operated under ice from the Barents Sea to the Laptev Sea.1946 – USS Atule used upward-beamed fathometer in Operation Nanook in the Davis Strait.1946–47 – USS Sennet used under-ice sonar in Operation High Jump in the Antarctic.1947 – USS Boarfish used upward-beamed echo sounder under pack ice in the Chukchi Sea.1948 – USS Carp developed techniques for making vertical ascents and descents through polynyas in the Chukchi Sea.1952 – USS Redfish used an expanded upward-beamed sounder array in the Beaufort Sea.1957 – USS Nautilus reached 87 degrees north near Spitsbergen.3 August 1958 – Nautilus used an inertial navigation system to reach the North Pole.17 March 1959 – USS Skate surfaced through the ice at the north pole.1960 – USS Sargo transited 900 miles (1,400 km) under ice over the shallow (125 to 180 feet or 38 to 55 metres deep) Bering-Chukchi shelf.1960 – USS Seadragon transited the Northwest Passage under ice.1962 – Soviet November-class submarine K-3 Leninsky Komsomol reached the north pole.1970 – USS Queenfish carried out an extensive undersea mapping survey of the Siberian continental shelf.1971 – HMS Dreadnought reached the North Pole.USS Gurnard conducted three Polar Exercises: 1976 (with US actor Charlton Heston aboard); 1984 joint operations with USS Pintado; and 1990 joint exercises with USS Seahorse.6 May 1986 – USS Ray, USS Archerfish and USS Hawkbill meet and surface together at the Geographic North Pole. First three-submarine surfacing at the Pole.19 May 1987 – HMS Superb joined USS Billfish and USS Sea Devil at the North Pole.March 2007 – USS Alexandria participated in the Joint US Navy/Royal Navy Ice Exercise 2007 (ICEX-2007) in the Arctic Ocean with the Trafalgar-class submarine HMS Tireless.March 2009 – USS Annapolis took part in Ice Exercise 2009 to test submarine operability and war-fighting capability in Arctic conditions. Technology  Submersion and trimming All surface ships, as well as surfaced submarines, are in a positively buoyant condition, weighing less than the volume of water they would displace if fully submerged. To submerge hydrostatically, a ship must have negative buoyancy, either by increasing its own weight or decreasing its displacement of water. To control their displacement, submarines have ballast tanks, which can hold varying amounts of water and air.For general submersion or surfacing, submarines use the forward and aft tanks, called Main Ballast Tanks (MBT), which are filled with water to submerge or with air to surface. Submerged, MBTs generally remain flooded, which simplifies their design, and on many submarines these tanks are a section of interhull space. For more precise and quick control of depth, submarines use smaller Depth Control Tanks (DCT) – also called hard tanks (due to their ability to withstand higher pressure), or trim tanks. The amount of water in depth control tanks can be controlled to change depth or to maintain a constant depth as outside conditions (chiefly water density) change. Depth control tanks may be located either near the submarine's center of gravity, or separated along the submarine body to prevent affecting trim.When submerged, the water pressure on a submarine's hull can reach 4 MPa (580 psi) for steel submarines and up to 10 MPa (1,500 psi) for titanium submarines like K-278 Komsomolets, while interior pressure remains relatively unchanged. This difference results in hull compression, which decreases displacement. Water density also marginally increases with depth, as the salinity and pressure are higher. This change in density incompletely compensates for hull compression, so buoyancy decreases as depth increases. A submerged submarine is in an unstable equilibrium, having a tendency to either sink or float to the surface. Keeping a constant depth requires continual operation of either the depth control tanks or control surfaces.Submarines in a neutral buoyancy condition are not intrinsically trim-stable. To maintain desired trim, submarines use forward and aft trim tanks. Pumps can move water between the tanks, changing weight distribution and pointing the sub up or down. A similar system is sometimes used to maintain stability.The hydrostatic effect of variable ballast tanks is not the only way to control the submarine underwater. Hydrodynamic maneuvering is done by several surfaces, which can be moved to create hydrodynamic forces when a submarine moves at sufficient speed. The stern planes, located near the propeller and normally horizontal, serve the same purpose as the trim tanks, controlling the trim, and are commonly used, while other control surfaces may not be present on all submarines. The fairwater planes on the sail and/or bow planes on the main body, both also horizontal, are closer to the center of gravity, and are used to control depth with less effect on the trim.When a submarine performs an emergency surfacing, all depth and trim methods are used simultaneously, together with propelling the boat upwards. Such surfacing is very quick, so the sub may even partially jump out of the water, potentially damaging submarine systems. Hull  Overview Modern submarines are cigar-shaped. This design, visible in early submarines is sometimes called a "teardrop hull". It reduces the hydrodynamic drag when submerged, but decreases the sea-keeping capabilities and increases drag while surfaced. Since the limitations of the propulsion systems of early submarines forced them to operate surfaced most of the time, their hull designs were a compromise. Because of the slow submerged speeds of those subs, usually well below 10 kt (18 km/h), the increased drag for underwater travel was acceptable. Late in World War II, when technology allowed faster and longer submerged operation and increased aircraft surveillance forced submarines to stay submerged, hull designs became teardrop shaped again to reduce drag and noise. USS Albacore (AGSS-569) was a unique research submarine that pioneered the American version of the teardrop hull form (sometimes referred to as an "Albacore hull") of modern submarines. On modern military submarines the outer hull is covered with a layer of sound-absorbing rubber, or anechoic plating, to reduce detection.The occupied pressure hulls of deep diving submarines such as DSV Alvin are spherical instead of cylindrical. This allows a more even distribution of stress at the great depth. A titanium frame is usually affixed to the pressure hull, providing attachment for ballast and trim systems, scientific instrumentation, battery packs, syntactic flotation foam, and lighting.A raised tower on top of a submarine accommodates the periscope and electronics masts, which can include radio, radar, electronic warfare, and other systems including the snorkel mast. In many early classes of submarines (see history), the control room, or "conn", was located inside this tower, which was known as the "conning tower". Since then, the conn has been located within the hull of the submarine, and the tower is now called the "sail". The conn is distinct from the "bridge", a small open platform in the top of the sail, used for observation during surface operation."Bathtubs" are related to conning towers but are used on smaller submarines. The bathtub is a metal cylinder surrounding the hatch that prevents waves from breaking directly into the cabin. It is needed because surfaced submarines have limited freeboard, that is, they lie low in the water. Bathtubs help prevent swamping the vessel. Single and double hulls Modern submarines and submersibles, as well as the oldest ones, usually have a single hull. Large submarines generally have an additional hull or hull sections outside. This external hull, which actually forms the shape of submarine, is called the outer hull (casing in the Royal Navy) or light hull, as it does not have to withstand a pressure difference. Inside the outer hull there is a strong hull, or pressure hull, which withstands sea pressure and has normal atmospheric pressure inside.As early as World War I, it was realized that the optimal shape for withstanding pressure conflicted with the optimal shape for seakeeping and minimal drag, and construction difficulties further complicated the problem. This was solved either by a compromise shape, or by using two hulls; internal for holding pressure, and external for optimal shape. Until the end of World War II, most submarines had an additional partial cover on the top, bow and stern, built of thinner metal, which was flooded when submerged. Germany went further with the Type XXI, a general predecessor of modern submarines, in which the pressure hull was fully enclosed inside the light hull, but optimized for submerged navigation, unlike earlier designs that were optimized for surface operation.After World War II, approaches split. The Soviet Union changed its designs, basing them on German developments. All post–World War II heavy Soviet and Russian submarines are built with a double hull structure. American and most other Western submarines switched to a primarily single-hull approach. They still have light hull sections in the bow and stern, which house main ballast tanks and provide a hydrodynamically optimized shape, but the main cylindrical hull section has only a single plating layer. Double hulls are being considered for future submarines in the United States to improve payload capacity, stealth and range. Pressure hull The pressure hull is generally constructed of thick high-strength steel with a complex structure and high strength reserve, and is separated with watertight bulkheads into several compartments. There are also examples of more than two hulls in a submarine, like the Typhoon class, which has two main pressure hulls and three smaller ones for control room, torpedoes and steering gear, with the missile launch system between the main hulls.The dive depth cannot be increased easily. Simply making the hull thicker increases the weight and requires reduction of onboard equipment weight, ultimately resulting in a bathyscaphe. This is acceptable for civilian research submersibles, but not military submarines.WWI submarines had hulls of carbon steel, with a 100-metre (330 ft) maximum depth. During WWII, high-strength alloyed steel was introduced, allowing 200-metre (660 ft) depths. High-strength alloy steel remains the primary material for submarines today, with 250–400-metre (820–1,310 ft) depths, which cannot be exceeded on a military submarine without design compromises. To exceed that limit, a few submarines were built with titanium hulls. Titanium can be stronger than steel, lighter, and is not ferromagnetic, important for stealth. Titanium submarines were built by the Soviet Union, which developed specialized high-strength alloys. It has produced several types of titanium submarines. Titanium alloys allow a major increase in depth, but other systems must be redesigned to cope, so test depth was limited to 1,000 metres (3,300 ft) for the Soviet submarine K-278 Komsomolets, the deepest-diving combat submarine. An Alfa-class submarine may have successfully operated at 1,300 metres (4,300 ft), though continuous operation at such depths would produce excessive stress on many submarine systems. Titanium does not flex as readily as steel, and may become brittle during many dive cycles. Despite its benefits, the high cost of titanium construction led to the abandonment of titanium submarine construction as the Cold War ended. Deep–diving civilian submarines have used thick acrylic pressure hulls.The deepest deep-submergence vehicle (DSV) to date is Trieste. On 5 October 1959, Trieste departed San Diego for Guam aboard the freighter Santa Maria to participate in Project Nekton, a series of very deep dives in the Mariana Trench. On 23 January 1960, Trieste reached the ocean floor in the Challenger Deep (the deepest southern part of the Mariana Trench), carrying Jacques Piccard (son of Auguste) and Lieutenant Don Walsh, USN. This was the first time a vessel, manned or unmanned, had reached the deepest point in the Earth's oceans. The onboard systems indicated a depth of 11,521 metres (37,799 ft), although this was later revised to 10,916 metres (35,814 ft) and more accurate measurements made in 1995 have found the Challenger Deep slightly shallower, at 10,911 metres (35,797 ft).Building a pressure hull is difficult, as it must withstand pressures at its required diving depth. When the hull is perfectly round in cross-section, the pressure is evenly distributed, and causes only hull compression. If the shape is not perfect, the hull is bent, with several points heavily strained. Inevitable minor deviations are resisted by stiffener rings, but even a one-inch (25 mm) deviation from roundness results in over 30 percent decrease of maximal hydrostatic load and consequently dive depth. The hull must therefore be constructed with high precision. All hull parts must be welded without defects, and all joints are checked multiple times with different methods, contributing to the high cost of modern submarines. (For example, each Virginia-class attack submarine costs US$2.6 billion, over US$200,000 per ton of displacement.) Propulsion The first submarines were propelled by humans. The first mechanically driven submarine was the 1863 French Plongeur, which used compressed air for propulsion. Anaerobic propulsion was first employed by the Spanish Ictineo II in 1864, which used a solution of zinc, manganese dioxide, and potassium chlorate to generate sufficient heat to power a steam engine, while also providing oxygen for the crew. A similar system was not employed again until 1940 when the German Navy tested a hydrogen peroxide-based system, the Walter turbine, on the experimental V-80 submarine and later on the naval U-791 and type XVII submarines.Until the advent of nuclear marine propulsion, most 20th-century submarines used batteries for running underwater and gasoline (petrol) or diesel engines on the surface, and for battery recharging. Early submarines used gasoline, but this quickly gave way to kerosene (paraffin), then diesel, because of reduced flammability. Diesel-electric became the standard means of propulsion. The diesel or gasoline engine and the electric motor, separated by clutches, were initially on the same shaft driving the propeller. This allowed the engine to drive the electric motor as a generator to recharge the batteries and also propel the submarine. The clutch between the motor and the engine would be disengaged when the submarine dived, so that the motor could drive the propeller. The motor could have multiple armatures on the shaft, which could be electrically coupled in series for slow speed and in parallel for high speed (these connections were called "group down" and "group up", respectively). Electric  Diesel-electric Early submarines used a direct mechanical connection between the engine and propeller, switching between diesel engines for surface running, and battery-driven electric motors for submerged propulsion.In 1928, the United States Navy's Bureau of Engineering proposed a diesel-electric transmission. Instead of driving the propeller directly while running on the surface, the submarine's diesel drove a generator that could either charge the submarine's batteries or drive the electric motor. This made electric motor speed independent of diesel engine speed, so the diesel could run at an optimum and non-critical speed. One or more diesel engines could be shut down for maintenance while the submarine continued to run on the remaining engine or battery power. The US pioneered this concept in 1929, in the S-class submarines S-3, S-6, and S-7. The first production submarines with this system were the Porpoise-class of the 1930s, and it was used on most subsequent US diesel submarines through the 1960s. No other navy adopted the system before 1945, apart from the Royal Navy's U-class submarines, though some submarines of the Imperial Japanese Navy used separate diesel generators for low speed running.Other advantages of such an arrangement were that a submarine could travel slowly with the engines at full power to recharge the batteries quickly, reducing time on the surface or on snorkel. It was then possible to isolate the noisy diesel engines from the pressure hull, making the submarine quieter. Additionally, diesel-electric transmissions were more compact.During World War II the Germans experimented with the idea of the schnorchel (snorkel) from captured Dutch submarines, but didn't see the need for them until rather late in the war. The schnorchel was a retractable pipe that supplied air to the diesel engines while submerged at periscope depth, allowing the boats to cruise and recharge their batteries while maintaining a degree of stealth. It was far from a perfect solution, however. There were problems with the device's valve sticking shut or closing as it dunked in rough weather; since the system used the entire pressure hull as a buffer, the diesels would instantaneously suck huge volumes of air from the boat's compartments, and the crew often suffered painful ear injuries. Speed was limited to 8 knots (15 km/h), lest the device snap from stress. The schnorchel also had the effect of making the boat essentially noisy and deaf in sonar terms. Finally, Allied radar eventually became sufficiently advanced that the schnorchel mast could be detected beyond visual range.While the snorkel renders a submarine far less detectable, it is not perfect. In clear weather, diesel exhaust can be seen on the surface to a distance of about three miles, while 'periscope feather' (the wave created by the snorkel or periscope moving through the water), is visible from far off in calm sea conditions. Modern radar is also capable of detecting a snorkel in calm sea conditions.The problem of the diesels causing a vacuum in the submarine when the head valve is submerged still exists in later model diesel submarines, but is mitigated by high-vacuum cut-off sensors that shut down the engines when the vacuum in the ship reaches a pre-set point. Modern snorkel induction masts use a fail-safe design using compressed air, controlled by a simple electrical circuit, to hold the "head valve" open against the pull of a powerful spring. Seawater washing over the mast shorts out exposed electrodes on top, breaking the control, and shutting the "head valve" while it is submerged. Air-independent propulsion During World War II, German Type XXI submarines (also known as "Elektroboote") were the first submarines designed to operate submerged for extended periods. Initially they were to carry hydrogen peroxide for long-term, fast air-independent propulsion, but were ultimately built with very large batteries instead. At the end of the War, the British and Soviets experimented with hydrogen peroxide/kerosene (paraffin) engines that could run surfaced and submerged. The results were not encouraging. Though the Soviet Union deployed a class of submarines with this engine type (codenamed Quebec by NATO), they were considered unsuccessful.The United States also used hydrogen peroxide in an experimental midget submarine, X-1. It was originally powered by a hydrogen peroxide/diesel engine and battery system until an explosion of her hydrogen peroxide supply on 20 May 1957. X-1 was later converted to use diesel-electric drive.Today several navies use air-independent propulsion. Notably Sweden uses Stirling technology on the Gotland-class and Södermanland-class submarines. The Stirling engine is heated by burning diesel fuel with liquid oxygen from cryogenic tanks. A newer development in air-independent propulsion is hydrogen fuel cells, first used on the German Type 212 submarine, with nine 34 kW or two 120 kW cells and soon to be used in the new Spanish S-80-class submarines. Nuclear power Steam power was resurrected in the 1950s with a nuclear-powered steam turbine driving a generator. By eliminating the need for atmospheric oxygen, the time that a submarine could remain submerged was limited only by its food stores, as breathing air was recycled and fresh water distilled from seawater. More importantly, a nuclear submarine has unlimited range at top speed. This allows it to travel from its operating base to the combat zone in a much shorter time and makes it a far more difficult target for most anti-submarine weapons. Nuclear-powered submarines have a relatively small battery and diesel engine/generator powerplant for emergency use if the reactors must be shut down.Nuclear power is now used in all large submarines, but due to the high cost and large size of nuclear reactors, smaller submarines still use diesel-electric propulsion. The ratio of larger to smaller submarines depends on strategic needs. The US Navy, French Navy, and the British Royal Navy operate only nuclear submarines, which is explained by the need for distant operations. Other major operators rely on a mix of nuclear submarines for strategic purposes and diesel-electric submarines for defense. Most fleets have no nuclear submarines, due to the limited availability of nuclear power and submarine technology.Diesel-electric submarines have a stealth advantage over their nuclear counterparts. Nuclear submarines generate noise from coolant pumps and turbo-machinery needed to operate the reactor, even at low power levels. Some nuclear submarines such as the American Ohio class can operate with their reactor coolant pumps secured, making them quieter than electric subs. A conventional submarine operating on batteries is almost completely silent, the only noise coming from the shaft bearings, propeller, and flow noise around the hull, all of which stops when the sub hovers in mid-water to listen, leaving only the noise from crew activity. Commercial submarines usually rely only on batteries, since they operate in conjunction with a mother ship.Several serious nuclear and radiation accidents have involved nuclear submarine mishaps. The Soviet submarine K-19 reactor accident in 1961 resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The Soviet submarine K-27 reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. The Soviet submarine K-431 accident in 1985 resulted in 10 fatalities and 49 other radiation injuries. Alternative propulsion Oil-fired steam turbines powered the British K-class submarines, built during World War I and later, to give them the surface speed to keep up with the battle fleet. The K-class subs were not very successful, however.Toward the end of the 20th century, some submarines—such as the British Vanguard class—began to be fitted with pump-jet propulsors instead of propellers. Though these are heavier, more expensive, and less efficient than a propeller, they are significantly quieter, providing an important tactical advantage.Magnetohydrodynamic drive (MHD) was portrayed as the operating principle behind the titular submarine's nearly silent propulsion system in the film adaptation of The Hunt for Red October. However, in the novel the Red October did not use MHD, but rather something more similar to the above-mentioned pump-jet. Armament The success of the submarine is inextricably linked to the development of the torpedo, invented by Robert Whitehead in 1866. His invention is essentially the same now as it was 140 years ago. Only with self-propelled torpedoes could the submarine make the leap from novelty to a weapon of war. Until the perfection of the guided torpedo, multiple "straight-running" torpedoes were required to attack a target. With at most 20 to 25 torpedoes stored on board, the number of attacks was limited. To increase combat endurance most World War I submarines functioned as submersible gunboats, using their deck guns against unarmed targets, and diving to escape and engage enemy warships. The importance of guns encouraged the development of the unsuccessful Submarine Cruiser such as the French Surcouf and the Royal Navy's X1 and M-class submarines. With the arrival of Anti-submarine warfare (ASW) aircraft, guns became more for defense than attack. A more practical method of increasing combat endurance was the external torpedo tube, loaded only in port.The ability of submarines to approach enemy harbours covertly led to their use as minelayers. Minelaying submarines of World War I and World War II were specially built for that purpose. Modern submarine-laid mines, such as the British Mark 5 Stonefish and Mark 6 Sea Urchin, can be deployed from a submarine's torpedo tubes.After World War II, both the US and the USSR experimented with submarine-launched cruise missiles such as the SSM-N-8 Regulus and P-5 Pyatyorka. Such missiles required the submarine to surface to fire its missiles. They were the forerunners of modern submarine-launched cruise missiles, which can be fired from the torpedo tubes of submerged submarines, for example the US BGM-109 Tomahawk and Russian RPK-2 Viyuga and versions of surface–to–surface anti-ship missiles such as the Exocet and Harpoon, encapsulated for submarine launch. Ballistic missiles can also be fired from a submarine's torpedo tubes, for example missiles such as the anti-submarine SUBROC. With internal volume as limited as ever and the desire to carry heavier warloads, the idea of the external launch tube was revived, usually for encapsulated missiles, with such tubes being placed between the internal pressure and outer streamlined hulls.The strategic mission of the SSM-N-8 and the P-5 was taken up by submarine-launched ballistic missile beginning with the US Navy's Polaris missile, and subsequently the Poseidon and Trident missiles.Germany is working on the torpedo tube-launched short-range IDAS missile, which can be used against ASW helicopters, as well as surface ships and coastal targets. Sensors A submarine can have a variety of sensors, depending on its missions. Modern military submarines rely almost entirely on a suite of passive and active sonars to locate targets. Active sonar relies on an audible "ping" to generate echoes to reveal objects around the submarine. Active systems are rarely used, as doing so reveals the sub's presence. Passive sonar is a set of sensitive hydrophones set into the hull or trailed in a towed array, normally trailing several hundred feet behind the sub. The towed array is the mainstay of NATO submarine detection systems, as it reduces the flow noise heard by operators. Hull mounted sonar is employed in addition to the towed array, as the towed array can't work in shallow depth and during maneuvering. In addition, sonar has a blind spot "through" the submarine, so a system on both the front and back works to eliminate that problem. As the towed array trails behind and below the submarine, it also allows the submarine to have a system both above and below the thermocline at the proper depth; sound passing through the thermocline is distorted resulting in a lower detection range.Submarines also carry radar equipment to detect surface ships and aircraft. Submarine captains are more likely to use radar detection gear than active radar to detect targets, as radar can be detected far beyond its own return range, revealing the submarine. Periscopes are rarely used, except for position fixes and to verify a contact's identity.Civilian submarines, such as the DSV Alvin or the Russian Mir submersibles, rely on small active sonar sets and viewing ports to navigate. The human eye cannot detect sunlight below about 300 feet (91 m) underwater, so high intensity lights are used to illuminate the viewing area. Navigation Early submarines had few navigation aids, but modern subs have a variety of navigation systems. Modern military submarines use an inertial guidance system for navigation while submerged, but drift error unavoidably builds over time. To counter this, the crew occasionally uses the Global Positioning System to obtain an accurate position. The periscope—a retractable tube with a prism system that provides a view of the surface—is only used occasionally in modern submarines, since the visibility range is short. The Virginia-class and Astute-class submarines use photonics masts rather than hull-penetrating optical periscopes. These masts must still be deployed above the surface, and use electronic sensors for visible light, infrared, laser range-finding, and electromagnetic surveillance. One benefit to hoisting the mast above the surface is that while the mast is above the water the entire sub is still below the water and is much harder to detect visually or by radar. Communication Military submarines use several systems to communicate with distant command centers or other ships. One is VLF (Very Low Frequency) radio, which can reach a submarine either on the surface or submerged to a fairly shallow depth, usually less than 250 feet (76 m). ELF (Extremely Low Frequency) can reach a submarine at greater depths, but has a very low bandwidth and is generally used to call a submerged sub to a shallower depth where VLF signals can reach. A submarine also has the option of floating a long, buoyant wire antenna to a shallower depth, allowing VLF transmissions by a deeply submerged boat.By extending a radio mast, a submarine can also use a "burst transmission" technique. A burst transmission takes only a fraction of a second, minimizing a submarine's risk of detection.To communicate with other submarines, a system known as Gertrude is used. Gertrude is basically a sonar telephone. Voice communication from one submarine is transmitted by low power speakers into the water, where it is detected by passive sonars on the receiving submarine. The range of this system is probably very short, and using it radiates sound into the water, which can be heard by the enemy.Civilian submarines can use similar, albeit less powerful systems to communicate with support ships or other submersibles in the area. Life support systems With nuclear power or air-independent propulsion, submarines can remain submerged for months at a time. Conventional diesel submarines must periodically resurface or run on snorkel to recharge their batteries. Most modern military submarines generate breathing oxygen by electrolysis of water (using a device called an "Elektrolytic Oxygen Generator"). Atmosphere control equipment includes a CO2 scrubber, which uses an amine absorbent to remove the gas from air and diffuse it into waste pumped overboard. A machine that uses a catalyst to convert carbon monoxide into carbon dioxide (removed by the CO2 scrubber) and bonds hydrogen produced from the ship's storage battery with oxygen in the atmosphere to produce water, is also used. An atmosphere monitoring system samples the air from different areas of the ship for nitrogen, oxygen, hydrogen, R-12 and R-114 refrigerants, carbon dioxide, carbon monoxide, and other gases. Poisonous gases are removed, and oxygen is replenished by use of an oxygen bank located in a main ballast tank. Some heavier submarines have two oxygen bleed stations (forward and aft). The oxygen in the air is sometimes kept a few percent less than atmospheric concentration to reduce fire danger.Fresh water is produced by either an evaporator or a reverse osmosis unit. The primary use for fresh water is to provide feedwater for the reactor and steam propulsion plants. It is also available for showers, sinks, cooking and cleaning once propulsion plant needs have been met. Seawater is used to flush toilets, and the resulting "black water" is stored in a sanitary tank until it is blown overboard using pressurized air or pumped overboard by using a special sanitary pump. The blackwater–discharge system is difficult to operate, and the German Type VIIC boat U-1206 was lost with casualties because of human error while using this system. Water from showers and sinks is stored separately in "grey water" tanks and discharged overboard using drain pumps.Trash on modern large submarines is usually disposed of using a tube called a Trash Disposal Unit (TDU), where it is compacted into a galvanized steel can. At the bottom of the TDU is a large ball valve. An ice plug is set on top of the ball valve to protect it, the cans atop the ice plug. The top breech door is shut, and the TDU is flooded and equalized with sea pressure, the ball valve is opened and the cans fall out assisted by scrap iron weights in the cans. The TDU is also flushed with seawater to ensure it is completely empty and the ball valve is clear before closing the valve. Crew A typical nuclear submarine has a crew of over 80; conventional boats typically have fewer than 40. The conditions on a submarine can be difficult because crew members must work in isolation for long periods of time, without family contact. Submarines normally maintain radio silence to avoid detection. Operating a submarine is dangerous, even in peacetime, and many submarines have been lost in accidents. Women Most navies prohibited women from serving on submarines, even after they had been permitted to serve on surface warships. The Royal Norwegian Navy became the first navy to allow females on its submarine crews in 1985. The Royal Danish Navy allowed female submariners in 1988. Others followed suit including the Swedish Navy (1989), the Royal Australian Navy (1998), the German Navy (2001) and the Canadian Navy (2002). In 1995, Solveig Krey of the Royal Norwegian Navy became the first female officer to assume command on a military submarine, HNoMS Kobben.On 8 December 2011, British Defence Secretary Philip Hammond announced that the UK's ban on women in submarines was to be lifted from 2013. Previously there were fears that women were more at risk from a build-up of carbon dioxide in the submarine. But a study showed no medical reason to exclude women, though pregnant women would still be excluded. Similar dangers to the pregnant woman and her fetus barred females from submarine service in Sweden in 1983, when all other positions were made available for them in the Swedish Navy. Today, pregnant women are still not allowed to serve on submarines in Sweden. However, the policymakers thought that it was discriminatory with a general ban and demanded that females should be tried on their individual merits and have their suitability evaluated and compared to other candidates. Further, they noted that a female complying with such high demands is unlikely to become pregnant. In May 2014, three women became the RN's first female submariners.Women have served on US Navy surface ships since 1993, and as of 2011–2012, began serving on submarines for the first time. Until presently, the Navy only allowed three exceptions to women being on board military submarines: female civilian technicians for a few days at most, women midshipmen on an overnight during summer training for Navy ROTC and Naval Academy, and family members for one-day dependent cruises. In 2009, senior officials, including then-Secretary of the Navy Ray Mabus, Joint Chief of Staff Admiral Michael Mullen, and Chief of Naval Operations Admiral Gary Roughead, began the process of finding a way to implement females on submarines. The US Navy rescinded its "no women on subs" policy in 2010.Both the US and British navies operate nuclear-powered submarines that deploy for periods of six months or longer. Other navies that permit women to serve on submarines operate conventionally powered submarines, which deploy for much shorter periods—usually only for a few months. Prior to the change by the US, no nation using nuclear submarines permitted women to serve on board.In 2011, the first class of female submarine officers graduated from Naval Submarine School's Submarine Officer Basic Course (SOBC) at the Naval Submarine Base New London. Additionally, more senior ranking and experienced female supply officers from the surface warfare specialty attended SOBC as well, proceeding to fleet Ballistic Missile (SSBN) and Guided Missile (SSGN) submarines along with the new female submarine line officers beginning in late 2011. By late 2011, several women were assigned to the Ohio-class ballistic missile submarine USS Wyoming. On 15 October 2013, the US Navy announced that two of the smaller Virginia-class attack submarines, USS Virginia and USS Minnesota, would have female crew-members by January 2015. Abandoning the vessel In an emergency, submarines can transmit a signal to other ships. The crew can use Submarine Escape Immersion Equipment to abandon the submarine. The crew can prevent a lung injury from the pressure change known as pulmonary barotrauma by exhaling during the ascent. Following escape from a pressurized submarine, the crew is at risk of developing decompression sickness. An alternative escape means is via a Deep Submergence Rescue Vehicle that can dock onto the disabled submarine. See also SupercavitationOhio Replacement SubmarineFuture of the Russian NavyAutonomous underwater vehicleCoastal submarineDepth chargeCategory:Fictional submarinesFlying submarineList of ships sunk by submarines by death tollList of submarine actionsList of submarine classesList of submarine museumsList of submarines of the Second World WarList of sunken nuclear submarinesMerchant submarineNuclear navySubmarine filmsSubmarine power cableSubmarine simulator, a computer game genre By country List of submarine operatorsAustralia – Collins-class submarineBritain – List of submarines of the Royal Navy, List of submarine classes of the Royal NavyChina – Submarines of the People's Liberation Army NavyFrance - Submarines in the French Navy, List of submarines of the French Navy, List of French submarine classes and typesGermany – List of U-boats of GermanyIndia – Submarines of the Indian NavyIsrael – Dolphin-class submarineJapan – Imperial Japanese Navy submarines, List of combatant ship classes of the Japan Maritime Self-Defense Force#SS : SubmarinePakistan – List of active Pakistan Navy ships#SubmarinesRussia – List of Soviet and Russian submarine classesSoviet Union – List of ships of the Soviet Navy#SubmarinesSpain – List of submarines in the Spanish NavyTurkey – List of submarines of the Turkish NavyUnited States – Submarines in the US Navy, List of submarines of the US Navy, List of US submarine classes, Naval Submarine Medical Research Laboratory References  Bibliography  External links U.S. Patent 708,553 – Submarine boatRole of the Modern SubmarineSubmariners Association – UK Submariners site and Boat DatabaseVideo from 1955 giving a detailed description of boat systems on YouTubeThe Invention of the SubmarineU.S. submarine photo archiveU.S. World War II Submarine Veterans History ProjectGerman Submarines of WWII, uboat.netRecord breaking Japanese SubmarinesList of Naval Submarines on naval-technology.comThe Fleet Type Submarine Online US Navy submarine training manuals, 1944–1946The Home Front: Manitowoc County in World War II: Video footage of submarine launches into Lake Michigan during World War IIAmerican Society of Safety Engineers. Journal of Professional Safety. Submarine Accidents: A 60-Year Statistical Assessment. C. Tingle. September 2009. pp. 31–39. Ordering full article; or Reproduction without graphics/tablesHistoric film footage showing submarines in WWI at europeanfilmgateway.eu
A motherboard (sometimes alternatively known as the mainboard, system board, baseboard, planar board or logic board, or colloquially, a mobo) is the main printed circuit board (PCB) found in general purpose microcomputers and other expandable systems. It holds and allows communication between many of the crucial electronic components of a system, such as the central processing unit (CPU) and memory, and provides connectors for other peripherals. Unlike a backplane, a motherboard usually contains significant sub-systems such as the central processor, the chipset's input/output and memory controllers, interface connectors, and other components integrated for general purpose use.Motherboard specifically refers to a PCB with expansion capability and as the name suggests, this board is often referred to as the "mother" of all components attached to it, which often include peripherals, interface cards, and daughtercards: sound cards, video cards, network cards, hard drives, or other forms of persistent storage; TV tuner cards, cards providing extra USB or FireWire slots and a variety of other custom components.Similarly, the term mainboard is applied to devices with a single board and no additional expansions or capability, such as controlling boards in laser printers, televisions, washing machines and other embedded systems with limited expansion abilities. History Prior to the invention of the microprocessor, a digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The Central Processing Unit (CPU), memory, and peripherals were housed on individual printed circuit boards, which were plugged into the backplane. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.The most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.During the late 1980s and 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: keyboard, mouse, floppy disk drive, serial ports, and parallel ports. By the late 1990s, many personal computer motherboards included consumer grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems. Design A motherboard provides the electrical connections by which the other components of the system communicate. Unlike a backplane, it also contains the central processing unit and hosts other subsystems and devices.A typical desktop computer has its microprocessor, main memory, and other essential components connected to the motherboard. Other components such as external storage, controllers for video display and sound, and peripheral devices may be attached to the motherboard as plug-in cards or via cables; in modern microcomputers it is increasingly common to integrate some of these peripherals into the motherboard itself.An important component of a motherboard is the microprocessor's supporting chipset, which provides the supporting interfaces between the CPU and the various buses and external components. This chipset determines, to an extent, the features and capabilities of the motherboard.Modern motherboards include:Sockets (or slots) in which one or more microprocessors may be installed. In the case of CPUs in ball grid array packages, such as the VIA C3, the CPU is directly soldered to the motherboard.Slots into which the system's main memory is to be installed (typically in the form of DIMM modules containing DRAM chips)A chipset which forms an interface between the CPU's front-side bus, main memory, and peripheral busesNon-volatile memory chips (usually Flash ROM in modern motherboards) containing the system's firmware or BIOSA clock generator which produces the system clock signal to synchronize the various componentsSlots for expansion cards (the interface to the system via the buses supported by the chipset)Power connectors, which receive electrical power from the computer power supply and distribute it to the CPU, chipset, main memory, and expansion cards. As of 2007, some graphics cards (e.g. GeForce 8 and Radeon R600) require more power than the motherboard can provide, and thus dedicated connectors have been introduced to attach them directly to the power supply.Connectors for hard drives, typically SATA only. Disk drives also connect to the power supply.Additionally, nearly all motherboards include logic and connectors to support commonly used input devices, such as USB for mouse devices and keyboards. Early personal computers such as the Apple II or IBM PC included only this minimal peripheral support on the motherboard. Occasionally video interface hardware was also integrated into the motherboard; for example, on the Apple II and rarely on IBM-compatible computers such as the IBM PC Jr. Additional peripherals such as disk controllers and serial ports were provided as expansion cards.Given the high thermal design power of high-speed computer CPUs and components, modern motherboards nearly always include heat sinks and mounting points for fans to dissipate excess heat. Form factor Motherboards are produced in a variety of sizes and shapes called computer form factor, some of which are specific to individual computer manufacturers. However, the motherboards used in IBM-compatible systems are designed to fit various case sizes. As of 2007, most desktop computer motherboards use the ATX standard form factor — even those found in Macintosh and Sun computers, which have not been built from commodity components. A case's motherboard and PSU form factor must all match, though some smaller form factor motherboards of the same family will fit larger cases. For example, an ATX case will usually accommodate a microATX motherboard.Laptop computers generally use highly integrated, miniaturized and customized motherboards. This is one of the reasons that laptop computers are difficult to upgrade and expensive to repair. Often the failure of one laptop component requires the replacement of the entire motherboard, which is usually more expensive than a desktop motherboard due to the large number of integrated components and their custom shape and size. CPU sockets A CPU socket (central processing unit) or slot is an electrical component that attaches to a Printed Circuit Board (PCB) and is designed to house a CPU (also called a microprocessor). It is a special type of integrated circuit socket designed for very high pin counts. A CPU socket provides many functions, including a physical structure to support the CPU, support for a heat sink, facilitating replacement (as well as reducing cost), and most importantly, forming an electrical interface both with the CPU and the PCB. CPU sockets on the motherboard can most often be found in most desktop and server computers (laptops typically use surface mount CPUs), particularly those based on the Intel x86 architecture. A CPU socket type and motherboard chipset must support the CPU series and speed. Integrated peripherals With the steadily declining costs and size of integrated circuits, it is now possible to include support for many peripherals on the motherboard. By combining many functions on one PCB, the physical size and total cost of the system may be reduced; highly integrated motherboards are thus especially popular in small form factor and budget computers.Disk controllers for a floppy disk drive, up to 2 PATA drives, and up to 6 SATA drives (including RAID 0/1 support)integrated graphics controller supporting 2D and 3D graphics, with VGA and TV outputintegrated sound card supporting 8-channel (7.1) audio and S/PDIF outputFast Ethernet network controller for 10/100 Mbit networkingUSB 2.0 controller supporting up to 12 USB portsIrDA controller for infrared data communication (e.g. with an IrDA-enabled cellular phone or printer)Temperature, voltage, and fan-speed sensors that allow software to monitor the health of computer components. Peripheral card slots A typical motherboard will have a different number of connections depending on its standard and form factor.A standard, modern ATX motherboard will typically have two or three PCI-Express 16x connection for a graphics card, one or two legacy PCI slots for various expansion cards, and one or two PCI-E 1x (which has superseded PCI). A standard EATX motherboard will have two to four PCI-E 16x connection for graphics cards, and a varying number of PCI and PCI-E 1x slots. It can sometimes also have a PCI-E 4x slot (will vary between brands and models).Some motherboards have two or more PCI-E 16x slots, to allow more than 2 monitors without special hardware, or use a special graphics technology called SLI (for Nvidia) and Crossfire (for AMD). These allow 2 to 4 graphics cards to be linked together, to allow better performance in intensive graphical computing tasks, such as gaming, video editing, etc. Temperature and reliability Motherboards are generally air cooled with heat sinks often mounted on larger chips, such as the Northbridge, in modern motherboards. Insufficient or improper cooling can cause damage to the internal components of the computer, or cause it to crash. Passive cooling, or a single fan mounted on the power supply, was sufficient for many desktop computer CPU's until the late 1990s; since then, most have required CPU fans mounted on their heat sinks, due to rising clock speeds and power consumption. Most motherboards have connectors for additional case fans and integrated temperature sensors to detect motherboard and CPU temperatures and controllable fan connectors which the BIOS or operating system can use to regulate fan speed. Alternatively computers can use a water cooling system instead of many fans.Some small form factor computers and home theater PCs designed for quiet and energy-efficient operation boast fan-less designs. This typically requires the use of a low-power CPU, as well as careful layout of the motherboard and other components to allow for heat sink placement.A 2003 study found that some spurious computer crashes and general reliability issues, ranging from screen image distortions to I/O read/write errors, can be attributed not to software or peripheral hardware but to aging capacitors on PC motherboards. Ultimately this was shown to be the result of a faulty electrolyte formulation, an issue termed capacitor plague.Motherboards use electrolytic capacitors to filter the DC power distributed around the board. These capacitors age at a temperature-dependent rate, as their water based electrolytes slowly evaporate. This can lead to loss of capacitance and subsequent motherboard malfunctions due to voltage instabilities. While most capacitors are rated for 2000 hours of operation at 105 °C (221 °F), their expected design life roughly doubles for every 10 °C (18 °F) below this. At 45 °C (113 °F) a lifetime of 15 years can be expected. This appears reasonable for a computer motherboard. However, many manufacturers deliver substandard capacitors, which significantly reduce life expectancy. Inadequate case cooling and elevated temperatures easily exacerbate this problem. It is possible, but time-consuming, to find and replace failed capacitors on personal computer motherboards. Air pollution and reliability High rates of motherboard failures in China and India appear to be due to "sulfurous air pollution produced by coal" that's burned to generate electricity. Air pollution corrodes the circuitry, according to Intel researchers. Bootstrapping using the Basic input output system Motherboards contain some non-volatile memory to initialize the system and load some startup software, usually an operating system, from some external peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor would load its program counter with the address of the boot ROM and start executing instructions from the ROM. These instructions initialized and tested the system hardware, displayed system information on the screen, performed RAM checks, and then loaded an initial program from an external or peripheral device. If none was available, then the computer would perform tasks from other memory stores or display an error message, depending on the model and design of the computer and the ROM version. For example, both the Apple II and the original IBM PC had Microsoft Cassette BASIC in ROM and would start that if no program could be loaded from disk.Most modern motherboard designs use a BIOS, stored in an EEPROM chip soldered to or socketed on the motherboard, to booting an operating system. Non-operating system boot programs are still supported on modern IBM PC-descended machines, but nowadays it is assumed that the boot program will be a complex operating system such as Microsoft Windows or Linux. When power is first supplied to the motherboard, the BIOS firmware tests and configures memory, circuitry, and peripherals. This Power-On Self Test (POST) may include testing some of the following things:Video adapterCards inserted into slots, such as conventional PCIFloppy driveTemperatures, voltages, and fan speeds for hardware monitoringCMOS used to store BIOS setup configurationKeyboard and MouseNetwork controllerOptical drives: CD-ROM or DVD-ROMSCSI hard driveIDE, EIDE, or Serial ATA Hard disk driveSecurity devices, such as a fingerprint reader or the state of a latching switch to detect intrusionUSB devices, such as a memory storage deviceOn recent motherboards the BIOS may also patch the central processor microcode if the BIOS detects that the installed CPU is one for which errata have been published. See also Accelerated Graphics PortComputer case screwsCMOS batteryDaughterboardList of computer hardware manufacturersMemory Reference Code – the part of the BIOS which handles memory timings on Intel motherboardsOverclockingSingle-board computerSwitched-mode power supply applicationsSymmetric multiprocessing References  External links Motherboard Form Factors - Silverstone ArticleMotherboards at DMOZList of motherboard manufacturers and links to BIOS updatesWhat is a motherboard?The Making of a Motherboard: ECS Factory TourThe Making of a Motherboard: Gigabyte Factory TourFront Panel I/O Connectivity Design Guide - v1.3 (pdf file)
An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs. All computer programs, excluding firmware, require an operating system to function.Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.The dominant desktop operating system is Microsoft Windows with a market share of around 83.3%. macOS by Apple Inc. is in second place (11.2%), and the varieties of Linux is in third position (1.55%). In the mobile (smartphone and tablet combined) sector, according to third quarter 2016 data, Android by Google is dominant with 87.5 percent and a growth rate 10.3 percent per year, followed by iOS by Apple with 12.1 percent and a per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications. Types of operating systems  Single- and multi-tasking A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency. This is achieved by time-sharing, dividing the available processor time between multiple processes that are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking. 32-bit versions of both Windows NT and Win9x, used preemptive multi-tasking. Single- and multi-user Single-user operating systems have no facilities to distinguish users, but may allow multiple programs to run in tandem. A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users. Distributed A distributed operating system manages a group of distinct computers and makes them appear to be a single computer. The development of networked computers that could be linked and communicate with each other gave rise to distributed computing. Distributed computations are carried out on more than one machine. When computers in a group work in cooperation, they form a distributed system. Templated In an OS, distributed and cloud computing context, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines. The technique is used both in virtualization and cloud computing management, and is common in large server warehouses. Embedded Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines like PDAs with less autonomy. They are able to operate with a limited number of resources. They are very compact and extremely efficient by design. Windows CE and Minix 3 are some examples of embedded operating systems. Real-time A real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. An event-driven system switches between tasks based on their priorities or external events while time-sharing operating systems switch tasks based on clock interrupts Library A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries. These libraries are composed with the application and configuration code to construct unikernels –  which are specialized, single address space, machine images that can be deployed to cloud or embedded environments. History Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plug boards. These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards. After programmable general purpose computers were invented, machine languages (consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that speed up the programming process (Stern, 1981).In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period of time and would arrive at a scheduled time with program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and generating computer code from human-readable symbolic code. This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England the job queue was at one time a washing line (clothes line) from which tapes were hung with different colored clothes-pegs to indicate job-priority.An improvement was the Atlas Supervisor introduced with the Manchester Atlas commissioned in 1962, "considered by many to be the first recognisable modern operating system". Brinch Hansen described it as "the most significant breakthrough in the history of operating systems." Mainframes Through the 1950s, many major features were pioneered in the field of operating systems, including batch processing, input/output interrupt, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094.During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and applications written for OS/360 can still be run on modern machines.OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during update. When the process is terminated for any reason, all of these resources are re-claimed by the operating system.The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: COS/360 (Compatibility Operating System), DOS/360 (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.In 1961, Burroughs Corporation introduced the B5000 with the MCP, (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no machine language or assembler, and indeed the MCP was the first OS to be written exclusively in a high-level language –  ESPOL, a dialect of ALGOL. MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. During development of the AS/400, IBM made an approach to Burroughs to licence MCP to run on the AS/400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. MCP is still in use today in the Unisys ClearPath/MCP line of computers.UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems. Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.General Electric and MIT developed General Electric Comprehensive Operating Supervisor (GECOS), which introduced the concept of ringed security privilege levels. After acquisition by Honeywell it was renamed General Comprehensive Operating System (GCOS).Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community.From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/165 and 360/168) were microprogrammed implementations.The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:Burroughs MCP –  B5000, 1961 to Unisys Clearpath/MCP, presentIBM OS/360 –  IBM System/360, 1966 to IBM z/OS, presentIBM CP-67 –  IBM System/360, 1967 to IBM z/VMUNIVAC EXEC 8 –  UNIVAC 1108, 1967, to OS 2200 Unisys Clearpath Dorado, present Microcomputers The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS). In the 1980s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative graphical user interface (GUI) to the Mac OS.The introduction of the Intel 80386 CPU chip in October 1985, with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X (macOS after latest name change).The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD. Examples of operating systems  Unix and Unix-like operating systems Unix was originally written in assembly language. Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name "UNIX" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. "UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.Four operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and Sparc servers, and PCs. Apple's macOS, a replacement for Apple's earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD.Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants. BSD and its descendants A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995). macOS macOS (formerly "Mac OS X" and later "OS X") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997. The operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 "Cheetah"). Since then, six more distinct "client" and "server" editions of macOS have been released, until the two were merged in OS X 10.7 "Lion".Prior to its merging with macOS, the server edition –  macOS Server –  was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. macOS Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as "OS X" (dropping "Mac" from the name). The server tools are now offered as an application. Linux The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smart-watches. Although estimates suggest that Linux is used on only 1.82% of all "desktop" (or laptop) PCs, it has been widely adopted for use in servers and embedded systems such as cell phones. Linux has superseded Unix on many platforms and is used on most supercomputers including the top 385. Many of the same computers are also on Green500 (but in different order), and Linux runs on the top 10. Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android. Google Chrome OS Chrome OS is an operating system based on the Linux kernel and designed by Google. It is developed out in the open in the Chromium OS open source variant and Google makes a proprietary variant of it (similar to the split for the Chrome and Chromium browser). Since Chromium OS targets computer users who spend most of their time on the Internet, it is mainly a web browser with limited ability to run local applications, though it has a built-in file manager and media player (in later versions, (modified) Android apps have also been supported, since the browser has been made to support them). Instead, it relies on Internet applications (or Web apps) used in the web browser to accomplish tasks such as word processing. Chromium OS differs from Chrome OS in that Chromium is open-source and used primarily by developers whereas Chrome OS is the operating system shipped out in Chromebooks. Microsoft Windows Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers. The latest version is Windows 10.In 2011, Windows 7 overtook Windows XP as most common version in use.Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS and 16-bit Windows 3.x drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and 32-bit ARM microprocessors. In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures.Server editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. However, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.ReactOS is a Windows-alternative operating system, which is being developed on the principles of Windows –  without using any of Microsoft's code. Other There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple's macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. OpenVMS, formerly from DEC, is still under active development by Hewlett-Packard. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research.Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9. Components The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component. Kernel With the aid of the firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc. Program execution The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices. Interrupts Interrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment. The alternative –  having the operating system "watch" the various sources of input for events (polling) that require action –  can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call. In modern operating systems, interrupts are handled by the operating system's kernel. Interrupts may come from either the computer's hardware or the running program.When a hardware device triggers an interrupt, the operating system's kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called a device driver, which may be part of the operating system's kernel, part of another program, or both. Device drivers may then relay information to a running program by various means.A program may also trigger an interrupt to the operating system. If a program wishes to access hardware, for example, it may interrupt the operating system's kernel, which causes control to be passed back to the kernel. The kernel then processes the request. If a program wishes additional resources (or wishes to shed resources) such as memory, it triggers an interrupt to get the kernel's attention. Modes Modern microprocessors (CPU or MPU) support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions. User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.At power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started the boundary between user mode and supervisor mode (also known as kernel mode) can be established.Supervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode. Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction. The software interrupt causes the microprocessor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control.In user mode, programs usually have access to a restricted set of microprocessor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system's operation. In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources.The term "user mode resource" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting, for example, by forceably terminating ("killing") the program). Memory management Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn't exist in all computers.In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt which cause the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway. Virtual memory The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.In modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand."Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there. Multitasking Multitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The AmigaOS is an exception, having preemptive multitasking from its very first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals). Disk access and file systems Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ext3 and ReiserFS in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system. Device drivers A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do. Networking Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's network address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware. Security A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between "privileged" and "non-privileged", systems commonly have a form of requester identity, such as a user name. To establish identity there may be a process of authentication. Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is authorization; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, "who has been reading this file?"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the Trusted Computer System Evaluation Criteria (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.Network services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.An alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java.Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing. User interface Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present. Graphical user interfaces Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of the classic Mac OS, the GUI is integrated into the kernel.While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and macOS are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.Many computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma 5 is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999. Real-time operating systems A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase. Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing. Operating system development as a hobby Operating system development is one of the most complicated activities in which a computing hobbyist may engage. A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.Examples of a hobby operating system include Syllable. Diversity of operating systems and portability Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.Unix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11.This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms like Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs. Market share Source: GartnerIn 2014, Android was first (currently not replicated by others, in a single year) operating system ever to ship on a billion devices, becoming the most popular operating system by installed base. See also  References  Further reading  External links Operating Systems at DMOZMultics History and the history of operating systems
A car is a wheeled, self-powered motor vehicle used for transportation and a product of the automotive industry. Most definitions of the term specify that cars are designed to run primarily on roads, to have seating for one to eight people, to typically have four wheels with tyres, and to be constructed principally for the transport of people rather than goods. The year 1886 is regarded as the birth year of the modern car. In that year, German inventor Karl Benz built the Benz Patent-Motorwagen. Cars did not become widely available until the early 20th century. One of the first cars that was accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the United States of America, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.Cars are equipped with controls used for driving, parking, passenger comfort and safety, and controlling a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex. Examples include rear reversing cameras, air conditioning, navigation systems, and in car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by deflagration of gasoline (also known as petrol) or diesel. Both fuels cause air pollution and are also blamed for contributing to climate change and global warming. Vehicles using alternative fuels such as ethanol flexible-fuel vehicles and natural gas vehicles are also gaining popularity in some countries. Electric cars, which were invented early in the history of the car, began to become commercially available in 2008.There are costs and benefits to car use. The costs of car usage include the cost of: acquiring the vehicle, interest payments (if the car is financed), repairs and auto maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance. The costs to society of car use include: maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Road traffic accidents are the largest cause of injury-related deaths worldwide.The benefits may include on-demand transportation, mobility, independence, and convenience. The societal benefits may include: economic benefits, such as job and wealth creation from car production, sales and maintenance, transportation provision, society well-being derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability for humans to move flexibly from place to place has far-reaching implications for the nature of societies. It was estimated in 2010 that the number of cars had risen to over 1 billion vehicles, up from the 500 million of 1986. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries. Etymology The word "car" is believed to originate from the Latin word carrus or carrum ("wheeled vehicle"), or the Middle English word carre (meaning two-wheel cart, from Old North French). In turn, these originated from the Gaulish word karros (a Gallic chariot). The Gaulish language was a branch of the Brythoic language which also used the word Karr; the Brythonig language evolved into Welsh (and Gaelic) where 'Car llusg' (a drag cart or sledge) and 'car rhyfel' (war chariot) still survive. It originally referred to any wheeled horse-drawn vehicle, such as a cart, carriage, or wagon. "Motor car" is attested from 1895, and is the usual formal name for cars in British English. "Autocar" is a variant that is also attested from 1895, but that is now considered archaic. It literally means "self-propelled car". The term "horseless carriage" was used by some to refer to the first cars at the time that they were being built, and is attested from 1895.The word "automobile" is a classical compound derived from the Ancient Greek word autós (αὐτός), meaning "self", and the Latin word mobilis, meaning "movable". It entered the English language from French, and was first adopted by the Automobile Club of Great Britain in 1897. Over time, the word "automobile" fell out of favour in Britain, and was replaced by "motor car". It remains a chiefly North American usage. An abbreviated form, "auto", was formerly a common way to refer to cars in English, but is now considered old-fashioned. The word is still used in some compound formations in American English, like "auto industry" and "auto mechanic". The abbreviated form is also used in Dutch and German. History The first working steam-powered vehicle was designed—and most likely built—by Ferdinand Verbiest, a Flemish member of a Jesuit mission in China around 1672. It was a 65-cm-long scale-model toy for the Chinese Emperor that was unable to carry a driver or a passenger. It is not known if Verbiest's model was ever built.Nicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle or car in about 1769; he created a steam-powered tricycle. He also constructed two steam tractors for the French Army, one of which is preserved in the French National Conservatory of Arts and Crafts. His inventions were, however, handicapped by problems with water supply and maintaining steam pressure. In 1801, Richard Trevithick built and demonstrated his Puffing Devil road locomotive, believed by many to be the first demonstration of a steam-powered road vehicle. It was unable to maintain sufficient steam pressure for long periods, and was of little practical use.The development of external combustion engines is detailed as part of the history of the car, but often treated separately from the development of true cars. A variety of steam-powered road vehicles were used during the first part of the 19th century, including steam cars, steam buses, phaetons, and steam rollers. Sentiment against them led to the Locomotive Acts of 1865.In 1807, Nicéphore Niépce and his brother Claude created what was probably the world's first internal combustion engine (which they called a Pyréolophore), but they chose to install it in a boat on the river Saone in France. Coincidentally, in 1807 the Swiss inventor François Isaac de Rivaz designed his own 'de Rivaz internal combustion engine' and used it to develop the world's first vehicle to be powered by such an engine. The Niépces' Pyréolophore was fuelled by a mixture of Lycopodium powder (dried spores of the Lycopodium plant), finely crushed coal dust and resin that were mixed with oil, whereas de Rivaz used a mixture of hydrogen and oxygen. Neither design was very successful, as was the case with others, such as Samuel Brown, Samuel Morey, and Etienne Lenoir with his hippomobile, who each produced vehicles (usually adapted carriages or carts) powered by internal combustion engines.In November 1881, French inventor Gustave Trouvé demonstrated the first working (three-wheeled) car powered by electricity at the International Exposition of Electricity, Paris. Although several other German engineers (including Gottlieb Daimler, Wilhelm Maybach, and Siegfried Marcus) were working on the problem at about the same time, Karl Benz generally is acknowledged as the inventor of the modern car.In 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a model intended for affordability. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz car to his line of products. Because France was more open to the early cars, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888 Bertha Benz, the wife of Karl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor. During the last years of the nineteenth century, Benz was the largest car company in the world with 572 units produced in 1899 and, because of its size, Benz & Cie., became a joint-stock company. The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Präsident automobil.Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890, and sold their first car in 1892 under the brand name Daimler. It was a horse-drawn stagecoach built by another manufacturer, which they retrofitted with an engine of their design. By 1895 about 30 vehicles had been built by Daimler and Maybach, either at the Daimler works or in the Hotel Hermann, where they set up shop after disputes with their backers. Benz, Maybach and the Daimler team seem to have been unaware of each other's early work. They never worked together; by the time of the merger of the two companies, Daimler and Maybach were no longer part of DMG. Daimler died in 1900 and later that year, Maybach designed an engine named Daimler-Mercedes that was placed in a specially ordered model built to specifications set by Emil Jellinek. This was a production of a small number of vehicles for Jellinek to race and market in his country. Two years later, in 1902, a new model DMG car was produced and the model was named Mercedes after the Maybach engine, which generated 35 hp. Maybach quit DMG shortly thereafter and opened a business of his own. Rights to the Daimler brand name were sold to other manufacturers.Karl Benz proposed co-operation between DMG and Benz & Cie. when economic conditions began to deteriorate in Germany following the First World War, but the directors of DMG refused to consider it initially. Negotiations between the two companies resumed several years later when these conditions worsened and, in 1924 they signed an Agreement of Mutual Interest, valid until the year 2000. Both enterprises standardized design, production, purchasing, and sales and they advertised or marketed their car models jointly, although keeping their respective brands. On 28 June 1926, Benz & Cie. and DMG finally merged as the Daimler-Benz company, baptizing all of its cars Mercedes Benz, as a brand honoring the most important model of the DMG cars, the Maybach design later referred to as the 1902 Mercedes-35 hp, along with the Benz name. Karl Benz remained a member of the board of directors of Daimler-Benz until his death in 1929, and at times his two sons also participated in the management of the company.In 1890, Émile Levassor and Armand Peugeot of France began producing vehicles with Daimler engines, and so laid the foundation of the automotive industry in France. In 1891, Auguste Doriot and his Peugeot colleague Louis Rigoulot completed the longest trip by a gasoline-powered vehicle when their self-designed and built Daimler powered Peugeot Type 3 completed 2,100 km (1,300 miles) from Valentigney to Paris and Brest and back again. They were attached to the first Paris–Brest–Paris bicycle race, but finished 6 days after the winning cyclist, Charles Terront.The first design for an American car with a gasoline internal combustion engine was made in 1877 by George Selden of Rochester, New York. Selden applied for a patent for a car in 1879, but the patent application expired because the vehicle was never built. After a delay of sixteen years and a series of attachments to his application, on 5 November 1895, Selden was granted a United States patent (U.S. Patent 549,160) for a two-stroke car engine, which hindered, more than encouraged, development of cars in the United States. His patent was challenged by Henry Ford and others, and overturned in 1911.In 1893, the first running, gasoline-powered American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts. The first public run of the Duryea Motor Wagon took place on 21 September 1893, on Taylor Street in Metro Center Springfield. The Studebaker Automobile Company, subsidiary of a long-established wagon and coach manufacturer, started to build cars in 1897 and commenced sales of electric vehicles in 1902 and gasoline vehicles in 1904.In Britain, there had been several attempts to build steam cars with varying degrees of success, with Thomas Rickett even attempting a production run in 1860. Santler from Malvern is recognized by the Veteran Car Club of Great Britain as having made the first gasoline-powered car in the country in 1894, followed by Frederick William Lanchester in 1895, but these were both one-offs. The first production vehicles in Great Britain came from the Daimler Company, a company founded by Harry J. Lawson in 1896, after purchasing the right to use the name of the engines. Lawson's company made its first car in 1897, and they bore the name Daimler.In 1892, German engineer Rudolf Diesel was granted a patent for a "New Rational Combustion Engine". In 1897, he built the first diesel engine. Steam-, electric-, and gasoline-powered vehicles competed for decades, with gasoline internal combustion engines achieving dominance in the 1910s. Although various pistonless rotary engine designs have attempted to compete with the conventional piston and crankshaft design, only Mazda's version of the Wankel engine has had more than very limited success. Mass production The large-scale, production-line manufacturing of affordable cars was debuted by Ransom Olds in 1901 at his Oldsmobile factory located in Lansing, Michigan and based upon stationary assembly line techniques pioneered by Marc Isambard Brunel at the Portsmouth Block Mills, England, in 1802. The assembly line style of mass production and interchangeable parts had been pioneered in the U.S. by Thomas Blanchard in 1821, at the Springfield Armory in Springfield, Massachusetts. This concept was greatly expanded by Henry Ford, beginning in 1913 with the world's first moving assembly line for cars at the Highland Park Ford Plant.As a result, Ford's cars came off the line in fifteen-minute intervals, much faster than previous methods, increasing productivity eightfold, while using less manpower (from 12.5-man-hours to 1 hour 33 minutes). It was so successful, paint became a bottleneck. Only Japan Black would dry fast enough, forcing the company to drop the variety of colors available before 1913, until fast-drying Duco lacquer was developed in 1926. This is the source of Ford's apocryphal remark, "any color as long as it's black". In 1914, an assembly line worker could buy a Model T with four months' pay.Ford's complex safety procedures—especially assigning each worker to a specific location instead of allowing them to roam about—dramatically reduced the rate of injury. The combination of high wages and high efficiency is called "Fordism," and was copied by most major industries. The efficiency gains from the assembly line also coincided with the economic rise of the United States. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.In the automotive industry, its success was dominating, and quickly spread worldwide seeing the founding of Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany 1925; in 1921, Citroen was the first native European manufacturer to adopt the production method. Soon, companies had to have assembly lines, or risk going broke; by 1930, 250 companies which did not, had disappeared.Development of automotive technology was rapid, due in part to the hundreds of small manufacturers competing to gain the world's attention. Key developments included electric ignition and the electric self-starter (both by Charles Kettering, for the Cadillac Motor Company in 1910–1911), independent suspension, and four-wheel brakes.Since the 1920s, nearly all cars have been mass-produced to meet market needs, so marketing plans often have heavily influenced car design. It was Alfred P. Sloan who established the idea of different makes of cars produced by one company, called the General Motors Companion Make Program, so that buyers could "move up" as their fortunes improved.Reflecting the rapid pace of change, makes shared parts with one another so larger production volume resulted in lower costs for each price range. For example, in the 1930s, LaSalles, sold by Cadillac, used cheaper mechanical parts made by Oldsmobile; in the 1950s, Chevrolet shared hood, doors, roof, and windows with Pontiac; by the 1990s, corporate powertrains and shared platforms (with interchangeable brakes, suspension, and other parts) were common. Even so, only major makers could afford high costs, and even companies with decades of production, such as Apperson, Cole, Dorris, Haynes, or Premier, could not manage: of some two hundred American car makers in existence in 1920, only 43 survived in 1930, and with the Great Depression, by 1940, only 17 of those were left.In Europe, much the same would happen. Morris set up its production line at Cowley in 1924, and soon outsold Ford, while beginning in 1923 to follow Ford's practice of vertical integration, buying Hotchkiss (engines), Wrigley (gearboxes), and Osberton (radiators), for instance, as well as competitors, such as Wolseley: in 1925, Morris had 41% of total British car production. Most British small-car assemblers, from Abbey to Xtra, had gone under. Citroen did the same in France, coming to cars in 1919; between them and other cheap cars in reply such as Renault's 10CV and Peugeot's 5CV, they produced 550,000 cars in 1925, and Mors, Hurtu, and others could not compete. Germany's first mass-manufactured car, the Opel 4PS Laubfrosch (Tree Frog), came off the line at Russelsheim in 1924, soon making Opel the top car builder in Germany, with 37.5% of the market.In Japan, car production was very limited before World War II. Only a handful of companines were producing vehicles in limited numbers, and these were small, three-wheeled for commercial uses, like Daihatsu, or were the result of partnering with European companies, like Isuzu building the Wolseley A-9 in 1922. Mitsubishi was also partnered with Fiat and built the Mitsubishi Model A based on a Fiat vehicle. Toyota, Nissan, Suzuki, Mazda, and Honda began as companies producing non-automotive products before the war, switching to car production during the 1950s. Kiichiro Toyoda's decision to take Toyoda Loom Works into automobile manufacturing would create what would eventually become Toyota Motor Corporation, the largest automobile manufacturer in the world. Subaru, meanwhile, was formed from a conglomerate of six companies who banded together as Fuji Heavy Industries, as a result of having been broken up under keiretsu legislation. Fuel and propulsion technologies Most cars in use today are propelled by an internal combustion engine, fueled by deflagration of gasoline or diesel. Both fuels are known to cause air pollution and are also blamed for contributing to climate change and global warming. Rapidly increasing oil prices, concerns about oil dependence, tightening environmental laws and restrictions on greenhouse gas emissions are propelling work on alternative power systems for cars. Efforts to improve or replace existing technologies include the development of hybrid vehicles, plug-in electric vehicles and hydrogen vehicles. Vehicles using alternative fuels such as ethanol flexible-fuel vehicles and natural gas vehicles are also gaining popularity in some countries. Cars for racing or speed records have sometimes employed jet or rocket engines, but these are impractical for common use.Oil consumption in the twentieth and twenty-first centuries has been abundantly pushed by car growth; the 1985–2003 oil glut even fuelled the sales of low-economy vehicles in OECD countries. The BRIC countries are adding to this consumption; in December 2009 China was briefly the largest car market. User interface Cars are equipped with controls used for driving, passenger comfort and safety, normally operated by a combination of the use of feet and hands, and occasionally by voice on 2000s-era cars. These controls include a steering wheel, pedals for operating the brakes and controlling the car's speed (and, in a manual transmission car, a clutch pedal), a shift lever or stick for changing gears, and a number of buttons and dials for turning on lights, ventilation and other functions. Modern cars' controls are now standardised, such as the location for the accelerator and brake, but this was not always the case. Controls are evolving in response to new technologies, for example the electric car and the integration of mobile communications.Since the car was first invented, its controls have become fewer and simpler through automation. For example, all cars once had a manual controls for the choke valve, clutch, ignition timing, and a crank instead of an electric starter. However new controls have also been added to vehicles, making them more complex. Examples include air conditioning, navigation systems, and in car entertainment. Another trend is the replacement of physical knob and switches for secondary controls with touchscreen controls such as BMW's iDrive and Ford's MyFord Touch. Another change is that while early cars' pedals were physically linked to the brake mechanism and throttle, in the 2010s, cars have increasingly replaced these physical linkages with electronic controls. Lighting Cars are typically fitted with multiple types of lights. These include headlights, which are used to illuminate the way ahead and make the car visible to other users, so that the vehicle can be used at night; in some jurisdictions, daytime running lights; red brake lights to indicate when the brakes are applied; amber turn signal lights to indicate the turn intentions of the driver; white-coloured reverse lights to illuminate the area behind the car (and indicate that the driver will be or is reversing); and on some vehicles, additional lights (e.g., side marker lights) to increase the visibility of the car. Interior lights on the ceiling of the car are usually fitted for the driver and passengers. Some vehicles also have a trunk light and, more rarely, an engine compartment light. Weight In the United States, "from 1975 to 1980, average [car] weight dropped from 1,842 to 1,464 kg (4,060 to 3,228 lb), likely in response to rising gasoline prices" and new fuel efficiency standards. The average new car weighed 1,461 kg (3,221 lb) in 1987 but 1,818 kg (4,009 lb) in 2010, due to modern steel safety cages, anti-lock brakes, airbags, and "more-powerful—if more-efficient—engines." Heavier cars are safer for the driver, from an accident perspective, but more dangerous for other vehicles and road users. The weight of a car influences fuel consumption and performance, with more weight resulting in increased fuel consumption and decreased performance. The SmartFortwo, a small city car, weighs 750–795 kg (1,655–1,755 lb). Heavier cars include full-size cars, SUVs and extended-length SUVs like the Suburban.According to research conducted by Julian Allwood of the University of Cambridge, global energy use could be heavily reduced by using lighter cars, and an average weight of 500 kg (1,100 lb) has been said to be well achievable. In some competitions such as the Shell Eco Marathon, average car weights of 45 kg (99 lb) have also been achieved. These cars are only single-seaters (still falling within the definition of a car, although 4-seater cars are more common), but they nevertheless demonstrate the amount by which car weights could still be reduced, and the subsequent lower fuel use (i.e. up to a fuel use of 2560 km/l). Seating and body style Most cars are designed to carry multiple occupants, often with four or five seats. Cars with five seats typically seat two passengers in the front and three in the rear. Full-size cars and large sport utility vehicles can often carry six, seven, or more occupants depending on the arrangement of the seats. In the other hand, sports cars are most often designed with only two seats. The differing needs for passenger capacity and their luggage or cargo space has resulted in the availability of a large variety of body styles to meet individual consumer requirements that include, among others, the sedan/saloon, hatchback, station wagon/estate, and minivan. Safety Road traffic accidents are the largest cause of injury-related deaths worldwide. Mary Ward became one of the first documented car fatalities in 1869 in Parsonstown, Ireland, and Henry Bliss one of the United States' first pedestrian car casualties in 1899 in New York City. There are now standard tests for safety in new cars, such as the EuroNCAP and the US NCAP tests, and insurance-industry-backed tests by the Insurance Institute for Highway Safety (IIHS).Worldwide, road traffic is becoming ever safer, in part due to efforts by the government to implement safety features in cars (e.g., seat belts, air bags, etc.), reduce unsafe driving practices (e.g., speeding, drinking and driving and texting and driving) and make road design more safe by adding features such as speed bumps, which reduce vehicle speed, and roundabouts, which reduce the likelihood of a head-on-collision (as compared with an intersection). Costs and benefits The costs of car usage, which may include the cost of: acquiring the vehicle, repairs and auto maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance, are weighed against the cost of the alternatives, and the value of the benefits – perceived and real – of vehicle usage. The benefits may include on-demand transportation, mobility, independence and convenience. During the 1920s, cars had another benefit: "[c]ouples finally had a way to head off on unchaperoned dates, plus they had a private space to snuggle up close at the end of the night."Similarly the costs to society of encompassing car use, which may include those of: maintaining roads, land use, air pollution, road congestion, public health, health care, and of disposing of the vehicle at the end of its life, can be balanced against the value of the benefits to society that car use generates. The societal benefits may include: economy benefits, such as job and wealth creation, of car production and maintenance, transportation provision, society wellbeing derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability for humans to move flexibly from place to place has far-reaching implications for the nature of societies. Environmental impact While there are different types of fuel that may power cars, most rely on gasoline or diesel. The United States Environmental Protection Agency states that the average vehicle emits 8,887 grams of carbon dioxide per gallon of gasoline. The average vehicle running on diesel fuel will emit 10,180 grams of carbon dioxide. Many governments are using fiscal policies (such as road tax or the US gas guzzler tax) to influence vehicle purchase decisions, with a low CO2 figure often resulting in reduced taxation. Fuel taxes may act as an incentive for the production of more efficient, hence less polluting, car designs (e.g. hybrid vehicles) and the development of alternative fuels. High fuel taxes may provide a strong incentive for consumers to purchase lighter, smaller, more fuel-efficient cars, or to not drive. On average, today's cars are about 75 percent recyclable, and using recycled steel helps reduce energy use and pollution. In the United States Congress, federally mandated fuel efficiency standards have been debated regularly, passenger car standards have not risen above the 27.5 miles per US gallon (8.6 L/100 km; 33.0 mpg‑imp) standard set in 1985. Light truck standards have changed more frequently, and were set at 22.2 miles per US gallon (10.6 L/100 km; 26.7 mpg‑imp) in 2007.The manufacture of vehicles is resource intensive, and many manufacturers now report on the environmental performance of their factories, including energy usage, waste and water consumption.The growth in popularity of the car allowed cities to sprawl, therefore encouraging more travel by car resulting in inactivity and obesity, which in turn can lead to increased risk of a variety of diseases.Transportation (of all types including trucks, buses and cars) is a major contributor to air pollution in most industrialised nations. According to the American Surface Transportation Policy Project nearly half of all Americans are breathing unhealthy air. Their study showed air quality in dozens of metropolitan areas has worsened over the last decade.Animals and plants are often negatively impacted by cars via habitat destruction and pollution. Over the lifetime of the average car the "loss of habitat potential" may be over 50,000 m2 (540,000 sq ft) based on primary production correlations. Animals are also killed every year on roads by cars, referred to as roadkill. More recent road developments are including significant environmental mitigations in their designs such as green bridges to allow wildlife crossings, and creating wildlife corridors.Growth in the popularity of vehicles and commuting has led to traffic congestion. Brussels was considered Europe's most congested city in 2011 according to TomTom. Emerging car technologies Car propulsion technologies that are under development include gasoline/electric and plug-in hybrids, battery electric vehicles, hydrogen cars, biofuels, and various alternative fuels. Research into future alternative forms of power include the development of fuel cells, Homogeneous Charge Compression Ignition (HCCI), Stirling engines, and even using the stored energy of compressed air or liquid nitrogen.New materials which may replace steel car bodies include duralumin, fiberglass, carbon fiber, and carbon nanotubes. Telematics technology is allowing more and more people to share cars, on a pay-as-you-go basis, through car share and carpool schemes. Communication is also evolving due to connected car systems. Autonomous car Fully autonomous vehicles, also known as driverless cars, already exist in prototype (such as the Google driverless car), and are expected to be commercially available around 2020. According to urban designer and futurist Michael E. Arth, driverless electric vehicles—in conjunction with the increased use of virtual reality for work, travel, and pleasure—could reduce the world's 800 million vehicles to a fraction of that number within a few decades. This would be possible if almost all private cars requiring drivers, which are not in use and parked 90% of the time, would be traded for public self-driving taxis that would be in near constant use. This would also allow for getting the appropriate vehicle for the particular need—a bus could come for a group of people, a limousine could come for a special night out, and a Segway could come for a short trip down the street for one person. Children could be chauffeured in supervised safety, DUIs would no longer exist, and 41,000 lives could be saved each year in the US alone. Open source development There have been several projects aiming to develop a car on the principles of open design, an approach to designing in which the plans for the machinery and systems are publicly shared, often without monetary compensation. The projects include OScar, Riversimple (through 40fires.org) and c,mm,n. None of the projects have reached significant success in terms of developing a car as a whole both from hardware and software perspective and no mass production ready open-source based design have been introduced as of late 2009. Some car hacking through on-board diagnostics (OBD) has been done so far. Industry The automotive industry designs, develops, manufactures, markets, and sells the world's motor vehicles. In 2008, more than 70 million motor vehicles, including cars and commercial vehicles were produced worldwide.In 2007, a total of 71.9 million new cars were sold worldwide: 22.9 million in Europe, 21.4 million in the Asia-Pacific Region, 19.4 million in the USA and Canada, 4.4 million in Latin America, 2.4 million in the Middle East and 1.4 million in Africa. The markets in North America and Japan were stagnant, while those in South America and other parts of Asia grew strongly. Of the major markets, China, Russia, Brazil and India saw the most rapid growth.About 250 million vehicles are in use in the United States. Around the world, there were about 806 million cars and light trucks on the road in 2007; they burn over 260 billion US gallons (980,000,000 m3) of gasoline and diesel fuel yearly. The numbers are increasing rapidly, especially in China and India. In the opinion of some, urban transport systems based around the car have proved unsustainable, consuming excessive energy, affecting the health of populations, and delivering a declining level of service despite increasing investments. Many of these negative impacts fall disproportionately on those social groups who are also least likely to own and drive cars. The sustainable transport movement focuses on solutions to these problems.In 2008, with rapidly rising oil prices, industries such as the automotive industry, are experiencing a combination of pricing pressures from raw material costs and changes in consumer buying habits. The industry is also facing increasing external competition from the public transport sector, as consumers re-evaluate their private vehicle usage. Roughly half of the US's fifty-one light vehicle plants are projected to permanently close in the coming years, with the loss of another 200,000 jobs in the sector, on top of the 560,000 jobs lost this decade. Combined with robust growth in China, in 2009, this resulted in China becoming the largest car producer and market in the world. China 2009 sales had increased to 13.6 million, a significant increase from one million of domestic car sales in 2000. Since then however, even in China and other BRIC countries, the automotive production is again falling. Alternatives Established alternatives for some aspects of car use include public transit such as buses, trolleybuses, trains, subways, tramways light rail, cycling, and walking. Car-share arrangements and carpooling are also increasingly popular, in the US and Europe. For example, in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007. Services like car sharing offering a residents to "share" a vehicle rather than own a car in already congested neighborhoods. Bike-share systems have been tried in some European cities, including Copenhagen and Amsterdam. Similar programs have been experimented with in a number of US Cities. Additional individual modes of transport, such as personal rapid transit could serve as an alternative to cars if they prove to be socially accepted. Other meanings The term motorcar has formerly also been used in the context of electrified rail systems to denote a car which functions as a small locomotive but also provides space for passengers and baggage. These locomotive cars were often used on suburban routes by both interurban and intercity railroad systems. See also  References  Further reading Halberstam, David, The Reckoning, New York, Morrow, 1986. ISBN 0-688-04838-2Kay, Jane Holtz, Asphalt nation : how the automobile took over America, and how we can take it back, New York, Crown, 1997. ISBN 0-517-58702-5Heathcote Williams, Autogeddon, New York, Arcade, 1991. ISBN 1-55970-176-5Wolfgang Sachs: For love of the automobile: looking back into the history of our desires, Berkeley: University of California Press, 1992, ISBN 0-520-06878-5 External links Fédération Internationale de l'AutomobileForum for the Automobile and Society
Football is a family of team sports that involve, to varying degrees, kicking a ball with the foot to score a goal. Unqualified, the word football is understood to refer to whichever form of football is the most popular in the regional context in which the word appears. Sports commonly called 'football' in certain places include: association football (known as soccer in some countries); gridiron football (specifically American football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); and Gaelic football. These different variations of football are known as football codes.Various forms of football can be identified in history, often as popular peasant games. Contemporary codes of football can be traced back to the codification of these games at English public schools during the nineteenth century. The expanse of the British Empire allowed these rules of football to spread to areas of British influence outside of the directly controlled Empire. By the end of the nineteenth century, distinct regional codes were already developing: Gaelic football, for example, deliberately incorporated the rules of local traditional football games in order to maintain their heritage. In 1888, The Football League was founded in England, becoming the first of many professional football competitions. During the twentieth century, several of the various kinds of football grew to become some of the most popular team sports in the world. Common elements The various codes of football share certain common elements: Players in American football, Canadian football, rugby union and rugby league take up positions in a limited area of the field at the start of the game. They tend to use throwing and running as the main ways of moving the ball, and only kick on certain limited occasions. Body tackling is a major skill, and games typically involve short passages of play of 5–90 seconds.Association football, Australian rules football and Gaelic football tend to use kicking to move the ball around the pitch, with handling more limited. Body tackles are less central to the game, and players are freer to move around the field (offside laws are typically less strict).Common rules among the sports include:Two teams of usually between 11 and 18 players; some variations that have fewer players (five or more per team) are also popular.A clearly defined area in which to play the game.Scoring goals or points by moving the ball to an opposing team's end of the field and either into a goal area, or over a line.Goals or points resulting from players putting the ball between two goalposts.The goal or line being defended by the opposing team.Players being required to move the ball—depending on the code—by kicking, carrying, or hand-passing the ball.Players using only their body to move the ball.In all codes, common skills include passing, tackling, evasion of tackles, catching and kicking. In most codes, there are rules restricting the movement of players offside, and players scoring a goal must put the ball either under or over a crossbar between the goalposts. Etymology There are conflicting explanations of the origin of the word "football". It is widely assumed that the word "football" (or the phrase "foot ball") refers to the action of the foot kicking a ball. There is an alternative explanation, which is that football originally referred to a variety of games in medieval Europe, which were played on foot. There is no conclusive evidence for either explanation. Early history  Ancient games The Ancient Greeks and Romans are known to have played many ball games, some of which involved the use of the feet. The Roman game harpastum is believed to have been adapted from a Greek team game known as "ἐπίσκυρος" (Episkyros) or "φαινίνδα" (phaininda), which is mentioned by a Greek playwright, Antiphanes (388–311 BC) and later referred to by the Christian theologian Clement of Alexandria (c. 150 – c. 215 AD). These games appear to have resembled rugby football. The Roman politician Cicero (106–43 BC) describes the case of a man who was killed whilst having a shave when a ball was kicked into a barber's shop. Roman ball games already knew the air-filled ball, the follis. Episkyros is recognised as an early form of football by FIFA.A Chinese game called Cuju (蹴鞠), Tsu' Chu, or Zuqiu (足球) has been recognised by FIFA as the first version of the game with regular rules. It existed during the Han Dynasty, the second and third centuries BC. The Japanese version of cuju is kemari (蹴鞠), and was developed during the Asuka period. This is known to have been played within the Japanese imperial court in Kyoto from about 600 AD. In kemari several people stand in a circle and kick a ball to each other, trying not to let the ball drop to the ground (much like keepie uppie). The game appears to have died out sometime before the mid-19th century. It was revived in 1903 and is now played at a number of festivals.There are a number of references to traditional, ancient, or prehistoric ball games, played by indigenous peoples in many different parts of the world. For example, in 1586, men from a ship commanded by an English explorer named John Davis, went ashore to play a form of football with Inuit (Eskimo) people in Greenland. There are later accounts of an Inuit game played on ice, called Aqsaqtuk. Each match began with two teams facing each other in parallel lines, before attempting to kick the ball through each other team's line and then at a goal. In 1610, William Strachey, a colonist at Jamestown, Virginia recorded a game played by Native Americans, called Pahsaheman. On the Australian continent several tribes of indigenous people played kicking and catching games with stuffed balls which have been generalised by historians as Marn Grook (Djab Wurrung for "game ball"). The earliest historical account is an anecdote from the 1878 book by Robert Brough-Smyth, The Aborigines of Victoria, in which a man called Richard Thomas is quoted as saying, in about 1841 in Victoria, Australia, that he had witnessed Aboriginal people playing the game: "Mr Thomas describes how the foremost player will drop kick a ball made from the skin of a possum and how other players leap into the air in order to catch it." Some historians have theorised that Marn Grook was one of the origins of Australian rules football.The Māori in New Zealand played a game called Ki-o-rahi consisting of teams of seven players play on a circular field divided into zones, and score points by touching the 'pou' (boundary markers) and hitting a central 'tupu' or target.Games played in Mesoamerica with rubber balls by indigenous peoples are also well-documented as existing since before this time, but these had more similarities to basketball or volleyball, and no links have been found between such games and modern football sports. Northeastern American Indians, especially the Iroquois Confederation, played a game which made use of net racquets to throw and catch a small ball; however, although it is a ball-goal foot game, lacrosse (as its modern descendant is called) is likewise not usually classed as a form of "football."These games and others may well go far back into antiquity. However, the main sources of modern football codes appear to lie in western Europe, especially England. Medieval and early modern Europe The Middle Ages saw a huge rise in popularity of annual Shrovetide football matches throughout Europe, particularly in England. An early reference to a ball game played in Britain comes from the 9th century Historia Brittonum, which describes "a party of boys ... playing at ball". References to a ball game played in northern France known as La Soule or Choule, in which the ball was propelled by hands, feet, and sticks, date from the 12th century.The early forms of football played in England, sometimes referred to as "mob football", would be played between neighbouring towns and villages, involving an unlimited number of players on opposing teams who would clash en masse, struggling to move an item, such as inflated animal's bladder to particular geographical points, such as their opponents' church, with play taking place in the open space between neighbouring parishes. The game was played primarily during significant religious festivals, such as Shrovetide, Christmas, or Easter, and Shrovetide games have survived into the modern era in a number of English towns (see below).The first detailed description of what was almost certainly football in England was given by William FitzStephen in about 1174–1183. He described the activities of London youths during the annual festival of Shrove Tuesday:After lunch all the youth of the city go out into the fields to take part in a ball game. The students of each school have their own ball; the workers from each city craft are also carrying their balls. Older citizens, fathers, and wealthy citizens come on horseback to watch their juniors competing, and to relive their own youth vicariously: you can see their inner passions aroused as they watch the action and get caught up in the fun being had by the carefree adolescents.Most of the very early references to the game speak simply of "ball play" or "playing at ball". This reinforces the idea that the games played at the time did not necessarily involve a ball being kicked.An early reference to a ball game that was probably football comes from 1280 at Ulgham, Northumberland, England: "Henry... while playing at ball.. ran against David". Football was played in Ireland in 1308, with a documented reference to John McCrocan, a spectator at a "football game" at Newcastle, County Down being charged with accidentally stabbing a player named William Bernard. Another reference to a football game comes in 1321 at Shouldham, Norfolk, England: "[d]uring the game at ball as he kicked the ball, a lay friend of his... ran against him and wounded himself".In 1314, Nicholas de Farndone, Lord Mayor of the City of London issued a decree banning football in the French used by the English upper classes at the time. A translation reads: "[f]orasmuch as there is great noise in the city caused by hustling over large foot balls [rageries de grosses pelotes de pee] in the fields of the public from which many evils might arise which God forbid: we command and forbid on behalf of the king, on pain of imprisonment, such game to be used in the city in the future." This is the earliest reference to football.In 1363, King Edward III of England issued a proclamation banning "...handball, football, or hockey; coursing and cock-fighting, or other such idle games", showing that "football" – whatever its exact form in this case – was being differentiated from games involving other parts of the body, such as handball.A game known as "football" was played in Scotland as early as the 15th century: it was prohibited by the Football Act 1424 and although the law fell into disuse it was not repealed until 1906. There is evidence for schoolboys playing a "football" ball game in Aberdeen in 1633 (some references cite 1636) which is notable as an early allusion to what some have considered to be passing the ball. The word "pass" in the most recent translation is derived from "huc percute" (strike it here) and later "repercute pilam" (strike the ball again) in the original Latin. It is not certain that the ball was being struck between members of the same team. The original word translated as "goal" is "metum", literally meaning the "pillar at each end of the circus course" in a Roman chariot race. There is a reference to "get hold of the ball before [another player] does" (Praeripe illi pilam si possis agere) suggesting that handling of the ball was allowed. One sentence states in the original 1930 translation "Throw yourself against him" (Age, objice te illi).King Henry IV of England also presented one of the earliest documented uses of the English word "football", in 1409, when he issued a proclamation forbidding the levying of money for "foteball".There is also an account in Latin from the end of the 15th century of football being played at Cawston, Nottinghamshire. This is the first description of a "kicking game" and the first description of dribbling: "[t]he game at which they had met for common recreation is called by some the foot-ball game. It is one in which young men, in country sport, propel a huge ball not by throwing it into the air but by striking it and rolling it along the ground, and that not with their hands but with their feet... kicking in opposite directions" The chronicler gives the earliest reference to a football pitch, stating that: "[t]he boundaries have been marked and the game had started.Other firsts in the mediæval and early modern eras:"a football", in the sense of a ball rather than a game, was first mentioned in 1486. This reference is in Dame Juliana Berners' Book of St Albans. It states: "a certain rounde instrument to play with ...it is an instrument for the foote and then it is calde in Latyn 'pila pedalis', a fotebal."a pair of football boots was ordered by King Henry VIII of England in 1526.women playing a form of football was first described in 1580 by Sir Philip Sidney in one of his poems: "[a] tyme there is for all, my mother often sayes, When she, with skirts tuckt very hy, with girles at football playes."the first references to goals are in the late 16th and early 17th centuries. In 1584 and 1602 respectively, John Norden and Richard Carew referred to "goals" in Cornish hurling. Carew described how goals were made: "they pitch two bushes in the ground, some eight or ten foote asunder; and directly against them, ten or twelue [twelve] score off, other twayne in like distance, which they terme their Goales". He is also the first to describe goalkeepers and passing of the ball between players.the first direct reference to scoring a goal is in John Day's play The Blind Beggar of Bethnal Green (performed circa 1600; published 1659): "I'll play a gole at camp-ball" (an extremely violent variety of football, which was popular in East Anglia). Similarly in a poem in 1613, Michael Drayton refers to "when the Ball to throw, And drive it to the Gole, in squadrons forth they goe". Calcio Fiorentino In the 16th century, the city of Florence celebrated the period between Epiphany and Lent by playing a game which today is known as "calcio storico" ("historic kickball") in the Piazza Santa Croce. The young aristocrats of the city would dress up in fine silk costumes and embroil themselves in a violent form of football. For example, calcio players could punch, shoulder charge, and kick opponents. Blows below the belt were allowed. The game is said to have originated as a military training exercise. In 1580, Count Giovanni de' Bardi di Vernio wrote Discorso sopra 'l giuoco del Calcio Fiorentino. This is sometimes said to be the earliest code of rules for any football game. The game was not played after January 1739 (until it was revived in May 1930). Official disapproval and attempts to ban football There have been many attempts to ban football, from the middle ages through to the modern day. The first such law was passed in England in 1314; it was followed by more than 30 in England alone between 1314 and 1667. Football faced armed opposition in the 18th Century when used as a cover for violent protest against the enclosure act. Women were banned from playing at English and Scottish Football League grounds in 1921, a ban that was only lifted in the 1970s. Female footballers still face similar problems in some parts of the world. Establishment of modern codes  English public schools While football continued to be played in various forms throughout Britain, its public schools (known as private schools in other countries) are widely credited with four key achievements in the creation of modern football codes. First of all, the evidence suggests that they were important in taking football away from its "mob" form and turning it into an organised team sport. Second, many early descriptions of football and references to it were recorded by people who had studied at these schools. Third, it was teachers, students and former students from these schools who first codified football games, to enable matches to be played between schools. Finally, it was at English public schools that the division between "kicking" and "running" (or "carrying") games first became clear.The earliest evidence that games resembling football were being played at English public schools – mainly attended by boys from the upper, upper-middle and professional classes – comes from the Vulgaria by William Herman in 1519. Herman had been headmaster at Eton and Winchester colleges and his Latin textbook includes a translation exercise with the phrase "We wyll playe with a ball full of wynde".Richard Mulcaster, a student at Eton College in the early 16th century and later headmaster at other English schools, has been described as "the greatest sixteenth Century advocate of football". Among his contributions are the earliest evidence of organised team football. Mulcaster's writings refer to teams ("sides" and "parties"), positions ("standings"), a referee ("judge over the parties") and a coach "(trayning maister)". Mulcaster's "footeball" had evolved from the disordered and violent forms of traditional football:[s]ome smaller number with such overlooking, sorted into sides and standings, not meeting with their bodies so boisterously to trie their strength: nor shouldring or shuffing one an other so barbarously ... may use footeball for as much good to the body, by the chiefe use of the legges.In 1633, David Wedderburn, a teacher from Aberdeen, mentioned elements of modern football games in a short Latin textbook called Vocabula. Wedderburn refers to what has been translated into modern English as "keeping goal" and makes an allusion to passing the ball ("strike it here"). There is a reference to "get hold of the ball", suggesting that some handling was allowed. It is clear that the tackles allowed included the charging and holding of opposing players ("drive that man back").A more detailed description of football is given in Francis Willughby's Book of Games, written in about 1660. Willughby, who had studied at Bishop Vesey's Grammar School, Sutton Coldfield, is the first to describe goals and a distinct playing field: "a close that has a gate at either end. The gates are called Goals." His book includes a diagram illustrating a football field. He also mentions tactics ("leaving some of their best players to guard the goal"); scoring ("they that can strike the ball through their opponents' goal first win") and the way teams were selected ("the players being equally divided according to their strength and nimbleness"). He is the first to describe a "law" of football: "they must not strike [an opponent's leg] higher than the ball".English public schools were the first to codify football games. In particular, they devised the first offside rules, during the late 18th century. In the earliest manifestations of these rules, players were "off their side" if they simply stood between the ball and the goal which was their objective. Players were not allowed to pass the ball forward, either by foot or by hand. They could only dribble with their feet, or advance the ball in a scrum or similar formation. However, offside laws began to diverge and develop differently at each school, as is shown by the rules of football from Winchester, Rugby, Harrow and Cheltenham, during between 1810 and 1850. The first known codes – in the sense of a set of rules – were those of Eton in 1815  and Aldenham in 1825.)During the early 19th century, most working class people in Britain had to work six days a week, often for over twelve hours a day. They had neither the time nor the inclination to engage in sport for recreation and, at the time, many children were part of the labour force. Feast day football played on the streets was in decline. Public school boys, who enjoyed some freedom from work, became the inventors of organised football games with formal codes of rules.Football was adopted by a number of public schools as a way of encouraging competitiveness and keeping youths fit. Each school drafted its own rules, which varied widely between different schools and were changed over time with each new intake of pupils. Two schools of thought developed regarding rules. Some schools favoured a game in which the ball could be carried (as at Rugby, Marlborough and Cheltenham), while others preferred a game where kicking and dribbling the ball was promoted (as at Eton, Harrow, Westminster and Charterhouse). The division into these two camps was partly the result of circumstances in which the games were played. For example, Charterhouse and Westminster at the time had restricted playing areas; the boys were confined to playing their ball game within the school cloisters, making it difficult for them to adopt rough and tumble running games.William Webb Ellis, a pupil at Rugby School, is said to have "with a fine disregard for the rules of football, as played in his time [emphasis added], first took the ball in his arms and ran with it, thus creating the distinctive feature of the rugby game." in 1823. This act is usually said to be the beginning of Rugby football, but there is little evidence that it occurred, and most sports historians believe the story to be apocryphal. The act of 'taking the ball in his arms' is often misinterpreted as 'picking the ball up' as it is widely believed that Webb Ellis' 'crime' was handling the ball, as in modern soccer, however handling the ball at the time was often permitted and in some cases compulsory, the rule for which Webb Ellis showed disregard was running forward with it as the rules of his time only allowed a player to retreat backwards or kick forwards.The boom in rail transport in Britain during the 1840s meant that people were able to travel further and with less inconvenience than they ever had before. Inter-school sporting competitions became possible. However, it was difficult for schools to play each other at football, as each school played by its own rules. The solution to this problem was usually that the match be divided into two halves, one half played by the rules of the host "home" school, and the other half by the visiting "away" school.The modern rules of many football codes were formulated during the mid- or late- 19th century. This also applies to other sports such as lawn bowls, lawn tennis, etc. The major impetus for this was the patenting of the world's first lawnmower in 1830. This allowed for the preparation of modern ovals, playing fields, pitches, grass courts, etc.Apart from Rugby football, the public school codes have barely been played beyond the confines of each school's playing fields. However, many of them are still played at the schools which created them (see Surviving UK school games below).Public schools' dominance of sports in the UK began to wane after the Factory Act of 1850, which significantly increased the recreation time available to working class children. Before 1850, many British children had to work six days a week, for more than twelve hours a day. From 1850, they could not work before 6 a.m. (7 a.m. in winter) or after 6 p.m. on weekdays (7 p.m. in winter); on Saturdays they had to cease work at 2 p.m. These changes mean that working class children had more time for games, including various forms of football. Firsts  Clubs Sports clubs dedicated to playing football began in the 18th century, for example London's Gymnastic Society which was founded in the mid-18th century and ceased playing matches in 1796.The first documented club to bear in the title a reference to being a 'football club' were called "The Foot-Ball Club" who were located in Edinburgh, Scotland, during the period 1824–41. The club forbade tripping but allowed pushing and holding and the picking up of the ball.In 1845, three boys at Rugby school were tasked with codifying the rules then being used at the school. These were the first set of written rules (or code) for any form of football. This further assisted the spread of the Rugby game. Competitions One of the longest running football fixture is the Cordner-Eggleston Cup, contested between Melbourne Grammar School and Scotch College, Melbourne every year since 1858. It is believed by many to also be the first match of Australian rules football, although it was played under experimental rules in its first year. The first football trophy tournament was the Caledonian Challenge Cup, donated by the Royal Caledonian Society of Melbourne, played in 1861 under the Melbourne Rules. The oldest football league is a rugby football competition, the United Hospitals Challenge Cup (1874), while the oldest rugby trophy is the Yorkshire Cup, contested since 1878. The South Australian Football Association (30 April 1877) is the oldest surviving Australian rules football competition. The oldest surviving soccer trophy is the Youdan Cup (1867) and the oldest national soccer competition is the English FA Cup (1871). The Football League (1888) is recognised as the longest running Association Football league. The first ever international football match took place between sides representing England and Scotland on March 5, 1870 at the Oval under the authority of the FA. The first Rugby international took place in 1871. Modern balls In Europe, early footballs were made out of animal bladders, more specifically pig's bladders, which were inflated. Later leather coverings were introduced to allow the balls to keep their shape. However, in 1851, Richard Lindon and William Gilbert, both shoemakers from the town of Rugby (near the school), exhibited both round and oval-shaped balls at the Great Exhibition in London. Richard Lindon's wife is said to have died of lung disease caused by blowing up pig's bladders. Lindon also won medals for the invention of the "Rubber inflatable Bladder" and the "Brass Hand Pump".In 1855, the U.S. inventor Charles Goodyear – who had patented vulcanised rubber – exhibited a spherical football, with an exterior of vulcanised rubber panels, at the Paris Exhibition Universelle. The ball was to prove popular in early forms of football in the U.S.A.The iconic ball with a regular pattern of hexagons and pentagons (see truncated icosahedron) did not become popular until the 1960s, and was first used in the World Cup in 1970. Modern ball passing tactics The earliest reference to a game of football involving players passing the ball and attempting to score past a goalkeeper was written in 1633 by David Wedderburn, a poet and teacher in Aberdeen, Scotland. Nevertheless, the original text does not state whether the allusion to passing as 'kick the ball back' ('Repercute pilam') was in a forward or backward direction or between members of the same opposing teams (as was usual at this time)"Scientific" football is first recorded in 1839 from Lancashire and in the modern game in Rugby football from 1862 and from Sheffield FC as early as 1865. The first side to play a passing combination game was the Royal Engineers AFC in 1869/70 By 1869 they were "work[ing] well together", "backing up" and benefiting from "cooperation". By 1870 the Engineers were passing the ball: "Lieut. Creswell, who having brought the ball up the side then kicked it into the middle to another of his side, who kicked it through the posts the minute before time was called" Passing was a regular feature of their style By early 1872 the Engineers were the first football team renowned for "play[ing] beautifully together" A double pass is first reported from Derby school against Nottingham Forest in March 1872, the first of which is irrefutably a short pass: "Mr Absey dribbling the ball half the length of the field delivered it to Wallis, who kicking it cleverly in front of the goal, sent it to the captain who drove it at once between the Nottingham posts" The first side to have perfected the modern formation was Cambridge University AFC and introduced the 2–3–5 "pyramid" formation. Cambridge rules In 1848, at Cambridge University, Mr. H. de Winton and Mr. J.C. Thring, who were both formerly at Shrewsbury School, called a meeting at Trinity College, Cambridge with 12 other representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury. An eight-hour meeting produced what amounted to the first set of modern rules, known as the Cambridge rules. No copy of these rules now exists, but a revised version from circa 1856 is held in the library of Shrewsbury School. The rules clearly favour the kicking game. Handling was only allowed when a player catches the ball directly from the foot entitling them to a free kick and there was a primitive offside rule, disallowing players from "loitering" around the opponents' goal. The Cambridge rules were not widely adopted outside English public schools and universities (but it was arguably the most significant influence on the Football Association committee members responsible for formulating the rules of Association football). Sheffield rules By the late 1850s, many football clubs had been formed throughout the English-speaking world, to play various codes of football. Sheffield Football Club, founded in 1857 in the English city of Sheffield by Nathaniel Creswick and William Prest, was later recognised as the world's oldest club playing association football. However, the club initially played its own code of football: the Sheffield rules. The code was largely independent of the public school rules, the most significant difference being the lack of an offside rule.The code was responsible for many innovations that later spread to association football. These included free kicks, corner kicks, handball, throw-ins and the crossbar. By the 1870s they became the dominant code in the north and midlands of England. At this time a series of rule changes by both the London and Sheffield FAs gradually eroded the differences between the two games until the adoption of a common code in 1877. Australian rules There is archival evidence of "foot-ball" games being played in various parts of Australia throughout the first half of the 19th century. The origins of an organised game of football known today as Australian rules football can be traced back to 1858 in Melbourne, the capital city of Victoria.In July 1858, Tom Wills, an Australian-born cricketer educated at Rugby School in England, wrote a letter to Bell's Life in Victoria & Sporting Chronicle, calling for a "foot-ball club" with a "code of laws" to keep cricketers fit during winter. This is considered by historians to be a defining moment in the creation of Australian rules football. Through publicity and personal contacts Wills was able to co-ordinate football matches in Melbourne that experimented with various rules, the first of which was played on July 31, 1858. One week later, Wills umpired a schoolboys match between Melbourne Grammar School and Scotch College. Following these matches, organised football in Melbourne rapidly increased in popularity.Wills and others involved in these early matches formed the Melbourne Football Club (the oldest surviving Australian football club) on May 14, 1859. Club members Wills, William Hammersley, J. B. Thompson and Thomas H. Smith met with the intention of forming a set of rules that would be widely adopted by other clubs. The committee debated rules used in English public school games; Wills pushed for various rugby football rules he learnt during his schooling. The first rules share similarities with these games, and were shaped to suit to Australian conditions. H. C. A. Harrison, a seminal figure in Australian football, recalled that his cousin Wills wanted "a game of our own". The code was distinctive in the prevalence of the mark, free kick, tackling, lack of an offside rule and that players were specifically penalised for throwing the ball.The Melbourne football rules were widely distributed and gradually adopted by the other Victorian clubs. The rules were updated several times during the 1860s to accommodate the rules of other influential Victorian football clubs. A significant redraft in 1866 by H. C. A. Harrison's committee accommodated the Geelong Football Club's rules, making the game then known as "Victorian Rules" increasingly distinct from other codes. It soon adopted cricket fields and an oval ball, used specialised goal and behind posts, and featured bouncing the ball while running and spectacular high marking. The game spread quickly to other Australian colonies. Outside of its heartland in southern Australia the code experienced a significant period of decline following World War I but has since grown throughout Australia and in other parts of the world, and the Australian Football League emerged as the dominant professional competition. Football Association During the early 1860s, there were increasing attempts in England to unify and reconcile the various public school games. In 1862, J. C. Thring, who had been one of the driving forces behind the original Cambridge Rules, was a master at Uppingham School and he issued his own rules of what he called "The Simplest Game" (these are also known as the Uppingham Rules). In early October 1863 another new revised version of the Cambridge Rules was drawn up by a seven member committee representing former pupils from Harrow, Shrewsbury, Eton, Rugby, Marlborough and Westminster.At the Freemasons' Tavern, Great Queen Street, London on the evening of October 26, 1863, representatives of several football clubs in the London Metropolitan area met for the inaugural meeting of The Football Association (FA). The aim of the Association was to establish a single unifying code and regulate the playing of the game among its members. Following the first meeting, the public schools were invited to join the association. All of them declined, except Charterhouse and Uppingham. In total, six meetings of the FA were held between October and December 1863. After the third meeting, a draft set of rules were published. However, at the beginning of the fourth meeting, attention was drawn to the recently published Cambridge Rules of 1863. The Cambridge rules differed from the draft FA rules in two significant areas; namely running with (carrying) the ball and hacking (kicking opposing players in the shins). The two contentious FA rules were as follows:IX. A player shall be entitled to run with the ball towards his adversaries' goal if he makes a fair catch, or catches the ball on the first bound; but in case of a fair catch, if he makes his mark he shall not run.X. If any player shall run with the ball towards his adversaries' goal, any player on the opposite side shall be at liberty to charge, hold, trip or hack him, or to wrest the ball from him, but no player shall be held and hacked at the same time.At the fifth meeting it was proposed that these two rules be removed. Most of the delegates supported this, but F. M. Campbell, the representative from Blackheath and the first FA treasurer, objected. He said: "hacking is the true football". However, the motion to ban running with the ball in hand and hacking was carried and Blackheath withdrew from the FA. After the final meeting on 8 December, the FA published the "Laws of Football", the first comprehensive set of rules for the game later known as Association Football. The term "soccer", in use since the late 19th century, derives from an Oxford University abbreviation of "Association".The first FA rules still contained elements that are no longer part of association football, but which are still recognisable in other games (such as Australian football and rugby football): for instance, a player could make a fair catch and claim a mark, which entitled him to a free kick; and if a player touched the ball behind the opponents' goal line, his side was entitled to a free kick at goal, from 15 yards (13.5 metres) in front of the goal line. Rugby football In Britain, by 1870, there were about 75 clubs playing variations of the Rugby school game. There were also "rugby" clubs in Ireland, Australia, Canada and New Zealand. However, there was no generally accepted set of rules for rugby until 1871, when 21 clubs from London came together to form the Rugby Football Union (RFU). The first official RFU rules were adopted in June 1871. These rules allowed passing the ball. They also included the try, where touching the ball over the line allowed an attempt at goal, though drop-goals from marks and general play, and penalty conversions were still the main form of contest. North American football codes As was the case in Britain, by the early 19th century, North American schools and universities played their own local games, between sides made up of students. For example, students at Dartmouth College in New Hampshire played a game called Old division football, a variant of the association football codes, as early as the 1820s. They remained largely "mob football" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common. The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale University, under pressure from the city of New Haven, banned the play of all forms of football in 1860, while Harvard University followed suit in 1861. In its place, two general types of football evolved: "kicking" games and "running" (or "carrying") games. A hybrid of the two, known as the "Boston game", was played by a group known as the Oneida Football Club. The club, considered by some historians as the first formal football club in the United States, was formed in 1862 by schoolboys who played the "Boston game" on Boston Common. The game began to return to American college campuses by the late 1860s. The universities of Yale, Princeton (then known as the College of New Jersey), Rutgers, and Brown all began playing "kicking" games during this time. In 1867, Princeton used rules based on those of the English Football Association.In Canada, the first documented football match was a practice game played on November 9, 1861, at University College, University of Toronto (approximately 400 yards west of Queen's Park). One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football. A "running game", resembling rugby football, was then taken up by the Montreal Football Club in Canada in 1868.On November 6, 1869, Rutgers faced Princeton in a game that was played with a round ball and, like all early games, used improvised rules. It is usually regarded as the first game of American intercollegiate football.Modern North American football grew out of a match between McGill University of Montreal, and Harvard University in 1874. During the game, the two teams alternated between the rugby-based rules used by McGill and the Boston Game rules used by Harvard. Within a few years, Harvard had both adopted McGill's rules and had persuaded other U.S. university teams to do the same. On November 23, 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit Convention in Springfield, Massachusetts, agreeing to adopt most of the Rugby Football Union rules, with some variations.In 1880, Yale coach Walter Camp, who had become a fixture at the Massasoit House conventions where the rules were debated and changed, devised a number of major innovations. Camp's two most important rule changes that diverged the American game from rugby was replacing the scrummage with the line of scrimmage and the establishment of the down-and-distance rules. American football still however remained a violent sport where collisions often led to serious injuries and sometimes even death. This led U.S. President Theodore Roosevelt to hold a meeting with football representatives from Harvard, Yale, and Princeton on October 9, 1905, urging them to make drastic changes. One rule change introduced in 1906, devised to open up the game and reduce injury, was the introduction of the legal forward pass. Though it was underutilised for years, this proved to be one of the most important rule changes in the establishment of the modern game.Over the years, Canada absorbed some of the developments in American football in an effort to distinguish it from a more rugby-oriented game. In 1903, the Ontario Rugby Football Union adopted the Burnside rules, which implemented the line of scrimmage and down-and-distance system from American football, among others. Canadian football then implemented the legal forward pass in 1929. American and Canadian football remain different codes, stemming from rule changes that the American side of the border adopted but the Canadian side has not. Gaelic football In the mid-19th century, various traditional football games, referred to collectively as caid, remained popular in Ireland, especially in County Kerry. One observer, Father W. Ferris, described two main forms of caid during this period: the "field game" in which the object was to put the ball through arch-like goals, formed from the boughs of two trees; and the epic "cross-country game" which took up most of the daylight hours of a Sunday on which it was played, and was won by one team taking the ball across a parish boundary. "Wrestling", "holding" opposing players, and carrying the ball were all allowed.By the 1870s, Rugby and Association football had started to become popular in Ireland. Trinity College, Dublin was an early stronghold of Rugby (see the Developments in the 1850s section, above). The rules of the English FA were being distributed widely. Traditional forms of caid had begun to give way to a "rough-and-tumble game" which allowed tripping.There was no serious attempt to unify and codify Irish varieties of football, until the establishment of the Gaelic Athletic Association (GAA) in 1884. The GAA sought to promote traditional Irish sports, such as hurling and to reject imported games like Rugby and Association football. The first Gaelic football rules were drawn up by Maurice Davin and published in the United Ireland magazine on February 7, 1887. Davin's rules showed the influence of games such as hurling and a desire to formalise a distinctly Irish code of football. The prime example of this differentiation was the lack of an offside rule (an attribute which, for many years, was shared only by other Irish games like hurling, and by Australian rules football). Schism in Rugby football The International Rugby Football Board (IRFB) was founded in 1886, but rifts were beginning to emerge in the code. Professionalism had already began to creep into the various codes of football.In England, by the 1890s, a long-standing Rugby Football Union ban on professional players was causing regional tensions within rugby football, as many players in northern England were working class and could not afford to take time off to train, travel, play and recover from injuries. This was not very different from what had occurred ten years earlier in soccer in Northern England but the authorities reacted very differently in the RFU, attempting to alienate the working class support in Northern England. In 1895, following a dispute about a player being paid broken time payments, which replaced wages lost as a result of playing rugby, representatives of the northern clubs met in Huddersfield to form the Northern Rugby Football Union (NRFU). The new body initially permitted only various types of player wage replacements. However, within two years, NRFU players could be paid, but they were required to have a job outside sport.The demands of a professional league dictated that rugby had to become a better "spectator" sport. Within a few years the NRFU rules had started to diverge from the RFU, most notably with the abolition of the line-out. This was followed by the replacement of the ruck with the "play-the-ball ruck", which allowed a two-player ruck contest between the tackler at marker and the player tackled. Mauls were stopped once the ball carrier was held, being replaced by a play-the ball-ruck. The separate Lancashire and Yorkshire competitions of the NRFU merged in 1901, forming the Northern Rugby League, the first time the name rugby league was used officially in England.Over time, the RFU form of rugby, played by clubs which remained members of national federations affiliated to the IRFB, became known as rugby union. Globalisation of association football The need for a single body to oversee association football had become apparent by the beginning of the 20th century, with the increasing popularity of international fixtures. The English Football Association had chaired many discussions on setting up an international body, but was perceived as making no progress. It fell to associations from seven other European countries: France, Belgium, Denmark, Netherlands, Spain, Sweden, and Switzerland, to form an international association. The Fédération Internationale de Football Association (FIFA) was founded in Paris on May 21, 1904. Its first president was Robert Guérin. The French name and acronym has remained, even outside French-speaking countries. Further divergence of the two rugby codes Rugby league rules diverged significantly from rugby union in 1906, with the reduction of the team from 15 to 13 players. In 1907, a New Zealand professional rugby team toured Australia and Britain, receiving an enthusiastic response, and professional rugby leagues were launched in Australia the following year. However, the rules of professional games varied from one country to another, and negotiations between various national bodies were required to fix the exact rules for each international match. This situation endured until 1948, when at the instigation of the French league, the Rugby League International Federation (RLIF) was formed at a meeting in Bordeaux.During the second half of 20th century, the rules changed further. In 1966, rugby league officials borrowed the American football concept of downs: a team was allowed to retain possession of the ball for four tackles (rugby union retains the original rule that a player who is tackled and brought to the ground must release the ball immediately). The maximum number of tackles was later increased to six (in 1971), and in rugby league this became known as the six tackle rule.With the advent of full-time professionals in the early 1990s, and the consequent speeding up of the game, the five metre off-side distance between the two teams became 10 metres, and the replacement rule was superseded by various interchange rules, among other changes.The laws of rugby union also changed during the 20th century, although less significantly than those of rugby league. In particular, goals from marks were abolished, kicks directly into touch from outside the 22 metre line were penalised, new laws were put in place to determine who had possession following an inconclusive ruck or maul, and the lifting of players in line-outs was legalised.In 1995, rugby union became an "open" game, that is one which allowed professional players. Although the original dispute between the two codes has now disappeared – and despite the fact that officials from both forms of rugby football have sometimes mentioned the possibility of re-unification – the rules of both codes and their culture have diverged to such an extent that such an event is unlikely in the foreseeable future. Use of the word "football" The word "football", when used in reference to a specific game can mean any one of those described above. Because of this, much friendly controversy has occurred over the term football, primarily because it is used in different ways in different parts of the English-speaking world. Most often, the word "football" is used to refer to the code of football that is considered dominant within a particular region. So, effectively, what the word "football" means usually depends on where one says it.In each of the United Kingdom, the United States, and Canada, one football code is known solely as "football", while the others generally require a qualifier. In New Zealand, "football" historically referred to rugby union, but more recently may be used unqualified to refer to association football. The sport meant by the word "football" in Australia is either Australian rules football or rugby league, depending on popularity (which largely conforms to what has been called the Barassi Line). In francophone Quebec, where Canadian football is more popular, the Canadian code is known as football while American football is known as Football américain and association football is known as le soccer. Of the 45 national FIFA (Fédération Internationale de Football Association) affiliates in which English is an official or primary language, most currently use Football in their organisations' official names; the FIFA affiliates in Canada and the United States use Soccer in their names. A few FIFA affiliates have recently "normalised" to using "Football", including:Australia's association football governing body changed its name in 2005 from using "soccer" to "football"New Zealand's governing body also changed in 2007, saying "the international game is called football."Samoa changed from "Samoa Football (Soccer) Federation" to "Football Federation Samoa" in 2009. Popularity Several of the football codes are the most popular team sports in the world. Globally, association football is played by over 250 million players in over 200 nations, and has the highest television audience in sport, making it the most popular in the world, American football is the most popular sport in the United States, with the annual Super Bowl game accounting for seven of the top eight of the most watched broadcasts in U.S. television history. Australian rules football has the highest spectator attendance of all sports in Australia. Similarly, Gaelic football is the most popular sport in Ireland in terms of match attendance, and the All-Ireland Football Final is the most watched event of that nation's sporting year. Football codes board  Football codes development tree  Present day codes and families  Association football and descendants These codes have in common the prohibition of the use of hands (by all players except the goalkeeper), unlike other codes where carrying or handling the ball is allowedAssociation football, also known as football, soccer, footy and footieIndoor/basketball court variants:Five-a-side football – played throughout the world under various rules including:Futebol de SalãoFutsal – the FIFA-approved five-a-side indoor gameMinivoetbal – the five-a-side indoor game played in East and West Flanders where it is extremely popularPapi fut – the five-a-side game played in outdoor basketball courts (built with goals) in Central America.Indoor soccer – the six-a-side indoor game, the Latin American variant (fútbol rápido, "fast football") is often played in open-air venuesMasters Football – six-a-side played in Europe by mature professionals (35 years and older)Paralympic football – modified game for athletes with a disability. Includes:Football 5-a-side – for visually impaired athletesFootball 7-a-side – for athletes with cerebral palsyAmputee football – for athletes with amputationsDeaf football – for athletes with hearing impairmentsPowerchair football – for athletes in electric wheelchairsBeach soccer, beach football or sand soccer – variant modified for play on sandStreet football – encompasses a number of informal variantsRush goalie – a variation in which the role of the goalkeeper is more flexible than normalHeaders and Volleys – where the aim is to score goals against a goalkeeper using only headers and volleysCrab football – players stand on their hands and feet and move around on their backs whilst playingSwamp soccer – the game as played on a swamp or bog fieldJorkyballRushballThere are also motorsport variations of the game. Rugby school football and descendants These codes have in common the ability of players to carry the ball with their hands, and to throw it to teammates, unlike association football where the use of hands is prohibited by anyone except the goal keeper. They also feature various methods of scoring based upon whether the ball is carried into the goal area, or kicked through a target.Rugby footballRugby unionMini rugby a variety for children.Rugby sevens and Rugby tens – variants for teams of reduced size.Rugby league – often referred to simply as "league", and usually known simply as "football" or "footy" in the Australian states of New South Wales and Queensland.Rugby league sevens and Rugby league nines – variant for teams of reduced size.Beach rugby – rugby played on sandTouch rugby – generic name for forms of rugby football which do not feature tackles, one variant has been formalisedTag Rugby – non-contact variant in which a flag attached to a player is removed to indicate a tackle.Gridiron footballAmerican football – called "football" in the United States and Canada, and "gridiron" in Australia and New Zealand.Nine-man football, eight-man football, six-man football – variants played primarily by smaller high schools that lack enough players to field full teams.Street football/backyard football – played without equipment or official fields and with simplified rulesTouch football – non-tackle variantsCanadian football – called simply "football" in Canada; "football" in Canada can mean either Canadian or American football depending on context. All of the variants listed for American football are also attested for Canadian football.Flag football – non-contact variant in which a flag attached to a player is removed to indicate a tackle.Indoor football, arena football – indoor variants Irish and Australian varieties These codes have in common the absence of an offside rule, the prohibition of continuous carrying of the ball (requiring a periodic bounce or solo (toe-kick), depending on the code) while running, handpassing by punching or tapping the ball rather than throwing it, and other traditions.Australian rules football – officially known as "Australian football", and informally as "football", "footy" or "Aussie rules". In some areas it is referred to as "AFL", the name of the main organising body and competitionAuskick – a version of Australian rules designed by the AFL for young childrenMetro footy (or Metro rules footy) – a modified version invented by the USAFL, for use on gridiron fields in North American cities (which often lack grounds large enough for conventional Australian rules matches)Kick-to-kick – informal versions of the game9-a-side footy – a more open, running variety of Australian rules, requiring 18 players in total and a proportionally smaller playing area (includes contact and non-contact varieties)Rec footy – "Recreational Football", a modified non-contact variation of Australian rules, created by the AFL, which replaces tackles with tagsTouch Aussie Rules – a non-tackle variation of Australian Rules played only in the United KingdomSamoa rules – localised version adapted to Samoan conditions, such as the use of rugby football fieldsMasters Australian football (a.k.a. Superules) – reduced contact version introduced for competitions limited to players over 30 years of ageWomen's Australian rules football – women's competition played with a smaller ball and (sometimes) reduced contactGaelic football – Played predominantly in Ireland. Commonly referred to as "football" or "Gaelic"Ladies Gaelic footballInternational rules football – a compromise code used for games between Gaelic and Australian Rules players Surviving medieval ball games  Inside the UK The Haxey Hood, played on Epiphany in Haxey, LincolnshireShrove Tuesday gamesScoring the Hales in Alnwick, NorthumberlandRoyal Shrovetide Football in Ashbourne, DerbyshireThe Shrovetide Ball Game in Atherstone, WarwickshireThe Shrove Tuesday Football Ceremony of the Purbeck Marblers in Corfe Castle, DorsetHurling the Silver Ball at St Columb Major in CornwallThe Ball Game in Sedgefield, County DurhamIn Scotland the Ba game ("Ball Game") is still popular around Christmas and Hogmanay at:Duns, BerwickshireScone, PerthshireKirkwall in the Orkney Islands Outside the UK Calcio Fiorentino – a modern revival of Renaissance football from 16th century Florence.la Soule – a modern revival of French medieval footballlelo burti – a Georgian traditional football game Surviving UK school games Games still played at UK public (independent) schools:Eton field gameEton wall gameHarrow footballWinchester College football Recent inventions and hybrid games Keepie uppie (keep up) – the art of juggling with a football using the feet, knees, chest, shoulders, and head.Footbag – several variations using a small bean bag or sand bag as a ball, the trade marked term hacky sack is sometimes used as a generic synonym.Freestyle football – participants are graded for their entertainment value and expression of skill. Based on FA rules Three sided footballTriskelion Based on rugby Force ’em backs a.k.a. forcing back, forcemanback Hybrid games Austus – a compromise between Australian rules and American football, invented in Melbourne during World War II.Bossaball – mixes Association football and volleyball and gymnastics; played on inflatables and trampolines.Cycle ball − a sport similar to association football played on bicyclesFootvolley – mixes Association football and beach volleyball; played on sandFootball tennis – mixes Association football and tennisKickball – a hybrid of Association football and baseball, invented in the United States in about 1942.Speedball – a combination of American football, soccer, and basketball, devised in the United States in 1912.Universal football – a hybrid of Australian rules and rugby league, trialled in Sydney in 1933.Volata – a game resembling Association football and European handball, devised by Italian fascist leader, Augusto Turati, in the 1920s.Wheelchair rugby – also known as Murderball, invented in Canada in 1977. Based on ice hockey and basketball rather than rugby.Note: although similar to football and volleyball in some aspects, Sepak takraw has ancient origins and cannot be considered a hybrid game. Tabletop games, video games and other recreations  Based on Association football SubbuteoBlow footballTable football – also known as foosball, table soccer, babyfoot, bar football or gettoneFantasy football (soccer)Button football – also known as Futebol de Mesa, Jogo de BotõesPenny footballFIFA Video Games SeriesPro Evolution SoccerMario StrikersLego Football Based on American football Paper footballBlood BowlFantasy football (American)Madden NFL Based on Australian football AFL video game seriesList of AFL video games Based on Rugby League football Sidhe's Rugby League seriesRugby League 3Australian Rugby League See also Football field (unit of length)List of types of footballList of players who have converted from one football code to anotherNames for association football1601 to 1725 in sports: FootballFootgolfUnderwater football Notes  References Eisenberg, Christiane and Pierre Lanfranchi, eds. (2006): Football History: International Perspectives; Special Issue, Historical Social Research 31, no. 1. 312 pages.Green, Geoffrey (1953); The History of the Football Association; Naldrett Press, LondonMandelbaum, Michael (2004); The Meaning of Sports; Public Affairs, ISBN 1-58648-252-1Williams, Graham (1994); The Code War; Yore Publications, ISBN 1-874427-65-8
Apple is an American multinational technology company headquartered in Cupertino, California that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple smartwatch, and the Apple TV digital media player. Apple's consumer software includes the macOS and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites. Its online services include the iTunes Store, the iOS App Store and Mac App Store, Apple Music, and iCloud.Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell personal computers. It was incorporated as Apple Computer, Inc. in January 1977, and was renamed as Apple Inc. in January 2007 to reflect its shifted focus toward consumer electronics. Apple (NASDAQ: AAPL) joined the Dow Jones Industrial Average in March 2015.Apple is the world's largest information technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer, by volume, after Samsung. In November 2014, Apple became the first U.S. company to be valued at over US$700 billion in addition to being the largest publicly traded corporation in the world by market capitalization. The company employs 115,000 full-time employees as of July 2015 and maintains 478 retail stores in seventeen countries as of March 2016. It operates the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. Consumers use more than one billion Apple products worldwide as of March 2016.Apple's worldwide annual revenue totaled $233 billion for the fiscal year ending in September 2015. This revenue accounts for approximately 1.25% of the total United States GDP. The company enjoys a high level of brand loyalty and, according to Interbrand's annual Best Global Brands report, has been the world's most valuable brand for 4 years in a row, with a valuation in 2016 of $178.1 billion. The corporation receives significant criticism regarding the labor practices of its contractors and its environmental and business practices, including the origins of source materials. History  1976–84: Founding and incorporation Apple was founded on April 1, 1976, by Steve Jobs, Steve Wozniak and Ronald Wayne to sell the Apple I personal computer kits. The Apple I kits were computers single-handedly designed and hand-built by Wozniak and first shown to the public at the Homebrew Computer Club. The Apple I was sold as a motherboard (with CPU, RAM, and basic textual-video chips), which was less than what is now considered a complete personal computer. The Apple I went on sale in July 1976 and was market-priced at $666.66 ($2,806 in 2017 dollars, adjusted for inflation).Apple was incorporated January 3, 1977, without Wayne, who sold his share of the company back to Jobs and Wozniak for $800. Multimillionaire Mike Markkula provided essential business expertise and funding of $250,000 during the incorporation of Apple. During the first five years of operations revenues grew exponentially, doubling about every four months. Between September 1977 and September 1980 yearly sales grew from $775,000 to $118m, an average annual growth rate of 533%.The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and open architecture. While early Apple II models used ordinary cassette tapes as storage devices, they were superseded by the introduction of a 5 1/4 inch floppy disk drive and interface called the Disk II. The Apple II was chosen to be the desktop platform for the first "killer app" of the business world: VisiCalc, a spreadsheet program. VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office. Before VisiCalc, Apple had been a distant third place competitor to Commodore and Tandy.By the end of the 1970s, Apple had a staff of computer designers and a production line. The company introduced the Apple III in May 1980 in an attempt to compete with IBM and Microsoft in the business and corporate computing market. Jobs and several Apple employees, including Jef Raskin, visited Xerox PARC in December 1979 to see the Xerox Alto. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (800,000 split-adjusted shares) of Apple at the pre-IPO price of $10 a share.Jobs was immediately convinced that all future computers would use a graphical user interface (GUI), and development of a GUI began for the Apple Lisa. In 1982, however, he was pushed from the Lisa team due to infighting. Jobs took over Jef Raskin's low-cost-computer project, the Macintosh. A race broke out between the Lisa team and the Macintosh team over which product would ship first. Lisa won the race in 1983 and became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price tag and limited software titles.On December 12, 1980, Apple went public at $22 per share, generating more capital than any IPO since Ford Motor Company in 1956 and immediately creating 300 millionaires. 1984–91: Success with Macintosh In 1984, Apple launched the Macintosh, the first personal computer to be sold without a programming language. Its debut was signified by "1984", a $1.5 million television commercial directed by Ridley Scott that aired during the third quarter of Super Bowl XVIII on January 22, 1984. The commercial is now hailed as a watershed event for Apple's success and was called a "masterpiece" by CNN and one of the greatest commercials of all time by TV Guide.The Macintosh initially sold well, but follow-up sales were not strong due to its high price and limited range of software titles. The machine's fortunes changed with the introduction of the LaserWriter, the first PostScript laser printer to be sold at a reasonable price, and PageMaker, an early desktop publishing package. It has been suggested that the combination of these three products were responsible for the creation of the desktop publishing market. The Macintosh was particularly powerful in the desktop publishing market due to its advanced graphics capabilities, which had necessarily been built in to create the intuitive Macintosh GUI.In 1985, a power struggle developed between Jobs and CEO John Sculley, who had been hired two years earlier. The Apple board of directors instructed Sculley to "contain" Jobs and limit his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from his leadership role at Apple. Sculley found out that Jobs had been attempting to organize a coup and called a board meeting at which Apple's board of directors sided with Sculley and removed Jobs from his managerial duties. Jobs resigned from Apple and founded NeXT Inc. the same year.After Jobs' departure, the Macintosh product line underwent a steady change of focus to higher price points, the so-called "high-right policy" named for the position on a chart of price vs. profits. Jobs had argued the company should produce products aimed at the consumer market and aimed for a $1000 price for the Macintosh, which they were unable to meet. Newer models selling at higher price points offered higher profit margin, and appeared to have no effect on total sales as power users snapped up every increase in power. Although some worried about pricing themselves out of the market, the high-right policy was in full force by the mid-1980s, notably due to Jean-Louis Gassée's mantra of "fifty-five or die", referring to the 55% profit margins of the Macintosh II.This policy began to backfire in the last years of the decade as new desktop publishing programs appeared on PC clones that offered some or much of the same functionality of the Macintosh but at far lower price points. The company lost its monopoly in this market, and had already estranged many of its original consumer customer base who could no longer afford their high priced products. The Christmas season of 1989 was the first in the company's history that saw declining sales, and led to a 20% drop in Apple's stock price. Gassée's objections were overruled, and he was forced from the company in 1990. Later that year, Apple introduced three lower cost models, the Macintosh Classic, Macintosh LC and Macintosh IIsi, all of which saw significant sales due to pent up demand.In 1991, Apple introduced the PowerBook, replacing the "luggable" Macintosh Portable with a design that set the current shape for almost all modern laptops. The same year, Apple introduced System 7, a major upgrade to the operating system which added color to the interface and introduced new networking capabilities. It remained the architectural basis for the Classic Mac OS. The success of the PowerBook and other products brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine MacAddict named the period between 1989 and 1991 as the "first golden age" of the Macintosh.Apple believed the Apple II series was too expensive to produce and took away sales from the low-end Macintosh. In 1990, Apple released the Macintosh LC, which featured a single expansion slot for the Apple IIe Card to help migrate Apple II users to the Macintosh platform; the Apple IIe was discontinued in 1993. 1991–97: Decline and restructuring The success of Apple's lower-cost consumer models, especially the LC, also led to cannibalization of their higher priced machines. To address this, management introduced several new brands, selling largely identical machines at different price points aimed at different markets. These were the high-end Quadra, the mid-range Centris line, and the ill-fated Performa series. This led to significant market confusion, as customers did not understand the difference between models.Apple also experimented with a number of other unsuccessful consumer targeted products during the 1990s, including digital cameras, portable CD audio players, speakers, video consoles, the eWorld online service, and TV appliances. Enormous resources were also invested in the problem-plagued Newton division based on John Sculley's unrealistic market forecasts. Ultimately, none of these products helped and Apple's market share and stock prices continued to slide.Throughout this period, Microsoft continued to gain market share with Windows by focusing on delivering software to cheap commodity personal computers, while Apple was delivering a richly engineered but expensive experience. Apple relied on high profit margins and never developed a clear response; instead, they sued Microsoft for using a GUI similar to the Apple Lisa in Apple Computer, Inc. v. Microsoft Corp. The lawsuit dragged on for years before it was finally dismissed. At this time, a series of major product flops and missed deadlines sullied Apple's reputation, and Sculley was replaced as CEO by Michael Spindler.By the early 1990s, Apple was developing alternative platforms to the Macintosh, such as A/UX. The Macintosh platform itself was becoming outdated because it was not built for multitasking and because several important software routines were programmed directly into the hardware. In addition, Apple was facing competition from OS/2 and UNIX vendors such as Sun Microsystems. The Macintosh would need to be replaced by a new platform or reworked to run on more powerful hardware.In 1994, Apple allied with IBM and Motorola in the AIM alliance with the goal of creating a new computing platform (the PowerPC Reference Platform), which would use IBM and Motorola hardware coupled with Apple software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind and thus counter Microsoft. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use Motorola's PowerPC processor.In 1996, Spindler was replaced by Gil Amelio as CEO. Amelio made numerous changes at Apple, including extensive layoffs and cut costs. After numerous failed attempts to improve Mac OS, first with the Taligent project and later with Copland and Gershwin, Amelio chose to purchase NeXT and its NeXTSTEP operating system and bring Steve Jobs back to Apple. 1997–2007: Return to profitability The NeXT deal was finalized on February 9, 1997, bringing Jobs back to Apple as an advisor. On July 9, 1997, Amelio was ousted by the board of directors after overseeing a three-year record-low stock price and crippling financial losses. Jobs acted as the interim CEO and began restructuring the company's product line; it was during this period that he identified the design talent of Jonathan Ive, and the pair worked collaboratively to rebuild Apple's status.At the 1997 Macworld Expo, Jobs announced that Apple would join Microsoft to release new versions of Microsoft Office for the Macintosh, and that Microsoft had made a $150 million investment in non-voting Apple stock. On November 10, 1997, Apple introduced the Apple Online Store, which was tied to a new build-to-order manufacturing strategy.On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of the Macintosh 128K: the iMac. The iMac design team was led by Ive, who would later design the iPod and the iPhone. The iMac featured modern technology and a unique design, and sold almost 800,000 units in its first five months.During this period, Apple completed numerous acquisitions to create a portfolio of digital production software for both professionals and consumers. In 1998, Apple purchased Macromedia's Key Grip software project, signaling an expansion into the digital video editing market. The sale was an outcome of Macromedia's decision to solely focus upon web development software. The product, still unfinished at the time of the sale, was renamed "Final Cut Pro" when it was launched on the retail market in April 1999. The development of Key Grip also led to Apple's release of the consumer video-editing product iMovie in October 1999. Next, Apple successfully acquired the German company Astarte, which had developed DVD authoring technology, as well as Astarte's corresponding products and engineering team in April 2000. Astarte's digital tool DVDirector was subsequently transformed into the professional-oriented DVD Studio Pro software product. Apple then employed the same technology to create iDVD for the consumer market. In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for the music productivity application Logic. The purchase of Emagic made Apple the first computer manufacturer to own a music software company. The acquisition was followed by the development of Apple's consumer-level GarageBand application. The release of iPhoto in the same year completed the iLife suite.Mac OS X, based on NeXT's OPENSTEP and BSD Unix, was released on March 24, 2001, after several years of development. Aimed at consumers and professionals alike, Mac OS X aimed to combine the stability, reliability and security of Unix with the ease of use afforded by an overhauled user interface. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X via the Classic Environment.On May 19, 2001, Apple opened its first official eponymous retail stores in Virginia and California. On October 23 of the same year, Apple debuted the iPod portable digital audio player. The product, which was first sold on November 10, 2001, was phenomenally successful with over 100 million units sold within six years. In 2003, Apple's iTunes Store was introduced. The service offered online music downloads for $0.99 a song and integration with the iPod. The iTunes store quickly became the market leader in online music services, with over 5 billion downloads by June 19, 2008.At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would begin producing Intel-based Mac computers in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, The Wall Street Journal reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. In January 2006, Apple's market cap surpassed that of Dell. Nine years prior, Dell's CEO Michael Dell had said that if he ran Apple he would "shut it down and give the money back to the shareholders." Although Apple's market share in computers had grown, it remained far behind competitors using Microsoft Windows, accounting for about 8% of desktops and laptops in the US.Since 2001, Apple's design team has progressively abandoned the use of translucent colored plastics first used in the iMac G3. This design change began with the titanium-made PowerBook and was followed by the iBook's white polycarbonate structure and the flat-panel iMac. 2007–11: Success with mobile devices During his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would thereafter be known as "Apple Inc.", because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The following day, Apple shares hit $97.80, an all-time high at that point. In May, Apple's share price passed the $100 mark. Apple would achieve widespread success with its iPhone, iPod Touch and iPad products, which introduced innovations in mobile phones, portable music players and personal computers respectively. Furthermore, by early 2007, 800,000 Final Cut Pro users were registered.In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players, if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce the corresponding changes to the iTunes Store.In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008, Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.On December 16, 2008, Apple announced that 2009 would be the last year the corporation would attend the Macworld Expo, after more than 20 years of attendance, and that senior vice president of Worldwide Product Marketing Philip Schiller would deliver the 2009 keynote address in lieu of the expected Jobs. The official press release explained that Apple was "scaling back" on trade shows in general, including Macworld Tokyo and the Apple Expo in Paris, France, primarily because the enormous successes of the Apple Retail Stores and website had rendered trade shows a minor promotional channel.On January 14, 2009, Jobs announced in an internal memo that he would be taking a six-month medical leave of absence from Apple until the end of June 2009 and would spend the time focusing on his health. In the email, Jobs stated that "the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well", and explained that the break would allow the company "to focus on delivering extraordinary products". Despite Jobs's absence, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and profit of $1.21 billion.After years of speculation and multiple rumored "leaks", Apple unveiled a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad ran the same touch-based operating system as the iPhone, and many iPhone apps were compatible with the iPad. This gave the iPad a large app catalog on launch, despite very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the US. It sold more than 300,000 units on its first day, and 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.In June 2010, Apple released the iPhone 4, which introduced video calling, multitasking, and a new uninsulated stainless steel design that acted as the phone's antenna. Later that year, Apple again refreshed its iPod line of MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with FaceTime, and an iPod Shuffle that brought back the buttons of earlier generations. Additionally, on October 20, Apple updated the MacBook Air laptop, iLife suite of applications, and unveiled Mac OS X Lion, the last version with the name Mac OS X.In October 2010, Apple shares hit an all-time high, eclipsing $300.On January 6, 2011, the company opened its Mac App Store, a digital software distribution platform similar to the iOS App Store.Alongside peer entities such as Atari and Cisco Systems, Apple was featured in the documentary Something Ventured, which premiered in 2011 and explored the three-decade era that led to the establishment and dominance of Silicon Valley.On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence for an indefinite period to allow him to focus on his health. Chief Operating Officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain "involved in major strategic decisions". Apple became the most valuable consumer-facing brand in the world. In June 2011, Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files and software which replaced MobileMe, Apple's previous attempt at content syncing.This would be the last product launch Jobs would attend before his death. It has been argued that Apple has achieved such efficiency in its supply chain that the company operates as a monopsony (one buyer, many sellers) and can dictate terms to its suppliers. In July 2011, due to the American debt-ceiling crisis, Apple's financial reserves were briefly larger than those of the U.S. Government.On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Cook and Jobs became Apple's chairman. Prior to this, Apple did not have a chairman and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson became Chairman of the Board in November. 2011–present: Post-Steve Jobs era; Tim Cook leadership On October 5, 2011, Steve Jobs died, marking the end of an era for Apple. The first major product announcement by Apple following Jobs's passing occurred on January 19, 2012, when Apple's Phil Schiller introduced iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs had stated in his biography that he wanted to reinvent the textbook industry and education.From 2011 to 2012, Apple released the iPhone 4S and iPhone 5, which featured improved cameras, an intelligent software assistant named Siri, and cloud-sourced data with iCloud; the third and fourth generation iPads, which featured Retina displays; and the iPad Mini, which featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. These launches were successful, with the iPhone 5 (released September 21, 2012) becoming Apple's biggest iPhone launch with over two million pre-orders and sales of three million iPads in three days following the launch of the iPad Mini and fourth generation iPad (released November 3, 2012). Apple also released a third-generation 13-inch MacBook Pro with a Retina display and new iMac and Mac Mini computers.On October 29, 2011, Apple purchased C3 Technologies, a mapping company, for $240 million, making it the third mapping company that Apple has purchased. On January 10, 2012, Apple paid $500 million to acquire Anobit, an Israeli hardware company that developed and supplied a proprietary memory signal processing technology that improved the performance of the flash-memory used in iPhones and iPads. On July 24, 2012, during a conference call with investors, Tim Cook said that he loved India, but that Apple was going to expect larger opportunities outside of India. Cook cited the 30% sourcing requirement from India as the reason.On August 20, 2012, Apple's rising stock price increased the company's market capitalization to a world-record $624 billion. This beat the non-inflation-adjusted record for market capitalization set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung appealed the damages award, which the Court reduced by $450 million. The Court further granted Samsung's request for a new trial. On November 10, 2012, Apple confirmed a global settlement that would dismiss all lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal with HTC.A previously confidential email written by Jobs a year before his death was presented during the proceedings of the Apple Inc. v. Samsung Electronics Co. lawsuits and became publicly available in early April 2014. With a subject line that reads "Top 100 – A," the email was sent only to the company's 100 most senior employees and outlines Jobs's vision of Apple Inc.'s future under 10 subheadings. Notably, Jobs declares a "Holy War with Google" for 2011 and schedules a "new campus" for 2015.In March 2013, Apple filed a patent for an augmented reality (AR) system that can identify objects in a live video stream and present information corresponding to these objects through a computer-generated information layer overlaid on top of the real-world image. Later in 2013, Apple acquired Embark Inc., a small Silicon Valley-based mapping company that builds free transit apps to help smartphone users navigate public transportation in U.S. cities, and PrimeSense, an Israeli 3D sensing company based in Tel Aviv. In December 2013, Apple Inc. purchased social analytics firm Topsy. Topsy is one of a small number of firms with real-time access to the messages that appear on Twitter and can "do real-time analysis of the trends and discussions happening on Twitter". The company also made several high-profile hiring decisions in 2013. On July 2, 2013, Apple recruited Paul Deneve, Belgian President and CEO of Yves Saint Laurent as a vice president reporting directly to Tim Cook. A mid-October 2013 announcement revealed that Burberry executive Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about US$3.2 billion and shares gained more than threefold.At the Worldwide Developer's Conference on June 10, 2013, Apple announced the seventh iOS operating system alongside OS X Mavericks, the tenth version of OS X, and a new Internet radio service called iTunes Radio. iTunes Radio, iOS 7 and OS X Mavericks were released fall 2013. On December 6, 2013, Apple Inc. launched iBeacon across its 254 U.S. retail stores. Using Bluetooth wireless technology, iBeacon senses the user's exact location within the Apple store and sends the user messages about products, events and other information, tailored to the user's location.Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013, in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident. On February 4, 2014, Cook met with Abdullah Gül, the President of Turkey, in Ankara to discuss the company's involvement in the Fatih project. Cook also confirmed that Turkey's first Apple Retail Store would be opened in Istanbul in April 2014.An anonymous Apple employee revealed to the Bloomberg media publication that the opening of a Tokyo, Japan, store was planned for 2014. A Japanese analyst has stated, "For Apple, the Japanese market is appealing in terms of quantity and price. There is room to expand tablet sales and a possibility the Japanese market expands if Apple's mobile carrier partners increase." As of June 13, 2014, Apple operated three stores in Tokyo. On October 1, 2013, Apple India executives unveiled a plan to expand further into the Indian market, following Cook's acknowledgment of the country in July 2013 when sales results showed that iPhone sales in India grew 400% during the second quarter of 2013.Apple Inc. reported that the company sold 51 million iPhones in the Q1 of 2014 (an all-time quarterly record), compared to 47.8 million in the year-ago quarter. Apple also sold 26 million iPads during the quarter, also an all-time quarterly record, compared to 22.9 million in the year-ago quarter. The Company sold 4.8 million Macs, compared to 4.1 million in the year-ago quarter. On May 28, 2014, Apple confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the Beats by Dr. Dre line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion, and to sell their products through Apple's retail outlets and resellers. Iovine felt that Beats had always "belonged" with Apple, as the company modeled itself after Apple's "unmatched ability to marry culture and technology." In August 2014, an Apple representative confirmed to the media that Anand Lal Shimpi, editor and publisher of the AnandTech website, had been recruited by Apple without elaborating on Lal Shimpi's role.Apple has been at the top of Interbrand's annual Best Global Brands report for 4 years in a row; 2013, 2014, 2015, and 2016, with a valuation of $178.1 billion.In December 2015, Apple bought a 70,000 square foot wafer fab building in San Jose, CA from Maxim Integrated for $18.2 million.In 2016, it was revealed that Apple would be making its first original scripted series, a six-episode drama about the life of Dr. Dre. Music Video director Paul Hunter will direct the series.On May 12, 2016, Apple Inc., invested US$1 billion in Didi Chuxing, a Chinese competitor to Uber. The Information reported in October 2016 that Apple had taken a board seat in Didi Chuxing, a move that James Vincent of The Verge speculated could be a strategic company decision by Apple to get closer to the automobile industry, particularly Didi Chuxing's reported interest in self-driving cars.On June 6, 2016, Forbes released their list of companies ranked on revenue generation. In the trailing fiscal year, Apple appeared on the list as the top tech company. It ranked third, overall, with $233 billion in revenue. This represents a movement upward of two spots from the previous year's list.On September 22, 2016, Apple Inc. acquired Tuplejump, an India/US-based machine learning company.On April 6, 2017, Apple launched Clips, an app that allows iPad and iPhone users to make and edit videos. The app provides a way to produce short videos to share with other users on the Messages app, Instagram, Facebook and other social networks. Apple also introduced Live Titles for Clips that allows users to add live animated captions and titles using their voice. Products  Mac Macs currently in production::iMac: Consumer all-in one desktop computer, introduced in 1998.Mac Mini: Consumer sub-desktop computer, introduced in 2005.MacBook: Consumer ultra-thin, ultra-portable notebook, introduced in 2006 and relaunched in 2015.MacBook Pro: Professional notebook, introduced in 2006.Mac Pro: Workstation desktop computer, introduced in 2006.MacBook Air: Consumer ultra-thin, ultra-portable notebook, introduced in 2008.Apple sells a variety of computer accessories for Macs, including Thunderbolt Display, Magic Mouse, Magic Trackpad, Magic Keyboard, the AirPort wireless networking products, and Time Capsule. iPod On October 23, 2001, Apple introduced the iPod digital music player. Several updated models have since been introduced, and the iPod brand is now the market leader in portable music players by a significant margin. More than 350 million units have shipped as of September 2012. Apple has partnered with Nike to offer the Nike+iPod Sports Kit, enabling runners to synchronize and monitor their runs with iTunes and the Nike+ website.Apple currently sells three variants of the iPod:iPod Shuffle: Ultra-portable digital audio player, currently available in a 2 GB model, introduced in 2005.iPod Nano: Portable media player, currently available in a 16 GB model, introduced in 2005. Earlier models featured the traditional iPod click wheel, but the current generation features a multi-touch interface and includes an FM radio and a pedometer.iPod Touch: Portable media player that runs iOS and is currently available in 16, 32, 64, and 128 GB models, introduced in 2007. The current generation features the Apple A8 processor, a Retina display, Siri and dual cameras on the front (1.2 megapixel sensor) and back (8 megapixel iSight). The latter camera supports HD video recording at 1080p and slow motion video at 120fps in 720p. iPhone At the Macworld Conference & Expo in January 2007, Steve Jobs introduced the long-anticipated iPhone, a convergence of an Internet-enabled smartphone and iPod. The first-generation iPhone was released on June 29, 2007, for $499 (4 GB) and $599 (8 GB) with an AT&T contract. On February 5, 2008, it was updated to have 16 GB of memory, in addition to the 8 GB and 4 GB models. It combined a 2.5G quad band GSM and EDGE cellular phone with features found in handheld devices, running scaled-down versions of Apple's Mac OS X (dubbed iPhone OS, later renamed iOS), with various Mac OS X applications such as Safari and Mail. It also includes web-based and Dashboard apps such as Google Maps and Weather. The iPhone features a 3.5-inch (89 mm) touchscreen display, Bluetooth, and Wi-Fi (both "b" and "g").A second version, the iPhone 3G, was released on July 11, 2008, with a reduced price of $199 for the 8 GB version and $299 for the 16 GB version. This version added support for 3G networking and assisted-GPS navigation. The flat silver back and large antenna square of the original model were eliminated in favor of a glossy, curved black or white back. Software capabilities were improved with the release of the App Store, which provided iPhone-compatible applications to download. On April 24, 2009, the App Store surpassed one billion downloads. On June 8, 2009, Apple announced the iPhone 3GS. It provided an incremental update to the device, including faster internal components, support for faster 3G speeds, video recording capability, and voice control.At the Worldwide Developers Conference (WWDC) on June 7, 2010, Apple announced the redesigned iPhone 4. It featured a 960x640 display, the Apple A4 processor, a gyroscope for enhanced gaming, a 5MP camera with LED flash, front-facing VGA camera and FaceTime video calling. Shortly after its release, reception issues were discovered by consumers, due to the stainless steel band around the edge of the device, which also serves as the phone's cellular signal and Wi-Fi antenna. The issue was corrected by a "Bumper Case" distributed by Apple for free to all owners for a few months. In June 2011, Apple overtook Nokia to become the world's biggest smartphone maker by volume. On October 4, 2011, Apple unveiled the iPhone 4S, which was first released on October 14, 2011. It features the Apple A5 processor and Siri voice assistant technology, the latter of which Apple had acquired in 2010. It also features an updated 8MP camera with new optics. Apple began a new accessibility feature, Made for iPhone Hearing Aids with the iPhone 4S. Made for iPhone Hearing Aids feature Live Listen, it can help you hear a conversation in a noisy room or hear someone speaking across the room. Apple sold 4 million iPhone 4S phones in the first three days of availability.On September 12, 2012, Apple introduced the iPhone 5. It has a 4-inch display, 4G LTE connectivity, and the upgraded Apple A6 chip, among several other improvements. Two million iPhones were sold in the first twenty-four hours of pre-ordering and over five million handsets were sold in the first three days of its launch. Upon the launch of the iPhone 5S and iPhone 5C, Apple set a new record for first-weekend smartphone sales by selling over nine million devices in the first three days of its launch. The release of the iPhone 5S and 5C was the first time that Apple simultaneously launched two models.A patent filed in July 2013 revealed the development of a new iPhone battery system that uses location data in combination with data on the user's habits to moderate the handsets power settings accordingly. Apple is working towards a power management system that will provide features such as the ability of the iPhone to estimate the length of time a user will be away from a power source to modify energy usage and a detection function that adjusts the charging rate to best suit the type of power source that is being used.In a March 2014 interview, Apple designer Jonathan Ive used the iPhone as an example of Apple's ethos of creating high-quality, life-changing products. He explained that the phones are comparatively expensive due to the intensive effort that is used to make them:We don't take so long and make the way we make for fiscal reasons ... Quite the reverse. The body is made from a single piece of machined aluminium ... The whole thing is polished first to a mirror finish and then is very finely textured, except for the Apple logo. The chamfers [smoothed-off edges] are cut with diamond-tipped cutters. The cutters don't usually last very long, so we had to figure out a way of mass-manufacturing long-lasting ones. The camera cover is sapphire crystal. Look at the details around the sim-card slot. It's extraordinary!On September 9, 2014, Apple introduced the iPhone 6, alongside the iPhone 6 Plus that both have screen sizes over 4-inches. One year later, Apple introduced the iPhone 6S, and iPhone 6S Plus, which introduced a new technology called 3D Touch, including an increase of the rear camera to 12 MP, and the FaceTime camera to 5 MP. On March 21, 2016, Apple introduced the iPhone SE that has a 4-inch size last used with the 5S and has the same internal hardware as the 6S.On September 7, 2016, Apple introduced the iPhone 7 and the iPhone 7 Plus, which feature improved system and graphics performance, add water resistance, a new rear dual-camera system on the 7 Plus model, and, controversially, remove the 3.5 mm headphone jack. iPad On January 27, 2010, Apple introduced their much-anticipated media tablet, the iPad. It offers multi-touch interaction with multimedia formats including newspapers, e-books, photos, videos, music, word processing documents, video games, and most existing iPhone apps using a 9.7-inch screen. It also includes a mobile version of Safari for web browsing, as well as access to the App Store, iTunes Library, iBookstore, Contacts, and Notes. Content is downloadable via Wi-Fi and optional 3G service or synced through the user's computer. AT&T was initially the sole U.S. provider of 3G wireless access for the iPad.On March 2, 2011, Apple introduced the iPad 2, which had a faster processor and a camera on the front and back. It also added support for optional 3G service provided by Verizon in addition to AT&T. The availability of the iPad 2 was initially limited as a result of a devastating earthquake and tsunami in Japan in March 2011.The third-generation iPad was released on March 7, 2012, and marketed as "the new iPad". It added LTE service from AT&T or Verizon, an upgraded A5X processor, and Retina display. The dimensions and form factor remained relatively unchanged, with the new iPad being a fraction thicker and heavier than the previous version and featuring minor positioning changes.On October 23, 2012, Apple's fourth-generation iPad came out, marketed as the "iPad with Retina display". It added the upgraded A6X processor and replaced the traditional 30-pin dock connector with the all-digital Lightning connector. The iPad Mini was also introduced. It featured a reduced 7.9-inch display and much of the same internal specifications as the iPad 2.On October 22, 2013, Apple introduced the iPad Air and the iPad Mini with Retina Display, both featuring a new 64-bit Apple A7 processor.The iPad Air 2 was unveiled on October 16, 2014. It added better graphics and central processing and a camera burst mode as well as minor updates. The iPad Mini 3 was unveiled at the same time.Since its launch, iPad users have downloaded over three billion apps. The total number of App Store downloads, as of June 2015, is over 100 billion.On September 9, 2015, Apple announced the iPad Pro, an iPad with a 12.9-inch display that supports two new accessories, the Smart Keyboard and Apple Pencil. A 9.7-inch iPad Pro was announced on March 21, 2016. Apple Watch The Apple Watch smartwatch was announced by Cook on September 9, 2014, and released on April 24, 2015. The wearable device consists of fitness-tracking capabilities that are similar to Fitbit, and must be used in combination with an iPhone to work (only the iPhone 5, or later models, are compatible with the Apple Watch).The second generation of Apple Watch, Apple Watch Series 2 and Apple Watch Series 1 were released in September 2016. Apple TV At the 2007 Macworld conference, Jobs demonstrated the Apple TV (previously known as the iTV), a set-top video device intended to bridge the sale of content from iTunes with high-definition televisions. The device links up to a user's TV and syncs, either via Wi-Fi or a wired network, with one computer's iTunes library and streams content from an additional four. The Apple TV originally incorporated a 40 GB hard drive for storage, included outputs for HDMI and component video, and played video at a maximum resolution of 720p. On May 31, 2007, a 160 GB hard disk drive was released alongside the existing 40 GB model. A software update released on January 15, 2008, allowed media to be purchased directly from the Apple TV.In September 2009, Apple discontinued the original 40 GB Apple TV and now continues to produce and sell the 160 GB Apple TV. On September 1, 2010, Apple released a completely redesigned Apple TV. The new device is 1/4 the size, runs quieter, and replaces the need for a hard drive with media streaming from any iTunes library on the network along with 8 GB of flash memory to cache media downloaded. Like the iPad and the iPhone, Apple TV runs on an A4 processor. The memory included in the device is half of that in the iPhone 4 at 256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod Touch.It has HDMI out as the only video out source. Features include access to the iTunes Store to rent movies and TV shows (purchasing has been discontinued), streaming from internet video sources, including YouTube and Netflix, and media streaming from an iTunes library. Apple also reduced the price of the device to $99. A third generation of the device was introduced at an Apple event on March 7, 2012, with new features such as higher resolution (1080p) and a new user interface.At the September 9, 2015, event, Apple unveiled an overhauled Apple TV, which now runs a variant of OS X, tvOS, and contains 32GB or 64 GB of NAND Flash to store games, programs, and to cache the current media playing. The release also coincided with the opening of a separate Apple TV App Store and a new Siri Remote with a glass touchpad, gyroscope, and microphone. Software Apple develops its own operating system to run on Macs, macOS, the latest version being macOS Sierra (version 10.12). Apple also independently develops computer software titles for its macOS operating system. Much of the software Apple develops is bundled with its computers. An example of this is the consumer-oriented iLife software package that bundles iMovie, iPhoto and GarageBand. For presentation, page layout and word processing, iWork is available, which includes Keynote, Pages, and Numbers. iTunes, QuickTime media player, and Software Update are available as free downloads for both macOS and Windows.Apple also offers a range of professional software titles. Their range of server software includes the operating system macOS Server; Apple Remote Desktop, a remote systems management application; and Xsan, a Storage Area Network file system. For the professional creative market, there is Final Cut Pro, a video production suite; Logic Pro, a comprehensive music toolkit; and Motion, an advanced effects composition program.Apple also offers online services with iCloud, which provides cloud storage and syncing for a wide range of data, including email, contacts, calendars, photos, and documents. It also offers iOS device backup, and is able to integrate directly with third-party apps for even greater functionality. iCloud is the fourth generation of online services provided by Apple, and was preceded by MobileMe, .Mac and iTools, all which met varying degrees of success. Electric vehicles According to the Sydney Morning Herald, Apple wants to start producing an electric car with autonomous driving as soon as 2020. Apple has made efforts to recruit battery development engineers and other electric automobile engineers from A123 Systems, LG Chem, Samsung Electronics, Panasonic, Toshiba, Johnson Controls and Tesla Motors. Apple Energy Apple Energy, LLC is a wholly owned subsidiary of Apple Inc. that sells solar energy. As of June 6, 2016, Apple's solar farms in California and Nevada have been declared to provide 217.9 megawatts of solar generation capacity. In addition to the company's solar energy production, Apple has received regulatory approval to construct a landfill gas energy plant in North Carolina. Apple will use the methane emissions to generate electricity. Apple's North Carolina data center is already powered entirely with energy from renewable sources. Corporate identity  Logo According to Steve Jobs, the company's name was inspired by his visit to an apple farm while on a fruitarian diet. Jobs thought the name "Apple" was "fun, spirited and not intimidating".Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's "rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. Janoff presented Jobs with several different monochromatic themes for the "bitten" logo, and Jobs immediately took a liking to it. However, Jobs insisted that the logo be colorized to humanize the company. The logo was designed with a bite so that it would not be confused with a cherry. The colored stripes were conceived to make the logo more accessible, and to represent the fact the Apple II could generate graphics in color. This logo is often erroneously referred to as a tribute to Alan Turing, with the bite mark a reference to his method of suicide. Both Janoff and Apple deny any homage to Turing in the design of the logo.On August 27, 1999 (the year following the introduction of the iMac G3), Apple officially dropped the rainbow scheme and began to use monochromatic logos nearly identical in shape to the previous rainbow incarnation. An Aqua-themed version of the monochrome logo was used from 1998 to 2003, and a glass-themed version was used from 2007 to 2013.Steve Jobs and Steve Wozniak were Beatles fans, but Apple Inc. had name and logo trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1967. This resulted in a series of lawsuits and tension between the two companies. These issues ended with settling of their most recent lawsuit in 2007. Advertising Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997 to 2002, the slogan "Think Different" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines — for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.From the introduction of the Macintosh in 1984 with the 1984 Super Bowl commercial to the more modern 'Get a Mac' adverts, Apple has been recognized for its efforts towards effective advertising and marketing for its products. However, claims made by later campaigns were criticized, particularly the 2005 Power Mac ads. Apple's product commercials gained a lot of attention as a result of their eye-popping graphics and catchy tunes. Musicians who benefited from an improved profile as a result of their songs being included on Apple commercials include Canadian singer Feist with the song "1234" and Yael Naïm with the song "New Soul". Brand loyalty Apple customers gained a reputation for devotion and loyalty early in the company's history. BYTE in 1984 stated thatThere are two kinds of people in the world: people who say Apple isn't just a company, it's a cause; and people who say Apple isn't a cause, it's just a company. Both groups are right. Nature has suspended the principle of noncontradiction where Apple is concerned.Apple is more than just a company because its founding has some of the qualities of myth ... Apple is two guys in a garage undertaking the mission of bringing computing power, once reserved for big corporations, to ordinary individuals with ordinary budgets. The company's growth from two guys to a billion-dollar corporation exemplifies the American Dream. Even as a large corporation, Apple plays David to IBM's Goliath, and thus has the sympathetic role in that myth.Apple evangelists were actively engaged by the company at one time, but this was after the phenomenon had already been firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon," while Ive explained in 2014 that "People have an incredibly personal relationship" with Apple's products. Apple Store openings can draw crowds of thousands, with some waiting in line as much as a day before the opening or flying in from other countries for the event. The opening of New York City's Fifth Avenue "Cube" store had a line half a mile long; a few Mac fans used the setting to propose marriage. The line for the Ginza opening in Tokyo was estimated to include thousands of people and exceeded eight city blocks. The high level of brand loyalty has been criticized and ridiculed, applying the epithet "Apple fanboy" and mocking the lengthy lines before a product launch. An internal memo leaked in 2015 suggested the company planned to discourage long lines and direct customers to purchase its products on its website.Fortune magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's "Best Global Brands" report. Boston Consulting Group has ranked Apple as the world's most innovative brand every year since 2005.John Sculley told The Guardian newspaper in 1997: "People talk about technology, but Apple was a marketing company. It was the marketing company of the decade." Research in 2002 by NetRatings indicate that the average Apple consumer was usually more affluent and better educated than other PC company consumers. The research indicated that this correlation could stem from the fact that on average Apple Inc. products were more expensive than other PC products.In response to a query about the devotion of loyal Apple consumers, Jonathan Ive responded:What people are responding to is much bigger than the object. They are responding to something rare—a group of people who do more than simply make something work, they make the very best products they possibly can. It's a demonstration against thoughtlessness and carelessness. Home page The Apple website home page has been used to commemorate, or pay tribute to, milestones and events outside of Apple's product offerings:2017: Martin Luther King Jr.2016: Muhammad Ali2016: Bill Campbell (board member and friend)2016: Martin Luther King Jr.2014: Robin Williams2013: Nelson Mandela2011: Steve Jobs2010: Jerome B. York (board member)2007: Al Gore (board member in honor of his Nobel Peace Prize)2005: Rosa Parks2003: Gregory Hines2001: George Harrison Headquarters Apple Inc.'s world corporate headquarters are located in the middle of Silicon Valley, at 1–6 Infinite Loop, Cupertino, California. This Apple campus has six buildings that total 850,000 square feet (79,000 m2) and was built in 1993 by Sobrato Development Cos.Apple has a satellite campus in neighboring Sunnyvale, California, where it houses a testing and research laboratory. AppleInsider published article in March 2014 claiming that Apple has a tucked away a top-secret facility where is developing the SG5 electric vehicle project codenamed "Titan" under the shell company name SixtyEight Research.In 2006, Apple announced its intention to build a second campus in Cupertino about 1 mile (1.6 km) east of the current campus and next to Interstate 280. The new campus building will be designed by Norman Foster. The Cupertino City Council approved the proposed "spaceship" design campus on October 15, 2013, after a 2011 presentation by Jobs detailing the architectural design of the new building and its environs. The new campus is planned to house up to 13,000 employees in one central, four-storied, circular building surrounded by extensive landscape. It will feature a café with room for 3,000 sitting people and parking underground as well as in a parking structure. The 2.8 million square foot facility will also include Jobs's original designs for a fitness center and a corporate auditorium.Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland. The facility, which opened in 1980, was Apple's first location outside of the United States. Apple Sales International, which deals with all of Apple's international sales outside of the USA, is located at Apple's campus in Cork along with Apple Distribution International, which similarly deals with Apple's international distribution network. On April 20, 2012, Apple added 500 new jobs at its European headquarters, increasing the total workforce from around 2,800 to 3,300 employees. The company will build a new office block on its Hollyhill Campus to accommodate the additional staff. Its United Kingdom headquarters is at Stockley Park on the outskirts of London.In February 2015, Apple opened their new 180,000-square-foot headquarters in Herzliya, Israel, which will accommodate approximately 800 employees. This opening was Apple's third office located within Israel; the first, also in Herzliya, was obtained as part of the Anobit acquisition, and the other is a research center in Haifa. Stores Apple has 478 retail stores (as of March 2016) in seventeen countries and an online store available in 39 countries. Each store is designed to suit the needs of the location and regulatory authorities. Apple has received numerous architectural awards for its store designs, particularly its midtown Manhattan location on Fifth Avenue.The Apple Store in Regent Street, London, was the first to open in Europe in November 2004, and is the most profitable shop in London with the highest sales per square foot, taking £60,000,000 pa, or £2,000 per square foot. The Regent Street store was surpassed in size by the nearby Apple Store in Covent Garden, which was surpassed in size by the Grand Central Terminal Apple Store, New York City, in December 2011.Of the 43,000 Apple employees in the United States 30,000 work at Apple Stores. Apple Store employees make above average pay for retail employees and are offered money toward college as well as gym memberships, 401k plans, healthcare plans, product discounts, and reduced price on purchase of stock.A May 2016 Business Insider article featuring a lengthy interview with a U.K. Apple Store retail worker highlighted significant dissatisfactions and issues for retail workers, including harassment and death threats from customers, an intense internal criticism policy that feels "like a cult", a lack of any significant bonus if a worker manages to secure a business contract worth "hundreds of thousands", a lack of promotion opportunities, and, despite a "generous" discount on any Apple product or Apple stock, are paid so little that many workers are unable to buy products themselves. Corporate affairs  Corporate culture Apple was one of several highly successful companies founded in the 1970s that bucked the traditional notions of corporate culture. Jobs often walked around the office barefoot even after Apple became a Fortune 500 company. By the time of the "1984" television commercial, Apple's informal culture had become a key trait that differentiated it from its competitors. According to a 2011 report in Fortune, this has resulted in a corporate culture more akin to a startup rather than a multinational corporation.As the company has grown and been led by a series of differently opinionated chief executives, it has arguably lost some of its original character. Nonetheless, it has maintained a reputation for fostering individuality and excellence that reliably attracts talented workers, particularly after Jobs returned to the company. Numerous Apple employees have stated that projects without Jobs's involvement often take longer than projects with it. To recognize the best of its employees, Apple created the Apple Fellows program which awards individuals who make extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, and Steve Wozniak.At Apple, employees are specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having "best-in-class" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet had no control of the inventory in his stores (this was done by Cook, who had a background in supply-chain management). Apple is also known for strictly enforcing accountability. Each project has a "directly responsible individual," or "DRI" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike other major U.S. companies Apple provides a relatively simple compensation policy for executives that does not include perks enjoyed by other CEOs like country club fees or private use of company aircraft. The company typically grants stock options to executives every other year.An editorial article in The Verge in September 2016 by technology journalist Thomas Ricker explored some of the public's perceived lack of innovation at Apple in recent years, specifically stating that Samsung has "matched and even surpassed Apple in terms of smartphone industrial design" and citing the belief that Apple is incapable of producing another breakthrough moment in technology with its products. He goes on to write that the criticism focuses on individual pieces of hardware rather than the ecosystem as a whole, stating "Yes, iteration is boring. But it's also how Apple does business. [...] It enters a new market and then refines and refines and continues refining until it yields a success". He acknowledges that people are wishing for the "excitement of revolution", but argues that people want "the comfort that comes with harmony". Furthermore, he writes that "a device is only the starting point of an experience that will ultimately be ruled by the ecosystem in which it was spawned", referring to how decent hardware products can still fail without a proper ecosystem (specifically mentioning that Walkman didn't have an ecosystem to keep users from leaving once something better came along), but how Apple devices in different hardware segments are able to communicate and cooperate through the iCloud cloud service with features including Universal Clipboard (in which text copied on one device can be pasted on a different device) as well as inter-connected device functionality including Auto Unlock (in which an Apple Watch can unlock a Mac in close proximity). He argues that Apple's ecosystem is its greatest innovation. Customer service In 1999 Apple retained Eight Inc. as a strategic retail design partner and began creating the Apple retail stores. Tim Kobe of Eight Inc. prepared an "Apple Retail" white paper for Jobs, outlining the ability of separate Apple retail stores to directly drive the Apple brand experience—Kobe used their recently completed work with The North Face and Nike as a basis for the white paper. The first two Apple Stores opened on May 19, 2001, in Tysons Corner, Virginia, and Glendale, California. More than 7,700 people visited Apple's first two stores in the opening weekend, spending a total of US$599,000. As of June 2014, Apple maintains 425 retail stores in fourteen countries. In addition to Apple products, the stores sell third-party products like software titles, digital cameras, camcorders and handheld organizers.A media article published in July 2013 provided details about Apple's "At-Home Apple Advisors" customer support program that serves as the corporation's call center. The advisors are employed within the U.S. and work remotely after undergoing a four-week training program and testing period. The advisors earn between US$9 and $12 per hour and receive intensive management to ensure a high quality of customer support. Manufacturing The company's manufacturing, procurement and logistics enable it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: "Nobody wants to buy sour milk".During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry standards groups to influence the future direction of technology standards such as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394 and is a legally mandated port in all Cable TV boxes in the United States. Labor practices The company advertised its products as being made in America until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by The New York Times, Apple insiders "believe the vast scale of overseas factories as well as the flexibility, diligence and industrial skills of foreign workers have so outpaced their American counterparts that "Made in the U.S.A." is no longer a viable option for most Apple products".In 2006, the Mail on Sunday reported on the working conditions of the Chinese factories where contract manufacturers Foxconn and Inventec produced the iPod. The article stated that one complex of factories that assembled the iPod and other items had over 200,000 workers living and working within it. Employees regularly worked more than 60 hours per week and made around $100 per month. A little over half of the workers' earnings was required to pay for rent and food from the company.Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published since 2008. In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.The Foxconn suicides occurred between January and November 2010, when 18 Foxconn (Chinese: 富士康) employees attempted suicide, resulting in 14 deaths—the company was the world's largest contract electronics manufacturer, for clients including Apple, at the time. The suicides drew media attention, and employment practices at Foxconn were investigated by Apple. Apple issued a public statement about the suicides, and company spokesperson Steven Dowling said:[Apple is] saddened and upset by the recent suicides at Foxconn ... A team from Apple is independently evaluating the steps they are taking to address these tragic events and we will continue our ongoing inspections of the facilities where our products are made.The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010. Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories. Apple committed to the implementation of changes following the suicides.Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a high suicide rate in a Foxconn facility in China making iPads and iPhones, albeit a lower rate than that of China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves. Workers in factories producing Apple products have also been exposed to n-hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.A 2014 BBC investigation found excessive hours and other problems persisted, despite Apple's promise to reform factory practice after the 2010 Foxconn suicides. The Pegatron factory was once again the subject of review, as reporters gained access to the working conditions inside through recruitment as employees. While the BBC maintained that the experiences of its reporters showed that labor violations were continuing since 2010, Apple publicly disagreed with the BBC and stated: "We are aware of no other company doing as much as Apple to ensure fair and safe working conditions".In December 2014, the Institute for Global Labour and Human Rights published a report which documented inhumane conditions for the 15,000 workers at a Zhen Ding Technology factory in Shenzhen, China, which serves as a major supplier of circuit boards for Apple's iPhone and iPad. According to the report, workers are pressured into 65-hour work weeks which leaves them so exhausted that they often sleep during lunch breaks. They are also made to reside in "primitive, dark and filthy dorms" where they sleep "on plywood, with six to ten workers in each crowded room." Omnipresent security personnel also routinely harass and beat the workers. Environmental practices and initiatives  Energy and resources Following a Greenpeace protest, Apple released a statement on April 17, 2012, committing to ending its use of coal and shifting to 100% renewable clean energy. By 2013 Apple was using 100% renewable energy to power their data centers. Overall, 75% of the company's power came from clean renewable sources.In 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category "Striding". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a "stuck icon", adding that Apple at the time was "a choice to avoid for the climate conscious consumer".As of 2016, Apple states that 100% of its U.S. operations run on renewable energy, 100% of Apple's data centers run on renewable energy and 93% of Apple's global operations run on renewable energy. However, the facilities are connected to the local grid which usually contains a mix of fossil and renewable sources, so Apple carbon offsets its electricity use. The Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to see the effect a product has on the environment. Each product receives a Gold, Silver, or Bronze rank depending on its efficiency and sustainability. Every Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible. Although Apple's data centers recycle water 35 times, the increased activity in retail, corporate and data centers also increase the amount of water use to 573 million gallons in 2015.In May 2015, Greenpeace evaluated the state of the Green Internet and commended Apple on their environmental practices saying, "Apple's commitment to renewable energy has helped set a new bar for the industry, illustrating in very concrete terms that a 100% renewable Internet is within its reach, and providing several models of intervention for other companies that want to build a sustainable Internet."During an event on March 21, 2016, Apple provided a status update on its environmental initiative to be 100% renewable in all of its worldwide operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and Social Initiatives who reports directly to CEO, Tim Cook, announced that as of March 2016, 93% of Apple's worldwide operations are powered with renewable energy. Also featured was the company's efforts to use sustainable paper in their product packaging; 99% of all paper used by Apple in the product packaging comes from post-consumer recycled paper or sustainably-managed forests, as the company continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina. Another partnership announced is with the World Wildlife Fund to preserve up to 1,000,000 acres of forests in China. Featured was the company's installation of a 40 MW solar power plant in the Sichuan province of China that was tailor made to coexist with the indigenous yaks that eat hay produced on the land, by raising the panels to be several feet off of the ground so the yaks and their feed would be unharmed grazing beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in the country. In Singapore, Apple has worked with the Singaporean government to cover the rooftops of 800 buildings in the city-state with solar panels allowing Apple's Singapore operations to be run on 100% renewable energy. Liam was introduced to the world, an advanced robotic disassembler and sorter designed by Apple Engineers in California specifically for recycling outdated or broken iPhones. Reuses and recycles parts from traded in products.Apple announced on August 16, 2016, that Lens Technology, one of its major suppliers in China, has committed to power all its glass production for Apple with 100 percent renewable energy by 2018. The commitment is a large step in Apple's efforts to help manufacturers lower their carbon footprint in China. Apple also announced that all 14 of its final assembly sites in China are now compliant with UL's Zero Waste to Landfill validation. The standard, which started in January 2015, certifies that all manufacturing waste is reused, recycled, composted, or converted into energy (when necessary). Since the program began, nearly, 140,000 metric tons of waste have been diverted from landfills. Toxins Following further campaigns by Greenpeace, in 2008, Apple became the first electronics manufacturer to fully eliminate all polyvinyl chloride (PVC) and brominated flame retardants (BFRs) in its complete product line. In June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays in its computers with mercury-free LED-backlit LCD displays and arsenic-free glass, starting with the upgraded MacBook Pro. Apple offers comprehensive and transparent information about the CO2e, emissions, materials, and electrical usage concerning every product they currently produce or have sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they offer for sale. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, and BFRs. All Apple products now have mercury-free LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple products have EPEAT Gold status and beat the latest Energy Star guidelines in each product's respective regulatory category.In November 2011, Apple was featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how "green" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10. Greenpeace praises Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. It continues to score well on the products rating with all Apple products now being free of PVC plastic and BFRs. However, the guide criticizes Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data and for not setting out any targets to reduce emissions. In January 2012, Apple requested that its cable maker, Volex, begin producing halogen-free USB and power cables. Green bonds In February 2016, Apple issued a US$1.5 billion green bond (climate bond), the first ever of its kind by a U.S. tech company. The green bond proceeds are dedicated to the financing of environmental projects. Finance Apple is the world's largest information technology company by revenue and the world's second-largest mobile phone manufacturer. It is also the largest publicly traded corporation in the world by market capitalization, with an estimated market capitalization of $446 billion by January 2014. On February 17, 2015, Apple became the first US corporation to be valued at over $750B. As of March 2016, Apple maintains 475 retail stores in seventeen countries, of which 207 are outside the U.S., as well as the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. It employs 115,000 permanent full-time employees as of July 2015 and 3,300 temporary full-time employees as of September 2012 worldwide.In its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues—a significant increase from its 2010 revenues of $65 billion—and nearly $82 billion in cash reserves. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors. On September 2012, Apple reached a record share price of more than $705 and closed at above 700. With 936,596,000 outstanding shares (as of June 30, 2012),The company's worldwide annual revenue in 2013 totaled $170 billion. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position. As of 2016, Apple has around US$234 billion of cash and marketable securities, of which 90% is located outside the United States for tax purposes.Apple amassed 65% of all profits made by the eight largest worldwide smartphone manufacturers in quarter one of 2014, according to a report by Canaccord Genuity. In the first quarter of 2015, the company garnered 92% of all earnings. Tax practices Apple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg and the British Virgin Islands to cut the taxes it pays around the world. According to The New York Times, in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s Apple was a pioneer of an accounting technique known as the "Double Irish with a Dutch sandwich," which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporation tax. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax.It is a matter of public record that Apple Inc. is the single largest taxpayer to the Department of the Treasury of the United States of America with an effective tax rate of approximately of 26% as of the Second Quarter of the Apple Fiscal Year 2016.In 2015, Reuters reported that Apple had earnings abroad of $54.4 billion which were untaxed by the IRS of the United States. Under U.S. tax law governed by the IRC, corporations don't pay income tax on overseas profits unless the profits are repatriated into the United States and as such Apple argues that to benefit its shareholders it will leave it overseas until a repatriation holiday or comprehensive tax reform takes place in the United States.On August 30, 2016, after a three-year investigation by the EU's competition commissioner that concluded that Apple received "illegal state aid" from Ireland, the EU ordered Apple to pay 13 billion euros ($14.5 billion), plus interest, in unpaid taxes. Specifically, the commissioner found that Apple had benefitted from Irish Department of Revenue tax rulings that allowed it to split the profits recorded by Apple Sales International internally between its Irish branch and a stateless "head office" entity lacking employees or premises (permitted under Irish law until 2013). The Chancellor of Austria, Christian Kern, put this decision into perspective by stating that "every Viennese cafe, every sausage stand pays more tax in Austria than a multinational corporation". Ownership Apple Inc. is a joint-stock company registered with the SEC. As of 30 December 2016, it has 5,257,816,000 outstanding shares. These are mainly held by institutional investors and funds.6.43% (337,545,664): The Vanguard Group, Inc.4.19% (219,739,579): State Street Corporation3.01% (157,982,573): FMR, LLC2.76% (144,750,804): BlackRock Institutional Trust Company2.01% (105,224,082): Vanguard Total Stock Market Index Fund1.46% (76,838,518): BlackRock Fund Advisors1.42% (74,581,785): Vanguard 500 Index Fund1.26% (66,180,770): Northern Trust Corporation1.14% (59,781,550): Vanguard Institutional Index Fund-Institutional Index Fund1.11% (58,148,652): Bank of New York Mellon Corporation1.09% (57,359,652): Berkshire Hathaway, Inc.1.08% (56,659,296): SPDR S&P 500 ETF Trust1.06% (55,512,801): T. Rowe Price Associates Inc. Litigation Apple has been a participant in various legal proceedings and claims since it began operation. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests. Some litigation examples include Apple v. Samsung, Apple v. Microsoft, Motorola Mobility v. Apple Inc., and Apple Corps v. Apple Computer. Apple has also had to defend itself against charges on numerous occasions of violating intellectual property rights. Most have been dismissed in the courts as shell companies known as patent trolls, with no evidence of actual use of patents in question. Most recently, on December 21, 2016, Nokia announced that in the U.S. and Germany, it has filed a suit against Apple, claiming that the latter's products infringe on Nokia's patents. Privacy stance Apple has made clear its stance on privacy and as such has made available Transparency Reports on the Governmental Requests it receives. Apple states plainly, "On devices running iOS 8 and later versions, your personal data is placed under the protection of your passcode.[305] For all devices running iOS 8 and later versions, Apple will not perform iOS data extractions in response to government search warrants because the files to be extracted are protected by an encryption key that is tied to the user's passcode, which Apple does not possess."In its latest "Who Has Your Back?" report, once again the Electronic Frontier Foundation (EFF) awarded Apple 5 out of 5 stars "commend[ing] Apple for its strong stance regarding user rights, transparency, and privacy."Following media reports about PRISM, NSA's massive electronic surveillance program, in June 2013, several technology companies were identified as participants, including Apple. According to leaks of said program, Apple joined the PRISM program in 2012. Charitable causes Apple is a partner of (PRODUCT)RED, a fundraising campaign for AIDS charity. In November 2014, Apple arranged for all App Store revenue in a two-week period to go to the fundraiser, generating more than US$20 million, and in March 2017, it released an iPhone 7 with a red color finish.In November 2012, Apple donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy.On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced that they have engaged in a partnership to, "help protect life on our planet." Apple released a special page in the iTunes App Store, Apps for Earth. In the arrangement, Apple has committed that through April 24, WWF will receive 100% of the proceeds from the applications participating in the App Store via both the purchases of any paid apps and the In-App Purchases. Apple and WWF's Apps for Earth campaign raised more than $8 million in total proceeds to support WWF's conservation work. WWF announced the results at WWDC 2016 in San Francisco. See also Apple media eventsPixar Apple Inc. – Wikipedia book References Sources Further reading  External links Official websiteApple Inc. companies grouped at OpenCorporates Geographic data related to Apple Inc. headquarters at OpenStreetMap
Economics (UK English: /iːkəˈnɒmɪks/, /ɛkəˈnɒmɪks/; US English: /ɛkəˈnɑːmɪks/, /ikəˈnɑːmɪks/) is "a social science concerned chiefly with description and analysis of the production, distribution, and consumption of goods and services" according to the Merriam-Webster Dictionary. The discipline was renamed in the late 19th century primarily due to Alfred Marshall from "political economy" to "economics" as a shorter term for "economic science" at a time when it became more open to rigorous thinking and made increased use of mathematics, which helped support efforts to have it accepted as a science and as a separate discipline outside of political science and other social sciences.Economics focuses on the behaviour and interactions of economic agents and how economies work. Consistent with this focus, textbooks often distinguish between microeconomics and macroeconomics. Microeconomics examines the behaviour of basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the entire economy (meaning aggregated production, consumption, savings, and investment) and issues affecting it, including unemployment of resources (labour, capital, and land), inflation, economic growth, and the public policies that address these issues (monetary, fiscal, and other policies).Other broad distinctions within economics include those between positive economics, describing "what is", and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.Economic analysis can be applied throughout society, as in business, finance, health care, and government. Economic analyses may also be applied to such diverse subjects as crime, education, the family, law, politics, religion, social institutions, war, science, and the environment. Education, for example, requires time, effort, and expenses, plus the foregone income and experience, yet these losses can be weighted against future benefits education may bring to the agent or the economy. At the turn of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism. The ultimate goal of economics is to improve the living conditions of people in their everyday life. Definitions There are a variety of modern definitions of economics. Some of the differences may reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as "an inquiry into the nature and causes of the wealth of nations", in particular as:a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the publick services.J.-B. Say (1803), distinguishing the subject from its public-policy uses, defines it as the science of production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined "the dismal science" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) defines the subject in a social context as:The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object.Alfred Marshall provides a still widely cited definition in his textbook Principles of Economics (1890) that extends analysis beyond wealth and from the societal to the microeconomic level:Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man.Lionel Robbins (1932) developed implications of what has been termed "[p]erhaps the most commonly accepted current definition of the subject":Economics is a science which studies human behaviour as a relationship between ends and scarce means which have alternative uses.Robbins describes the definition as not classificatory in "pick[ing] out certain kinds of behaviour" but rather analytical in "focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity." He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. We cannot define economics as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).Some subsequent comments criticized the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.Gary Becker, a contributor to the expansion of economics into new areas, describes the approach he favours as "combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly." One commentary characterizes the remark as making economics an approach rather than a subject matter but with great specificity as to the "choice process and the type of social interaction that [such] analysis involves." The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve. Microeconomics  Markets Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and government regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.In theory, in a free market the aggregates (sum of) of quantity demanded by buyers and quantity supplied by sellers will be equal and reach economic equilibrium over time in reaction to price changes; in practice, various issues may prevent equilibrium, and any equilibrium reached may not necessarily be morally equitable. For example, if the supply of healthcare services is limited by external factors, the equilibrium price may be unaffordable for many who desire it but cannot pay for it.Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a "price taker" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.Forms include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Unlike perfect competition, imperfect competition invariably means market power is unequally distributed. Firms under imperfect competition have the potential to be "price makers", which means that, by holding a disproportionately high share of market power, they can influence the prices of their products.Microeconomics studies individual markets by simplifying the economic system by assuming that activity in the market being analysed does not affect other markets. This method of analysis is known as partial-equilibrium analysis (supply and demand). This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across all markets. This method studies both changes in markets and their interactions leading towards equilibrium. Production, cost, and efficiency In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods (new computers, bananas, etc.), and "guns" vs "butter".Opportunity cost refers to the economic cost of production: the value of the next best opportunity foregone. Choices must be made between desirable yet mutually exclusive actions. It has been described as expressing "the basic relationship between scarcity and choice". For example, if a baker uses a sack of flour to make pretzels one morning, then the baker cannot use either the flour or the morning to make bagels instead. Part of the cost of making pretzels is that neither the flour nor the morning are available any longer, for use in some other way. The opportunity cost of an activity is an element in ensuring that scarce resources are used efficiently, such that the cost is weighed against the value of that activity in deciding on more or less of it. Opportunity costs are not restricted to monetary or financial costs but could be measured by the real cost of output forgone, leisure, or anything else that provides the alternative benefit (utility).Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.Economic efficiency describes how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs, or in other words, the amount of "waste" is reduced. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say "guns" and "butter"). The PPF is a table or graph (as at the right) showing the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.Scarcity is represented in the figure by people being willing but unable in the aggregate to consume beyond the PPF (such as at X) and by the negative slope of the curve. If production of one good increases along the curve, production of the other good decreases, an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a real opportunity cost. Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. Along the PPF, scarcity implies that choosing more of one good in the aggregate entails doing with less of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.By construction, each point on the curve shows productive efficiency in maximizing output for given total inputs. A point inside the curve (as at A), is feasible but represents production inefficiency (wasteful use of inputs), in that output of one or both goods could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organization of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organize society for the most efficient use of resources has been described as the "essence of economics", where the subject "makes its unique contribution." Specialization Specialization is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus relatively cheaper, input.Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialize in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialization in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.The general theory of specialization applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.An example that combines features above is a country that specializes in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialization of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate. Supply and demand Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is "constrained utility maximization" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.Supply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be profit-maximizers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The "Law of Supply" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.For a given quantity of a consumer good, the point on the demand curve indicates the value, or marginal utility, to consumers for that unit. It measures what the consumer would be prepared to pay for that unit. The corresponding point on the supply curve measures marginal cost, the increase in total cost to the supplier for the corresponding unit of the good. The price in equilibrium is determined by supply and demand. In a perfectly competitive market, supply and demand equate marginal cost and marginal utility at equilibrium.On the supply side of the market, some factors of production are described as (relatively) variable in the short run, which affects the cost of changing output levels. Their usage rates can be changed easily, such as electrical power, raw-material inputs, and over-time and temp work. Other inputs are relatively fixed, such as plant and equipment and key personnel. In the long run, all inputs may be adjusted by management. These distinctions translate to differences in the elasticity (responsiveness) of the supply curve in the short and long runs and corresponding differences in the price-quantity change from a shift on the supply or demand side of the market.Marginalist theory, such as above, describes the consumers as attempting to reach most-preferred positions, subject to income and wealth constraints while producers attempt to maximize profits subject to their own constraints, including demand for goods produced, technology, and the price of inputs. For the consumer, that point comes where marginal utility of a good, net of price, reaches zero, leaving no net gain from further consumption increases. Analogously, the producer compares marginal revenue (identical to price for the perfect competitor) against the marginal cost of a good, with marginal profit the difference. At the point where marginal profit reaches zero, further increases in production of the good stop. For movement to market equilibrium and for changes in equilibrium, price and quantity also change "at the margin": more-or-less of something, rather than necessarily all-or-nothing.Other applications of demand and supply include the distribution of income among the factors of production, including labour and capital, through factor markets. In a competitive labour market for example the quantity of labour employed and the price of labour (the wage rate) depends on the demand for labour (from employers for production) and supply of labour (from potential workers). Labour economics examines the interaction of workers and employers through such markets to explain patterns and changes of wages and other labour income, labour mobility, and (un)employment, productivity through human capital, and related public-policy issues.Demand-and-supply analysis is used to explain the behaviour of perfectly competitive markets, but as a standard of comparison it can be extended to any type of market. It can also be generalized to explain variables across the economy, for example, total output (estimated as real GDP) and the general price level, as studied in macroeconomics. Tracing the qualitative and quantitative effects of variables that change supply and demand, whether in the short or long run, is a standard exercise in applied economics. Economic theory may also specify conditions such that supply and demand through the market is an efficient mechanism for allocating resources. Firms People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through firms. The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organize their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organization generalizes from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimize business decisions, including unit-cost minimization and profit maximization, given the firm's objectives and constraints imposed by technology and market conditions. Uncertainty and game theory Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organization, discussed above, to model different types of firm behaviour, for example in an oligopolistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. As a method heavily used in behavioural economics, it postulates that agents choose strategies to maximize their payoffs, given the strategies of other agents with at least partially conflicting interests.In this, it generalizes maximization approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as formulation of nuclear strategies, ethics, political science, and evolutionary biology.Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation.Some market organizations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's "Market for Lemons" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a "lemon" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving).Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market ("incomplete markets"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure. Market failure The term "market failure" encompasses several problems which may undermine standard economic assumptions. Although economists categorize market failures differently, the following categories emerge in the main texts.Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.Natural monopoly, or the overlapping concepts of "practical" and "technical" monopoly, is an extreme case of failure of competition as a restraint on producers. Extreme economies of scale are one possible cause.Public goods are goods which are undersupplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidize or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesized long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.Some specialized fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or "public bads".Policy options include regulations that reflect cost-benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights. Public sector Public finance is the field of economics that deals with budgeting the revenues and expenditures of a public sector entity, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost-benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like.Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society. Macroeconomics Macroeconomics examines the economy as a whole to explain broad aggregates and their interactions "top down", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.Since at least the 1960s, macroeconomics has been characterized by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject.Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth. Growth Growth economics studies factors that explain economic growth – the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the level of output per capita between countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting. Business cycle The economics of a depression were the spur for the creation of "macroeconomics" as a separate discipline field of study. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled The General Theory of Employment, Interest and Money outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilize output over the business cycle. Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of The General Theory.Over the years, understanding of the business cycle has branched into various research programmes, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with neoclassical economics, stating that Keynesianism is correct in the short run but qualified by neoclassical-like considerations in the intermediate and long run.New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and "rational expectations" theory, led by Robert Lucas, and real business cycle theory.In contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are "sticky", which means they do not adjust instantaneously to changes in economic conditions.Thus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the "long run" may be very long. Unemployment The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes.Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Wages may be too high because of minimum wage laws or union activity. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment. Inflation and monetary policy Money is a means of final payment for goods in most price system economies and the unit of account in which prices are typically stated. An apt statement by Francis Amasa Walker, a well-known economist is, "Money is what money does." Money has a general acceptability, a relative consistency in value, divisibility, durability, portability, elastic in supply and survives with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others.As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialized producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.At the level of an economy, theory and evidence are consistent with a positive relationship running from the total money supply to the nominal value of total output and to the general price level. For this reason, management of the money supply is a key aspect of monetary policy. Fiscal policy Governments implement fiscal policy that influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government use resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.Sceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from fiscal policy will be offset by the increased savings rate intended to pay for future higher taxes. International economics International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalization.The distinct field of development economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.Economic systems is the branch of economics that studies the methods and institutions by which societies determine the ownership, direction, and allocation of economic resources. An economic system of a society is the unit of analysis.Among contemporary systems at different ends of the organizational spectrum are socialist systems and capitalist systems, in which most production occurs in respectively state-run and private enterprises. In between are mixed economies. A common element is the interaction of economic and political influences, broadly described as political economy. Comparative economic systems studies the relative performance and behaviour of different economies or systems.The U.S. Export-Import Bank defines a Marxist-Lenninist state as having a centrally planned economy. They are now rare, examples can still be seen in Cuba, North Korea and Laos. Practice Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialize in econometrics and mathematical methods. Theory Mainstream economic theory relies upon a priori quantitative economic models, which employ a variety of concepts. Theory typically proceeds with an assumption of ceteris paribus, which means holding constant explanatory variables other than the one under consideration. When creating theories, the objective is to find ones which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories.In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modeling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models in microfoundations.The aforementioned microeconomic concepts play a major part in macroeconomic models – for instance, in monetary theory, the quantity theory of money predicts that increases in the money supply increase inflation, and inflation is assumed to be influenced by rational expectations. In development economics, slower growth in developed nations has been sometimes predicted because of the declining marginal returns of investment and capital, and this has been observed in the Four Asian Tigers. Sometimes an economic hypothesis is only qualitative, not quantitative.Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, Paul Samuelson's treatise Foundations of Economic Analysis (1947) used mathematical methods to represent the theory, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called operationally meaningful theorems in economics, which are theorems that can conceivably be refuted by empirical data. Empirical investigation Economic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance ("signal strength") of the hypothesized relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.Criticism based on professional standards and non-replicability of results serve as further checks against bias, errors, and over-generalization, although much economic research has been accused of being non-replicable, and prestigious journals have been accused of not facilitating replication through the provision of the code and data. Like theories, uses of test statistics are themselves open to critical analysis, although critical commentary on papers in economics in prestigious journals such as the American Economic Review has declined precipitously in the past 40 years. This has been attributed to journals' incentives to maximize citations in order to rank higher on the Social Science Citation Index (SSCI).In applied economics, input-output models employing linear programming methods are quite common. Large amounts of data are run through computer programs to analyse the impact of certain policies; IMPLAN is one well-known example.Experimental economics has promoted the use of scientifically controlled experiments. This has reduced long-noted distinction of economics from natural sciences allowed direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct; for example, the ultimatum game has revealed that people reject unequal offers.In behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a "genuine science." Profession The professionalization of economics, reflected in the growth of graduate programmes on the subject, has been described as "the main change in economics since around 1900". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study.In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national Treasury, Central Bank or Bureau of Statistics.The Nobel Memorial Prize in Economic Sciences (commonly known as the Nobel Prize in Economics) is a prize awarded to economists each year for outstanding intellectual contributions in the field. Related subjects Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, cultural economics, family economics and institutional economics.Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Georg Simmel's The Philosophy of Money (1900). More recently, the works of Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field. History Economic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Notable writers from Antiquity through to the Renaissance include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as "coming nearer than any other group to being the 'founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.Two groups, later called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy.Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject. Classical political economy The publication of Adam Smith's The Wealth of Nations in 1776, has been described as "the effective birth of economics as a separate discipline." The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the Physiocratic idea that only agriculture was productive.Smith discusses potential benefits of specialization by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His "theorem" that "the division of labor is limited by the extent of the market" has been described as the "core of a theory of the functions of firm and industry" and a "fundamental principle of economic organization." To Smith has also been ascribed "the most important substantive proposition in all of economics" and foundation of resource-allocation theory – that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).In an argument that includes "one of the most famous passages in all economics," Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this:He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it.The Rev. Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Lincoln Simon has criticized Malthus's conclusions.While Adam Smith emphasized the production of income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was the first to state and prove the principle of comparative advantage, according to which each country should specialize in producing and exporting goods in that it has a lower relative cost of production, rather relying only on its own production. It has been termed a "fundamental analytical explanation" for gains from trade.Coming at the end of the Classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.Value theory was important in classical theory. Smith wrote that the "real price of every thing ... is the toil and trouble of acquiring it" as influenced by its scarcity. Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size. Marxism Marxist (later, Marxian) economics descends from classical economics. It derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in German in 1867. In it, Marx focused on the labour theory of value and the theory of surplus value which, he believed, explained the exploitation of labour by capital. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production and the theory of surplus value demonstrated how the workers only got paid a proportion of the value their work had created. Neoclassical economics At the dawn as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his "Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth" (1803). These three items are considered by the science only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has prevailed up to our time, saved by substituting the word "wealth" for "goods and services" meaning that wealth may include non material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of a particular aspect of human behaviour, the one that falls under the influence of scarcity, which forces people to choose, allocate scarce resources to competing ends, and economize (seeking the greatest welfare while avoiding the wasting of scarce resources). For Robbins, the insufficiency was solved, and his definition allows us to proclaim, with an easy conscience, education economics, safety and security economics, health economics, war economics, and of course, production, distribution and consumption economics as valid subjects of the economic science.Citing Robbins: "Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses". After discussing it for decades, Robbins' definition became widely accepted by mainstream economists, and it has opened way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition. Due to the lack of strong consensus, and that production, distribution and consumption of goods and services is the prime area of study of economics, the old definition still stands in many quarters.A body of theory later termed "neoclassical economics" or "marginalism" formed from about 1870 to 1910. The term "economics" was popularized by such neoclassical economists as Alfred Marshall as a concise synonym for 'economic science' and a substitute for the earlier "political economy". This corresponded to the influence on the subject of mathematical methods used in the natural sciences.Neoclassical economics systematized supply and demand as joint determinants of price and quantity in market equilibrium, affecting both the allocation of output and the distribution of income. It dispensed with the labour theory of value inherited from classical economics in favour of a marginal utility theory of value on the demand side and a more general theory of costs on the supply side. In the 20th century, neoclassical theorists moved away from an earlier notion suggesting that total utility for a society could be measured in favour of ordinal utility, which hypothesizes merely behaviour-based relations across persons.In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.Neoclassical economics is occasionally referred as orthodox economics whether by its critics or sympathizers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalize earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income.Neoclassical economics studies the behaviour of individuals, households, and organizations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more resource-controlling players to attain the best possible outcome under bounded rational conditions. In other words, resource-controlling agents maximize value subject to the constraints imposed by the information the agents have, their cognitive limitations, and the finite amount of time they have to make and execute a decision. Economic science centres on the activities of the economic agents that comprise society. They are the focus of economic analysis.An approach to understanding these processes, through the study of agent behaviour under scarcity, may go as follows:The continuous interplay (exchange or trade) done by economic actors in all markets sets the prices for all goods and services which, in turn, make the rational managing of scarce resources possible. At the same time, the decisions (choices) made by the same actors, while they are pursuing their own interest, determine the level of output (production), consumption, savings, and investment, in an economy, as well as the remuneration (distribution) paid to the owners of labour (in the form of wages), capital (in the form of profits) and land (in the form of rent). Each period, as if they were in a giant feedback system, economic players influence the pricing processes and the economy, and are in turn influenced by them until a steady state (equilibrium) of all variables involved is reached or until an external shock throws the system toward a new equilibrium point. Because of the autonomous actions of rational interacting agents, the economy is a complex adaptive system. Keynesian economics Keynesian economics derives from John Maynard Keynes, in particular his book The General Theory of Employment, Interest and Money (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low "effective demand" and why even price flexibility and monetary policy might be unavailing. The term "revolutionary" has been applied to the book in its impact on economic analysis.Keynesian economics has two successors. Post-Keynesian economics also concentrates on macroeconomic rigidities and adjustment processes. Research on micro foundations for their models is represented as based on real-life practices rather than simple optimizing models. It is generally associated with the University of Cambridge and the work of Joan Robinson.New-Keynesian economics is also associated with developments in the Keynesian fashion. Within this group researchers tend to share with other economists the emphasis on models employing micro foundations and optimizing behaviour but with a narrower focus on standard Keynesian themes such as price and wage rigidity. These are usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones. Chicago school of economics The Chicago School of economics is best known for its free market advocacy and monetarist ideas. According to Milton Friedman and monetarists, market economies are inherently stable if the money supply does not greatly expand or contract. Ben Bernanke, former Chairman of the Federal Reserve, is among the economists today generally accepting Friedman's analysis of the causes of the Great Depression.Milton Friedman effectively took many of the basic principles set forth by Adam Smith and the classical economists and modernized them. One example of this is his article in the September 1970 issue of The New York Times Magazine, where he claims that the social responsibility of business should be "to use its resources and engage in activities designed to increase its profits ... (through) open and free competition without deception or fraud." Other schools and approaches Other well-known schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Austrian School, the Freiburg School, the School of Lausanne, post-Keynesian economics and the Stockholm school. Contemporary mainstream economics is sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago-school approach.Within macroeconomics there is, in general order of their appearance in the literature; classical economics, Keynesian economics, the neoclassical synthesis, post-Keynesian economics, monetarism, new classical economics, and supply-side economics. Alternative developments include ecological economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, feminist economics and biophysical economics. Agreements According to various random and anonymous surveys of members of the American Economic Association, economists have agreement about the following propositions by percentage:A ceiling on rents reduces the quantity and quality of housing available. (93% agree)Tariffs and import quotas usually reduce general economic welfare. (93% agree)Flexible and floating exchange rates offer an effective international monetary arrangement. (90% agree)Fiscal policy (e.g., tax cut and/or government expenditure increase) has a significant stimulative impact on a less than fully employed economy. (90% agree)The United States should not restrict employers from outsourcing work to foreign countries. (90% agree)Economic growth in developed countries like the United States leads to greater levels of well-being. (88% agree)The United States should eliminate agricultural subsidies. (85% agree)An appropriately designed fiscal policy can increase the long-run rate of capital formation. (85% agree)Local and state governments should eliminate subsidies to professional sports franchises. (85% agree)If the federal budget is to be balanced, it should be done over the business cycle rather than yearly. (85% agree)The gap between Social Security funds and expenditures will become unsustainably large within the next fifty years if current policies remain unchanged. (85% agree)Cash payments increase the welfare of recipients to a greater degree than do transfers-in-kind of equal cash value. (84% agree)A large federal budget deficit has an adverse effect on the economy. (83% agree)The redistribution of income in the United States is a legitimate role for the government. (83% agree)Inflation is caused primarily by too much growth in the money supply. (83% agree)The United States should not ban genetically modified crops. (82% agree)A minimum wage increases unemployment among young and unskilled workers. (79% agree)The government should restructure the welfare system along the lines of a “negative income tax.” (79% agree)Effluent taxes and marketable pollution permits represent a better approach to pollution control than imposition of pollution ceilings. (78% agree)Government subsidies on ethanol in the United States should be reduced or eliminated. (78% agree) Criticisms  General criticisms "The dismal science" is a derogatory alternative name for economics devised by the Victorian historian Thomas Carlyle in the 19th century. It is often stated that Carlyle gave economics the nickname "the dismal science" as a response to the late 18th century writings of The Reverend Thomas Robert Malthus, who grimly predicted that starvation would result, as projected population growth exceeded the rate of increase in the food supply. However, the actual phrase was coined by Carlyle in the context of a debate with John Stuart Mill on slavery, in which Carlyle argued for slavery, while Mill opposed it.Some economists, like John Stuart Mill or Léon Walras, have maintained that the production of wealth should not be tied to its distribution.In The Wealth of Nations, Adam Smith addressed many issues that are currently also the subject of debate and dispute. Smith repeatedly attacks groups of politically aligned individuals who attempt to use their collective influence to manipulate a government into doing their bidding. In Smith's day, these were referred to as factions, but are now more commonly called special interests, a term which can comprise international bankers, corporate conglomerations, outright oligopolies, monopolies, trade unions and other groups.Economics per se, as a social science, is independent of the political acts of any government or other decision-making organization, however, many policymakers or individuals holding highly ranked positions that can influence other people's lives are known for arbitrarily using a plethora of economic concepts and rhetoric as vehicles to legitimize agendas and value systems, and do not limit their remarks to matters relevant to their responsibilities. The close relation of economic theory and practice with politics is a focus of contention that may shade or distort the most unpretentious original tenets of economics, and is often confused with specific social agendas and value systems.Notwithstanding, economics legitimately has a role in informing government policy. It is, indeed, in some ways an outgrowth of the older field of political economy. Some academic economic journals are currently focusing increased efforts on gauging the consensus of economists regarding certain policy issues in hopes of effecting a more informed political environment. Currently, there exists a low approval rate from professional economists regarding many public policies. Policy issues featured in a recent survey of American Economic Association economists include trade restrictions, social insurance for those put out of work by international competition, genetically modified foods, curbside recycling, health insurance (several questions), medical malpractice, barriers to entering the medical profession, organ donations, unhealthy foods, mortgage deductions, taxing internet sales, Wal-Mart, casinos, ethanol subsidies, and inflation targeting.In Steady State Economics 1977, leading ecological economist and steady-state theorist Herman Daly argues that there exist logical inconsistencies between the emphasis placed on economic growth and the limited availability of natural resources.Issues like central bank independence, central bank policies and rhetoric in central bank governors discourse or the premises of macroeconomic policies (monetary and fiscal policy) of the state, are focus of contention and criticism.Deirdre McCloskey has argued that many empirical economic studies are poorly reported, and she and Stephen Ziliak argue that although her critique has been well-received, practice has not improved. This latter contention is controversial.A 2002 International Monetary Fund study looked at "consensus forecasts" (the forecasts of large groups of economists) that were made in advance of 60 different national recessions in the 1990s: in 97% of the cases the economists did not predict the contraction a year in advance. On those rare occasions when economists did successfully predict recessions, they significantly underestimated their severity. Criticisms of assumptions Economics has been subject to criticism that it relies on unrealistic, unverifiable, or highly simplified assumptions, in some cases because these assumptions simplify the proofs of desired conclusions. Examples of such assumptions include perfect information, profit maximization and rational choices. The field of information economics includes both mathematical-economical research and also behavioural economics, akin to studies in behavioural psychology.Nevertheless, prominent mainstream economists such as Keynes and Joskow have observed that much of economics is conceptual rather than quantitative, and difficult to model and formalize quantitatively. In a discussion on oligopoly research, Paul Joskow pointed out in 1975 that in practice, serious students of actual economies tended to use "informal models" based upon qualitative factors specific to particular industries. Joskow had a strong feeling that the important work in oligopoly was done through informal observations while formal models were "trotted out ex post". He argued that formal models were largely not important in the empirical work, either, and that the fundamental factor behind the theory of the firm, behaviour, was neglected.In recent years, feminist critiques of neoclassical economic models gained prominence, leading to the formation of feminist economics. Contrary to common conceptions of economics as a positive and objective science, feminist economists call attention to the social construction of economics and highlight the ways in which its models and methods reflect masculine preferences. Primary criticisms focus on failures to account for: the selfish nature of actors (homo economicus); exogenous tastes; the impossibility of utility comparisons; the exclusion of unpaid work; and the exclusion of class and gender considerations. Feminist economics developed to address these concerns, and the field now includes critical examinations of many areas of economics including paid and unpaid work, economic epistemology and history, globalization, household economics and the care economy. In 1988, Marilyn Waring published the book If Women Counted, in which she argues that the discipline of economics ignores women's unpaid work and the value of nature; according to Julie A. Nelson, If Women Counted "showed exactly how the unpaid work traditionally done by women has been made invisible within national accounting systems" and "issued a wake-up call to issues of ecological sustainability." Bjørnholt and McKay argue that the financial crisis of 2007–08 and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet.Philip Mirowski observes thatThe imperatives of the orthodox research programme [of economic science] leave little room for maneuver and less room for originality. ... These mandates ... Appropriate as many mathematical techniques and metaphorical expressions from contemporary respectable science, primarily physics as possible. ... Preserve to the maximum extent possible the attendant nineteenth-century overtones of "natural order" ... Deny strenuously that neoclassical theory slavishly imitates physics. ... Above all, prevent all rival research programmes from encroaching ... by ridiculing all external attempts to appropriate twentieth century physics models. ... All theorizing is [in this way] held hostage to nineteenth-century concepts of energy.In a series of peer-reviewed journal and conference papers and books published over a period of several decades, John McMurtry has provided extensive criticism of what he terms the "unexamined assumptions and implications [of economics], and their consequent cost to people's lives."Nassim Nicholas Taleb and Michael Perelman are two additional scholars who criticized conventional or mainstream economics. Taleb opposes most economic theorizing, which in his view suffers acutely from the problem of overuse of Plato's Theory of Forms, and calls for cancellation of the Nobel Memorial Prize in Economics, saying that the damage from economic theories can be devastating. Michael Perelman provides extensive criticism of economics and its assumptions in all his books (and especially his books published from 2000 to date), papers and interviews.Despite these concerns, mainstream graduate programs have become increasingly technical and mathematical. See also Business ethicsEconomics terminology that differs from common usageEconomic ideologyEconomic policyEconomic unionFree tradeList of economic communitiesList of economics filmsList of free trade agreementsSocioeconomicsGeneral:Glossary of economicsIndex of economics articlesOutline of economics References  Further reading Grinin, L., Korotayev, A. and Tausch A. (2016) Economic Cycles, Crises, and the Global Periphery. Springer International Publishing, Heidelberg, New York, Dordrecht, London, ISBN 978-3-319-17780-9; http://www.springer.com/de/book/9783319412603McCann, Charles Robert, Jr., 2003. The Elgar Dictionary of Economic Quotations, Edward Elgar. Preview.Jean Baptiste Say (1821). A Treatise on Political Economy: Or The Production, Distribution, and Consumption of Wealth. one. Wells and Lilly. Jean Baptiste Say (1821). A Treatise on Political Economy; Or The Production, Distribution, and Consumption of Wealth. two. Wells and Lilly. Tausch, Arno (2015). The political algebra of global value change. General models and implications for the Muslim world. With Almas Heshmati and Hichem Karoui. (1st ed.). Nova Science Publishers, New York. ISBN 978-1-62948-899-8.  External links General informationInstitutions and organizationsStudy resources
Marketing is the study and management of exchange relationships. The American Marketing Association has defined marketing as "the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large." Marketing is used to create the customer, to keep the customer and to satisfy the customer. With the customer as the focus of its activities, it can be concluded that Marketing is one of the premier components of Business Management - the other being Innovation. Other services and management activities such as Operations (or Production), Human Resources, Accounting, Law and Legal aspects can be "bought in" or "contracted out". Definition Marketing is defined by the American Marketing Association as "the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large." The term developed from the original meaning which referred literally to going to a market to buy or sell goods or services. Seen from a systems point of view, sales process engineering views marketing as "a set of processes that are interconnected and interdependent with other functions, whose methods can be improved using a variety of relatively new approaches."The Chartered Institute of Marketing defines marketing as "the management process responsible for identifying, anticipating and satisfying customer requirements profitably." A similar concept is the value-based marketing which states the role of marketing to contribute to increasing shareholder value. In this context, marketing can be defined as "the management process that seeks to maximise returns to shareholders by developing relationships with valued customers and creating a competitive advantage."Marketing practice tended to be seen as a creative industry in the past, which included advertising, distribution and selling. However, because the academic study of marketing makes extensive use of social sciences, psychology, sociology, mathematics, economics, anthropology and neuroscience, the profession is now widely recognized as a science, allowing numerous universities to offer Master-of-Science (MSc) programmes. The overall process starts with marketing research and goes through market segmentation, business planning and execution, ending with pre and post-sales promotional activities. It is also related to many of the creative arts. The marketing literature is also adept at re-inventing itself and its vocabulary according to the times and the culture. The marketing concept The term marketing concept pertains to the fundamental premise of modern marketing. This concept proposes that in order to satisfy its organizational objectives, an organization should anticipate the needs and wants of consumers and satisfy these more effectively than competitors. Marketing and marketing concepts are directly related. Marketing orientations An orientation, in the marketing context, relates to a perception or attitude a firm holds towards its product or service, essentially concerning consumers and end-users. There exist several common orientations: Product orientation A firm employing a product orientation is chiefly concerned with the quality of its own product. A firm would also assume that as long as its product was of a high standard, people would buy and consume the product.This works most effectively when the firm has good insights about customers and their needs and desires, as for example in the case of Sony Walkman or Apple iPod, whether these derive from intuitions or research. Sales orientation A firm using a sales orientation focuses primarily on the selling/promotion of a particular product, and not determining new consumer desires as such. Consequently, this entails simply selling an already existing product, and using promotion techniques to attain the highest sales possible.Such an orientation may suit scenarios in which a firm holds dead stock, or otherwise sells a product that is in high demand, with little likelihood of changes in consumer tastes diminishing demand. Production orientation A firm focusing on a production orientation specializes in producing as much as possible of a given product or service. Thus, this signifies a firm exploiting economies of scale, until the minimum efficient scale is reached.A production orientation may be deployed when a high demand for a product or service exists, coupled with a good certainty that consumer tastes do not rapidly alter (similar to the sales orientation). Marketing orientation The marketing orientation is perhaps the most common orientation used in contemporary marketing. It involves a firm essentially basing its marketing plans around the marketing concept, and thus supplying products to suit new consumer tastes.As an example, a firm would employ market research to gauge consumer desires, use R&D to develop a product attuned to the revealed information, and then utilize promotion techniques to ensure persons know the product exists. The marketing orientation often has three prime facets, which are: Customer orientation A firm in the market economy can survive by producing goods that persons are willing and able to buy. Consequently, ascertaining consumer demand is vital for a firm's future viability and even existence as a going concern. Organizational orientation In this sense, a firm's marketing department is often seen as of prime importance within the functional level of an organization.Information from an organization's marketing department would be used to guide the actions of other department's within the firm. As an example, a marketing department could ascertain (via marketing research) that consumers desired a new type of product, or a new usage for an existing product. With this in mind, the marketing department would inform the R&D department to create a prototype of a product/service based on consumers' new desires.The production department would then start to manufacture the product, while the marketing department would focus on the promotion, distribution, pricing, etc. of the product. Additionally, a firm's finance department would be consulted, with respect to securing appropriate funding for the development, production and promotion of the product.Inter-departmental conflicts may occur, should a firm adhere to the marketing orientation. Production may oppose the installation, support and servicing of new capital stock, which may be needed to manufacture a new product. Finance may oppose the required capital expenditure, since it could undermine a healthy cash flow for the organization. Mutually beneficial exchange In a transaction in the market economy, a firm gains revenue, which thus leads to more profits/market share/sales. A consumer on the other hand gains the satisfaction of a need/want, utility, reliability and value for money from the purchase of a product or service. As no one has to buy goods from any one supplier in the market economy, firms must entice consumers to buy goods with contemporary marketing ideals. The Four Ps In the early 1960s, Professor Neil Borden at Harvard Business School identified a number of company performance actions that can influence the consumer decision to purchase goods or services. Borden suggested that all those actions of the company represented a “Marketing Mix”. Professor E. Jerome McCarthy, at the Michigan State University in the early 1960s, suggested that the Marketing Mix contained 4 elements: product, price, place and promotion.ProductThe product aspects of marketing deal with the specifications of the actual goods or services, and how it relates to the end-user's needs and wants. The scope of a product generally includes supporting elements such as warranties, guarantees, and support.PricingThis refers to the process of setting a price for a product, including discounts. The price need not be monetary; it can simply be what is exchanged for the product or services, e.g. time, energy, or attention. Methods of setting prices optimally are in the domain of pricing science.Placement (or distribution)This refers to how the product gets to the customer; for example, point-of-sale placement or retailing. This third P has also sometimes been called Place, referring to the channel by which a product or service is sold (e.g. online vs. retail), which geographic region or industry, to which segment (young adults, families, business people), etc. also referring to how the environment in which the product is sold in can affect sales.PromotionThis includes advertising, sales promotion, including promotional education, publicity, and personal selling. Branding refers to the various methods of promoting the product, brand, or company.These four elements are often referred to as the marketing mix, which a marketer can use to craft a marketing plan.The four Ps model is most useful when marketing low value consumer products. Industrial products, services, high value consumer products require adjustments to this model. Services marketing must account for the unique nature of services.Industrial or B2B marketing must account for the long term contractual agreements that are typical in supply chain transactions. Relationship marketing attempts to do this by looking at marketing from a long term relationship perspective rather than individual transactions.As a counter to this, Morgan, in Riding the Waves of Change (Jossey-Bass, 1988), suggests that one of the greatest limitations of the 4 Ps approach "is that it unconsciously emphasizes the inside–out view (looking from the company outwards), whereas the essence of marketing should be the outside–in approach".In order to recognize the different aspects of selling services, as opposed to Products, a further three Ps were added to make a range of Seven Ps for service industries:Process - the way in which orders are handled, customers are satisfied and the service is delivered.Physical Evidence - is tangible evidence of the service customers will receive (for example a holiday brochure).People - the people meeting and dealing with the customers.As markets have become more satisfied, the 7 Ps have become relevant to those companies selling products, as well as those solely involved with services: customers now differentiate between sellers of goods by the service they receive in the process from the people involved.Some authors cite a further P - Packaging - this is thought by many to be part of Product, but in certain markets (Japan, China for example) and with certain products (perfume, cosmetics) the packaging of a product has a greater importance - maybe even than the product itself. The marketing environment The term "marketing environment" relates to all of the factors (whether internal, external, direct or indirect) that affect a firm's marketing decision-making/planning. A firm's marketing environment consists of three main areas, which are:The macro-environment, over which a firm holds little controlThe micro-environment, over which a firm holds a greater amount (though not necessarily total) control The macro-environment A firm's marketing macro-environment consists of a variety of external factors that manifest on a large (or macro) scale. These are typically economic, social, political or technological phenomena. A common method of assessing a firm's macro-environment is via a PESTLE (Political, Economic, Social, Technological, Legal, Ecological) analysis. Within a PESTLE analysis, a firm would analyze national political issues, culture and climate, key macroeconomic conditions, health and indicators (such as economic growth, inflation, unemployment, etc.), social trends/attitudes, and the nature of technology's impact on its society and the business processes within the society. The micro-environment A firm's micro-environment comprises factors pertinent to the firm itself, or stakeholders closely connected with the firm or company.A firm's micro-environment typically spans:Customers/consumersEmployeesSuppliersThe MediaBy contrast to the macro-environment, an organization holds a greater degree of control over these factors. Marketing research Marketing research involves conducting research to support marketing activities, and the statistical interpretation of data into information. This information is then used by managers to plan marketing activities, gauge the nature of a firm's marketing environment, attain information from suppliers, etc.A distinction should be made between marketing research and market research. Market research pertains to research in a given market. As an example, a firm may conduct research in a target market, after selecting a suitable market segment. In contrast, marketing research relates to all research conducted within marketing. Thus, market research is a subset of marketing research.Marketing researchers use statistical methods (such as quantitative research, qualitative research, hypothesis tests, Chi-square tests, linear regression, correlation coefficients, frequency distributions, Poisson and binomial distributions, etc.) to interpret their findings and convert data into information. The Marketing Research Process Marketing research spans a number of stages, including:Define the problemDevelop a research planCollect the dataInterpret data into informationDisseminate information formally in the form of a report Market segmentation Market segmentation consists of taking the total heterogeneous market for a product and dividing it into several sub-markets or segments, each of which tends to be homogeneous in all significant aspects. The purposes of market segmentation Market segmentation is conducted for two main purposes, including:- A better allocation of a firm's finite resources- To better serve the more diversified tastes of contemporary Western consumersA firm only possesses a certain amount of resources. Accordingly, it must make choices (and appreciate the related costs) in servicing specific groups of consumers.Moreover, with more diversity in the tastes of modern consumers, firms are taking noting the benefit of servicing a multiplicity of new markets. Overview of segmentation process Segmentation can be defined in terms of the STP acronym, meaning Segment, Target, Position. Segment Segmentation involves the initial splitting up of consumers into persons of like needs/wants/tastes.Four commonly used criteria are used for segmentation, which include:Geographical (a country, region, city, town, etc.)Psychographic (i.e. personality traits or character traits which influence consumer behaviour)Demographic (e.g. age, gender, socio-economic class, etc.)Behavioural (e.g. brand loyalty, usage rate, etc.) Target Once a segment has been identified, a firm must ascertain whether the segment is beneficial for them to service.The DAMP acronym (meaning Discernable, Accessible, Measurable and Profitable) are used as criteria to gauge the viability of a target market. DAMP is explained in further detail below:- Discernable - how a segment can be differentiated from other segments.- Accessible - how a segment can be accessed via Marketing Communications produced by a firm- Measurable - can the segment be quantified and its size determined?- Profitable - can a sufficient return on investment be attained from a segment's servicing?The next step in the targeting process is the level of differentiation involved in a segment serving. Three modes of differentiation exist, which are commonly applied by firms. These are:Undifferentiated - where a company produces a like product for all of a market segmentDifferentiated - in which a firm produced slight modifications of a product within a segmentNiche - in which an organisation forges a product to satisfy a specialised target market Position Positioning concerns how to position a product in the minds of consumers.A firm often performs this by producing a perceptual map, which denotes products produced in its industry according to how consumers perceive their price and quality. From a product's placing on the map, a firm would tailor its marketing communications to suit meld with the product's perception among consumers. Marketing communications Marketing communications is defined by actions a firm takes to communicate with end-users, consumers and external parties. Marketing communications encompasses four distinct subsets, which are: Personal sales Oral presentation given by a salesperson who approaches individuals or a group of potential customers:Live, interactive relationshipPersonal interestAttention and responseInteresting presentationClear and thorough. Sales promotion Short-term incentives to encourage buying of products:Instant appealAnxiety to sellAn example is coupons or a sale. People are given an incentive to buy, but this does not build customer loyalty or encourage future repeat buys. A major drawback of sales promotion is that it is easily copied by competition. It cannot be used as a sustainable source of differentiation. Public Relations Public Relations (or PR, as an acronym) is the use of media tools by a firm in order to promote goodwill from an organization to a target market segment, or other consumers of a firm's good/service. PR stems from the fact that a firm cannot seek to antagonize or inflame its market base, due to incurring a lessened demand for its good/service. Organizations undertake PR in order to assure consumers, and to forestall negative perceptions towards it.PR can span:InterviewsSpeeches/PresentationsCorporate literature, such as financial statements, brochures, etc. Publicity Publicity involves attaining space in media, without having to pay directly for such coverage. As an example, an organization may have the launch of a new product covered by a newspaper or TV news segment. This benefits the firm in question since it is making consumers aware of its product, without necessarily paying a newspaper or television station to cover the event. Advertising Advertising occurs when a firm directly pays a media channel to publicize its product. Common examples of this include TV and radio adverts, billboards, branding, sponsorship, etc. Marketing communications "mix" Marketing communications is a "sub-mix" within the Promotion aspect of the marketing mix, as the exact nature of how to apply marketing communications depends on the nature of the product in question.Accordingly, a given product would require a unique communications mix, in order to convey successfully information to consumers. Some products may require a stronger emphasis on personal sales, while others may need more focus on advertising. Marketing Planning The area of marketing planning involves forging a plan for a firm's marketing activities. A marketing plan can also pertain to a specific product, as well as to an organisation's overall marketing strategy.Generally speaking, an organisation's marketing planning process is derived from its overall business strategy. Thus, when top management are devising the firm's strategic direction/mission, the intended marketing activities are incorporated into this plan. Marketing Planning Process Within the overall strategic marketing plan, the stages of the process are listed as thus:Mission StatementCorporate ObjectivesMarketing AuditSWOT analysisAssumptions arising from the Audit and SWOT analysisMarketing objectives derived from the assumptionsAn estimation of the expected results of the objectivesIdentification of alternative plans/mixesBudgeting for the marketing planA first-year implementation program Levels of marketing objectives within an organization As stated previously, the senior management of a firm would formulate a general business strategy for a firm. However, this general business strategy would be interpreted and implemented in different contexts throughout the firm. Corporate Corporate marketing objectives are typically broad-based in nature, and pertain to the general vision of the firm in the short, medium or long-term.As an example, if one pictures a group of companies (or a conglomerate), top management may state that sales for the group should increase by 25% over a ten year period. Strategic business unit Strategic business unit (SBU), in this case, means strategic business unit. An SBU is a subsidiary within a firm, which participates within a given market/industry. The SBU would embrace the corporate strategy, and attune it to its own particular industry. For instance, an SBU may partake in the sports goods industry. It thus would ascertain how it would attain additional sales of sports goods, in order to satisfy the overall business strategy. Functional The functional level relates to departments within the SBUs, such as marketing, finance, HR, production, etc. The functional level would adopt the SBU's strategy and determine how to accomplish the SBU's own objectives in its market.To use the example of the sports goods industry again, the marketing department would draw up marketing plans, strategies and communications to help the SBU achieve its marketing aims. Product Life Cycle The Product Life Cycle (or PLC, for short) is a tool used by marketing managers to gauge the progress of a product, especially relating to sales/revenue accrued over time. The PLC is based on a few key assumptions, including:- A given product would possess an Introduction, Growth, Maturity and Decline stage. - No product lasts perpetually on the market. - A firm must employ differing strategies, according to where a product is on the PLC. Introduction In this stage, a product is launched onto the market. To stimulate growth of sales/revenue, use of advertising may be high, in order heighten awareness of the product in question. Growth The product's sales/revenue is increasing, which may stimulate more marketing communications to sustain sales. More entrants enter into the market, to reap the apparent high profits that the industry is producing. Maturity A product's sales start to level off, and an increasing number of entrants to a market produce price falls for the product. Firms may utilise sales promotions to raise sales. Decline Demand for a good begins to taper off, and the firm may opt to discontinue manufacture of the product. This is so, if revenue for the product comes from efficiency savings in production, over actual sales of a good/service. However, if a product services a niche market, or is complementary to another product, it may continue manufacture of the product, despite a low level of sales/revenue being accrued. Customer focus Many companies today have a customer focus (or market orientation). This implies that the company focuses its activities and products on consumer demands. Generally there are three ways of doing this: the customer-driven approach, the sense of identifying market changes and the product innovation approach.In the consumer-driven approach, consumer wants are the drivers of all strategic marketing decisions. No strategy is pursued until it passes the test of consumer research. Every aspect of a market offering, including the nature of the product itself, is driven by the needs of potential consumers. The starting point is always the consumer. The rationale for this approach is that there is no point spending R&D funds developing products that people will not buy. History attests to many products that were commercial failures in spite of being technological breakthroughs.A formal approach to this customer-focused marketing is known as SIVA (Solution, Information, Value, Access). This system is basically the four Ps renamed and reworded to provide a customer focus.The SIVA Model provides a demand/customer centric version alternative to the well-known 4Ps supply side model (product, price, place, promotion) of marketing management. Product focus In a product innovation approach, the company pursues product innovation, then tries to develop a market for the product. Product innovation drives the process and marketing research is conducted primarily to ensure that profitable market segment(s) exist for the innovation. The rationale is that customers may not know what options will be available to them in the future so we should not expect them to tell us what they will buy in the future. However, marketers can aggressively over-pursue product innovation and try to overcapitalize on a niche. When pursuing a product innovation approach, marketers must ensure that they have a varied and multi-tiered approach to product innovation. It is claimed that if Thomas Edison depended on marketing research he would have produced larger candles rather than inventing light bulbs. Many firms, such as research and development focused companies, successfully focus on product innovation. Many purists doubt whether this is really a form of marketing orientation at all, because of the ex post status of consumer research. Some even question whether it is marketing.An emerging area of study and practice concerns internal marketing, or how employees are trained and managed to deliver the brand in a way that positively impacts the acquisition and retention of customers (employer branding).Diffusion of innovations research explores how and why people adopt new products, services and ideas.A relatively new form of marketing uses the Internet and is called Internet marketing or more generally e-marketing, affiliate marketing, desktop advertising or online marketing. It tries to perfect the segmentation strategy used in traditional marketing. It targets its audience more precisely, and is sometimes called personalized marketing or one-to-one marketing.With consumers' eroding attention span and willingness to give time to advertising messages, marketers are turning to forms of permission marketing such as branded content, custom media and reality marketing.The use of herd behavior in marketing.The Economist reported a recent conference in Rome on the subject of the simulation of adaptive human behavior. It shared mechanisms to increase impulse buying and get people "to buy more by playing on the herd instinct." The basic idea is that people will buy more of products that are seen to be popular, and several feedback mechanisms to get product popularity information to consumers are mentioned, including smart-cart technology and the use of Radio Frequency Identification Tag technology. A "swarm-moves" model was introduced by a Florida Institute of Technology researcher, which is appealing to supermarkets because it can "increase sales without the need to give people discounts."Marketing is also used to promote business' products and is a great way to promote the business.Other recent studies on the "power of social influence" include an "artificial music market in which some 14,000 people downloaded previously unknown songs" (Columbia University, New York); a Japanese chain of convenience stores which orders its products based on "sales data from department stores and research companies;" a Massachusetts company exploiting knowledge of social networking to improve sales; and online retailers who are increasingly informing consumers about "which products are popular with like-minded consumers" (e.g., Amazon, eBay). See also  Types of marketing  References  Bibliography Bartels, Robert. "The history of marketing thought." (1988). onlineChristensen, Clayton M. (1997), The innovator's dilemma: when new technologies cause great firms to fail, Boston, Massachusetts, USA: Harvard Business School Press, ISBN 978-0-87584-585-2.  (edit)Church, Roy, and Andrew Godley. The Emergence of Modern Marketing (2003) online editionHollander, Stanley C., et al. "Periodization in marketing history." Journal of Macromarketing 25.1 (2005): 32-41. onlineTedlow, Richard S., and Geoffrey G. Jones, eds. The Rise and Fall of Mass Marketing (Routledge, 2014).Weitz, Barton A. and Robin Wensley, eds. Handbook of Marketing (2002). External links  The dictionary definition of marketing at Wiktionary Quotations related to marketing at Wikiquote Marketing at Wikibooks
An airplane or aeroplane (informally plane) is a powered, fixed-wing aircraft that is propelled forward by thrust from a jet engine or propeller. Airplanes come in a variety of sizes, shapes, and wing configurations. The broad spectrum of uses for airplanes includes recreation, transportation of goods and people, military, and research. Commercial aviation is a massive industry involving the flying of tens of thousands of passengers daily on airliners. Most airplanes are flown by a pilot on board the aircraft, but some are designed to be remotely or computer-controlled.The Wright brothers invented and flew the first airplane in 1903, recognized as "the first sustained and controlled heavier-than-air powered flight". They built on the works of George Cayley dating from 1799, when he set forth the concept of the modern airplane (and later built and flew models and successful passenger-carrying gliders). Between 1867 and 1896, the German pioneer of human aviation Otto Lilienthal also studied heavier-than-air flight. Following its limited use in World War I, aircraft technology continued to develop. Airplanes had a presence in all the major battles of World War II. The first jet aircraft was the German Heinkel He 178 in 1939. The first jet airliner, the de Havilland Comet, was introduced in 1952. The Boeing 707, the first widely successful commercial jet, was in commercial service for more than 50 years, from 1958 to at least 2013. Etymology and usage First attested in English in the late 19th century (prior to the first sustained powered flight), the word airplane, like aeroplane, derives from the French aéroplane, which comes from the Greek ἀήρ (aēr), "air" and either Latin planus, "level", or Greek πλάνος (planos), "wandering". "Aéroplane" originally referred just to the wing, as it is a plane moving through the air. In an example of synecdoche, the word for the wing came to refer to the entire aircraft.In the United States and Canada, the term "airplane" is used for powered fixed-wing aircraft. In the United Kingdom and most of the Commonwealth, the term "aeroplane" (pronounced /ˈɛərəpleɪn/) is usually applied to these aircraft. History  Antecedents Many stories from antiquity involve flight, such as the Greek legend of Icarus and Daedalus, and the Vimana in ancient Indian epics. Around 400 BC in Greece, Archytas was reputed to have designed and built the first artificial, self-propelled flying device, a bird-shaped model propelled by a jet of what was probably steam, said to have flown some 200 m (660 ft). This machine may have been suspended for its flight.Some of the earliest recorded attempts with gliders were those by the 9th-century poet Abbas ibn Firnas and the 11th-century monk Eilmer of Malmesbury; both experiments injured their pilots. Leonardo da Vinci researched the wing design of birds and designed a man-powered aircraft in his Codex on the Flight of Birds (1502).In 1799, George Cayley set forth the concept of the modern airplane as a fixed-wing flying machine with separate systems for lift, propulsion, and control. Cayley was building and flying models of fixed-wing aircraft as early as 1803, and he built a successful passenger-carrying glider in 1853. In 1856, Frenchman Jean-Marie Le Bris made the first powered flight, by having his glider "L'Albatros artificiel" pulled by a horse on a beach. Then Alexander F. Mozhaisky also made some innovative designs. In 1883, the American John J. Montgomery made a controlled flight in a glider. Other aviators who made similar flights at that time were Otto Lilienthal, Percy Pilcher, and Octave Chanute.Sir Hiram Maxim built a craft that weighed 3.5 tons, with a 110-foot (34 meter) wingspan that was powered by two 360-horsepower (270 kW) steam engines driving two propellers. In 1894, his machine was tested with overhead rails to prevent it from rising. The test showed that it had enough lift to take off. The craft was uncontrollable, which Maxim, it is presumed, realized, because he subsequently abandoned work on it.In the 1890s, Lawrence Hargrave conducted research on wing structures and developed a box kite that lifted the weight of a man. His box kite designs were widely adopted. Although he also developed a type of rotary aircraft engine, he did not create and fly a powered fixed-wing aircraft.Between 1867 and 1896 the German pioneer of human aviation Otto Lilienthal developed heavier-than-air flight. He was the first person to make well-documented, repeated, successful gliding flights. Early powered flights The Wright brothers flights in 1903 are recognized by the Fédération Aéronautique Internationale (FAI), the standard setting and record-keeping body for aeronautics, as "the first sustained and controlled heavier-than-air powered flight". By 1905, the Wright Flyer III was capable of fully controllable, stable flight for substantial periods. The Wright brothers credited Otto Lilienthal as a major inspiration for their decision to pursue manned flight.In 1906, Alberto Santos-Dumont made what was claimed to be the first airplane flight unassisted by catapult and set the first world record recognized by the Aéro-Club de France by flying 220 meters (720 ft) in less than 22 seconds. This flight was also certified by the FAI.An early aircraft design that brought together the modern monoplane tractor configuration was the Blériot VIII design of 1908. It had movable tail surfaces controlling both yaw and pitch, a form of roll control supplied either by wing warping or by ailerons and controlled by its pilot with a joystick and rudder bar. It was an important predecessor of his later Blériot XI Channel-crossing aircraft of the summer of 1909.In Romania the aircraft, A. Vlaicu nr. 1, was finished in 1909, and was test flown on June 17, 1910. From the first flight the airplane had no need of changes. The plane was made from a single aluminum spar 10 m (33 ft) long which supported the entire aircraft, making it very easy to fly. Ten planes were made for the Romanian Air Force, being the second-ever military air force in the world.World War I served as a testbed for the use of the airplane as a weapon. Airplanes demonstrated their potential as mobile observation platforms, then proved themselves to be machines of war capable of causing casualties to the enemy. The earliest known aerial victory with a synchronized machine gun-armed fighter aircraft occurred in 1915, by German Luftstreitkräfte Leutnant Kurt Wintgens. Fighter aces appeared; the greatest (by number of Aerial Combat victories) was Manfred von Richthofen.Following WWI, aircraft technology continued to develop. Alcock and Brown crossed the Atlantic non-stop for the first time in 1919. The first international commercial flights took place between the United States and Canada in 1919.Airplanes had a presence in all the major battles of World War II. They were an essential component of the military strategies of the period, such as the German Blitzkrieg, The Battle of Britain, and the American and Japanese aircraft carrier campaigns of the Pacific War. Development of jet aircraft The first 'operational' jet aircraft was the German Heinkel He 178, which was tested in 1939. In 1943, the Messerschmitt Me 262, the first 'operational' jet fighter aircraft, went into service in the German Luftwaffe. In October 1947, the Bell X-1 was the first aircraft to exceed the speed of sound.The first jet airliner, the de Havilland Comet, was introduced in 1952. The Boeing 707, the first widely successful commercial jet, was in commercial service for more than 50 years, from 1958 to 2010. The Boeing 747 was the world's biggest passenger aircraft from 1970 until it was surpassed by the Airbus A380 in 2005. Propulsion  Propeller engines Smaller and older propeller planes make use of reciprocating engines (or piston engines) to turn a propeller to create thrust. The amount of thrust a propeller creates is determined by its disk area - the area in which the blades rotate. If the area is too small, efficiency is poor, and if the area is large, the propeller must rotate at a very low speed to avoid going supersonic and creating a lot of noise, and not much thrust. Because of this limitation, propellers are favored for planes which travel at below mach .5, while jets are a better choice above that speed. Propeller engines may be quieter than jet engines (though not always) and may cost less to purchase or maintain and so remain common on light general aviation aircraft such as the Cessna 172. Larger modern propeller planes such as the Dash 8 use a jet engine to turn the propeller, primarily because an equivalent piston engine in power output would be much larger and more complex. Jet engines Jet aircraft are propelled by jet engines, which are used because the aerodynamic limitations of propellers do not apply to jet propulsion. These engines are much more powerful than a reciprocating engine for a given size or weight and are comparatively quiet and work well at higher altitude. Most modern jet planes use turbofan jet engines which balance the advantages of a propeller, while retaining the exhaust speed and power of a jet. This is essentially a ducted propeller attached to a jet engine, much like a turboprop, but with a smaller diameter. When installed on an airliner, it is efficient so long as it remains below the speed of sound (or subsonic). Jet fighters and other supersonic aircraft that do not spend a great deal of time supersonic also often use turbofans, but to function, air intake ducting is needed to slow the air down so that when it arrives at the front of the turbofan, it is subsonic. When passing through the engine, it is then re-accelerated back to supersonic speeds. To further boost the power output, fuel is dumped into the exhaust stream, where it ignites. This is called an afterburner and has been used on both pure jet aircraft and turbojet aircraft although it is only normally used on combat aircraft due to the amount of fuel consumed, and even then may only be used for short periods of time. Supersonic airliners (e.g. Concorde) are no longer in use largely because flight at supersonic speed creates a sonic boom which is prohibited in most heavily populated areas, and because of the much higher consumption of fuel supersonic flight requires.Jet aircraft possess high cruising speeds (700 to 900 km/h (430 to 560 mph)) and high speeds for takeoff and landing (150 to 250 km/h (93 to 155 mph)). Due to the speed needed for takeoff and landing, jet aircraft use flaps and leading edge devices to control the lift and speed. Many jet aircraft also use thrust reversers to slow down the aircraft upon landing. Electric engines An electric aircraft runs on electric motors rather than internal combustion engines, with electricity coming from fuel cells, solar cells, ultracapacitors, power beaming, or batteries. Currently, flying electric aircraft are mostly experimental prototypes, including manned and unmanned aerial vehicles, but there are some production models on the market already. Rocket engines In World War II, the Germans deployed the Me 163 Komet rocket-powered aircraft. The first plane to break the sound barrier in level flight was a rocket plane – the Bell X-1. The later North American X-15 broke many speed and altitude records and laid much of the groundwork for later aircraft and spacecraft design. Rocket aircraft are not in common usage today, although rocket-assisted take offs are used for some military aircraft. Recent rocket aircraft include the SpaceShipOne and the XCOR EZ-Rocket.There are many rocket-powered aircraft/spacecraft planes, the spaceplanes, that are designed to fly outside Earth's atmosphere. Ramjet and scramjet engines A ramjet is a form of jet engine that contains no major moving parts and can be particularly useful in applications requiring a small and simple engine for high-speed use, such as with missiles. Ramjets require forward motion before they can generate thrust and so are often used in conjunction with other forms of propulsion, or with an external means of achieving sufficient speed. The Lockheed D-21 was a Mach 3+ ramjet-powered reconnaissance drone that was launched from a parent aircraft. A ramjet uses the vehicle's forward motion to force air through the engine without resorting to turbines or vanes. Fuel is added and ignited, which heats and expands the air to provide thrust.A scramjet is a supersonic ramjet and aside from differences with dealing with internal supersonic airflow works like a conventional ramjet. This type of engine requires a very high initial speed in order to work. The NASA X-43, an experimental unmanned scramjet, set a world speed record in 2004 for a jet-powered aircraft with a speed of Mach 9.7, nearly 7,500 miles per hour (12,100 km/h). Design and manufacture Most airplanes are constructed by companies with the objective of producing them in quantity for customers. The design and planning process, including safety tests, can last up to four years for small turboprops or longer for larger planes.During this process, the objectives and design specifications of the aircraft are established. First the construction company uses drawings and equations, simulations, wind tunnel tests and experience to predict the behavior of the aircraft. Computers are used by companies to draw, plan and do initial simulations of the aircraft. Small models and mockups of all or certain parts of the plane are then tested in wind tunnels to verify its aerodynamics.When the design has passed through these processes, the company constructs a limited number of prototypes for testing on the ground. Representatives from an aviation governing agency often make a first flight. The flight tests continue until the aircraft has fulfilled all the requirements. Then, the governing public agency of aviation of the country authorizes the company to begin production.In the United States, this agency is the Federal Aviation Administration (FAA), and in the European Union, European Aviation Safety Agency (EASA). In Canada, the public agency in charge and authorizing the mass production of aircraft is Transport Canada.In the case of international sales, a license from the public agency of aviation or transport of the country where the aircraft is to be used is also necessary. For example, airplanes made by the European company, Airbus, need to be certified by the FAA to be flown in the United States, and airplanes made by U.S.-based Boeing need to be approved by the EASA to be flown in the European Union.Quieter planes are becoming more and more necessary due to the increase in air traffic, particularly over urban areas, as aircraft noise pollution is a major concern.Small planes can be designed and constructed by amateurs as homebuilts. Other homebuilt aircraft can be assembled using pre-manufactured kits of parts that can be assembled into a basic plane and must then be completed by the builder.There are few companies that produce planes on a large scale. However, the production of a plane for one company is a process that actually involves dozens, or even hundreds, of other companies and plants, that produce the parts that go into the plane. For example, one company can be responsible for the production of the landing gear, while another one is responsible for the radar. The production of such parts is not limited to the same city or country; in the case of large plane manufacturing companies, such parts can come from all over the world.The parts are sent to the main plant of the plane company, where the production line is located. In the case of large planes, production lines dedicated to the assembly of certain parts of the plane can exist, especially the wings and the fuselage.When complete, a plane is rigorously inspected to search for imperfections and defects. After approval by inspectors, the plane is put through a series of flight tests to assure that all systems are working correctly and that the plane handles properly. Upon passing these tests, the plane is ready to receive the "final touchups" (internal configuration, painting, etc.), and is then ready for the customer. Characteristics  Airframe The structural parts of a fixed-wing aircraft are called the airframe. The parts present can vary according to the aircraft's type and purpose. Early types were usually made of wood with fabric wing surfaces, When engines became available for powered flight around a hundred years ago, their mounts were made of metal. Then as speeds increased more and more parts became metal until by the end of WWII all-metal aircraft were common. In modern times, increasing use of composite materials has been made.Typical structural parts include:One or more large horizontal wings, often with an airfoil cross-section shape. The wing deflects air downward as the aircraft moves forward, generating lifting force to support it in flight. The wing also provides stability in roll to stop the aircraft from rolling to the left or right in steady flight.A fuselage, a long, thin body, usually with tapered or rounded ends to make its shape aerodynamically smooth. The fuselage joins the other parts of the airframe and usually contains important things such as the pilot, payload and flight systems.A vertical stabilizer or fin is a vertical wing-like surface mounted at the rear of the plane and typically protruding above it. The fin stabilizes the plane's yaw (turn left or right) and mounts the rudder which controls its rotation along that axis.A horizontal stabilizer or tailplane, usually mounted at the tail near the vertical stabilizer. The horizontal stabilizer is used to stabilize the plane's pitch (tilt up or down) and mounts the elevators which provide pitch control.Landing gear, a set of wheels, skids, or floats that support the plane while it is on the surface. On seaplanes the bottom of the fuselage or floats (pontoons) support it while on the water. On some planes the landing gear retracts during flight to reduce drag. Wings The wings of a fixed-wing aircraft are static planes extending either side of the aircraft. When the aircraft travels forwards, air flows over the wings which are shaped to create lift. This shape is called an airfoil and is shaped like a bird's wing. Wing structure Airplanes have flexible wing surfaces which are stretched across a frame and made rigid by the lift forces exerted by the airflow over them. Larger aircraft have rigid wing surfaces which provide additional strength.Whether flexible or rigid, most wings have a strong frame to give them their shape and to transfer lift from the wing surface to the rest of the aircraft. The main structural elements are one or more spars running from root to tip, and many ribs running from the leading (front) to the trailing (rear) edge.Early airplane engines had little power, and lightness was very important. Also, early airfoil sections were very thin, and could not have a strong frame installed within. So until the 1930s most wings were too lightweight to have enough strength and external bracing struts and wires were added. When the available engine power increased during the 1920s and 30s, wings could be made heavy and strong enough that bracing was not needed any more. This type of unbraced wing is called a cantilever wing. Wing configuration The number and shape of the wings varies widely on different types. A given wing plane may be full-span or divided by a central fuselage into port (left) and starboard (right) wings. Occasionally even more wings have been used, with the three-winged triplane achieving some fame in WWI. The four-winged quadruplane and other multiplane designs have had little success.A monoplane has a single wing plane, a biplane has two stacked one above the other, a tandem wing has two placed one behind the other. When the available engine power increased during the 1920s and 30s and bracing was no longer needed, the unbraced or cantilever monoplane became the most common form of powered type.The wing planform is the shape when seen from above. To be aerodynamically efficient, a wing should be straight with a long span from side to side but have a short chord (high aspect ratio). But to be structurally efficient, and hence light weight, a wing must have a short span but still enough area to provide lift (low aspect ratio).At transonic speeds (near the speed of sound), it helps to sweep the wing backwards or forwards to reduce drag from supersonic shock waves as they begin to form. The swept wing is just a straight wing swept backwards or forwards.The delta wing is a triangle shape which may be used for a number of reasons. As a flexible Rogallo wing it allows a stable shape under aerodynamic forces, and so is often used for ultralight aircraft and even kites. As a supersonic wing it combines high strength with low drag and so is often used for fast jets.A variable geometry wing can be changed in flight to a different shape. The variable-sweep wing transforms between an efficient straight configuration for takeoff and landing, to a low-drag swept configuration for high-speed flight. Other forms of variable planform have been flown, but none have gone beyond the research stage. Fuselage A fuselage is a long, thin body, usually with tapered or rounded ends to make its shape aerodynamically smooth. The fuselage may contain the flight crew, passengers, cargo or payload, fuel and engines. The pilots of manned aircraft operate them from a cockpit located at the front or top of the fuselage and equipped with controls and usually windows and instruments. A plane may have more than one fuselage, or it may be fitted with booms with the tail located between the booms to allow the extreme rear of the fuselage to be useful for a variety of purposes. Wings vs. bodies  Flying wing A flying wing is a tailless aircraft which has no definite fuselage. Most of the crew, payload and equipment are housed inside the main wing structure.The flying wing configuration was studied extensively in the 1930s and 1940s, notably by Jack Northrop and Cheston L. Eshelman in the United States, and Alexander Lippisch and the Horten brothers in Germany. After the war, a number of experimental designs were based on the flying wing concept, but the known difficulties remained intractable. Some general interest continued until the early 1950s but designs did not necessarily offer a great advantage in range and presented a number of technical problems, leading to the adoption of "conventional" solutions like the Convair B-36 and the B-52 Stratofortress. Due to the practical need for a deep wing, the flying wing concept is most practical for designs in the slow-to-medium speed range, and there has been continual interest in using it as a tactical airlifter design.Interest in flying wings was renewed in the 1980s due to their potentially low radar reflection cross-sections. Stealth technology relies on shapes which only reflect radar waves in certain directions, thus making the aircraft hard to detect unless the radar receiver is at a specific position relative to the aircraft - a position that changes continuously as the aircraft moves. This approach eventually led to the Northrop B-2 Spirit stealth bomber. In this case the aerodynamic advantages of the flying wing are not the primary needs. However, modern computer-controlled fly-by-wire systems allowed for many of the aerodynamic drawbacks of the flying wing to be minimized, making for an efficient and stable long-range bomber. Blended wing body Blended wing body aircraft have a flattened and airfoil shaped body, which produces most of the lift to keep itself aloft, and distinct and separate wing structures, though the wings are smoothly blended in with the body.Thus blended wing bodied aircraft incorporate design features from both a futuristic fuselage and flying wing design. The purported advantages of the blended wing body approach are efficient high-lift wings and a wide airfoil-shaped body. This enables the entire craft to contribute to lift generation with the result of potentially increased fuel economy. Lifting body A lifting body is a configuration in which the body itself produces lift. In contrast to a flying wing, which is a wing with minimal or no conventional fuselage, a lifting body can be thought of as a fuselage with little or no conventional wing. Whereas a flying wing seeks to maximize cruise efficiency at subsonic speeds by eliminating non-lifting surfaces, lifting bodies generally minimize the drag and structure of a wing for subsonic, supersonic, and hypersonic flight, or, spacecraft re-entry. All of these flight regimes pose challenges for proper flight stability.Lifting bodies were a major area of research in the 1960s and 70s as a means to build a small and lightweight manned spacecraft. The US built a number of famous lifting body rocket planes to test the concept, as well as several rocket-launched re-entry vehicles that were tested over the Pacific. Interest waned as the US Air Force lost interest in the manned mission, and major development ended during the Space Shuttle design process when it became clear that the highly shaped fuselages made it difficult to fit fuel tankage. Empennage and foreplane The classic airfoil section wing is unstable in flight and difficult to control. Flexible-wing types often rely on an anchor line or the weight of a pilot hanging beneath to maintain the correct attitude. Some free-flying types use an adapted airfoil that is stable, or other ingenious mechanisms including, most recently, electronic artificial stability.But in order to achieve trim, stability and control, most fixed-wing types have an empennage comprising a fin and rudder which act horizontally and a tailplane and elevator which act vertically. This is so common that it is known as the conventional layout. Sometimes there may be two or more fins, spaced out along the tailplane.Some types have a horizontal "canard" foreplane ahead of the main wing, instead of behind it. This foreplane may contribute to the lift, the trim, or control of the aircraft, or to several of these. Controls and instruments Airplanes have complex flight control systems. The main controls allow the pilot to direct the aircraft in the air by controlling the attitude (roll, pitch and yaw) and engine thrust.On manned aircraft, cockpit instruments provide information to the pilots, including flight data, engine output, navigation, communications and other aircraft systems that may be installed. Safety When risk is measured by deaths per passenger kilometer, air travel is approximately 10 times safer than travel by bus or rail. However, when using the deaths per journey statistic, air travel is significantly more dangerous than car, rail, or bus travel. Air travel insurance is relatively expensive for this reason- insurers generally use the deaths per journey statistic. There is a significant difference between the safety of airliners and that of smaller private planes, with the per-mile statistic indicating that airliners are 8.3 times safer than smaller planes. See also Abbas Ibn FirnasAircraft flight mechanicsAirlinerAviationAviation and the environmentAviation historyFuel efficiencyList of altitude records reached by different aircraft typesManeuvering speedRotorcraftWright Brothers References  Bibliography Blatner, David. The Flying Book: Everything You've Ever Wondered About Flying On Airplanes. ISBN 0-8027-7691-4 External links The Aeroplane centreAirliners.netAerospaceweb.orgHow Airplanes Work – Howstuffworks.com
Animation is the process of making the illusion of motion and the illusion of change by means of the rapid display of a sequence of images that minimally differ from each other. The illusion—as in motion pictures in general—is thought to rely on the phi phenomenon. Animators are artists who specialize in the creation of animation. Animation can be recorded with either analogue media, a flip book, motion picture film, video tape, digital media, including formats with animated GIF, Flash animation, and digital video. To display animation, a digital camera, computer, or projector are used along with new technologies that are produced.Animation creation methods include the traditional animation creation method and those involving stop motion animation of two and three-dimensional objects, paper cutouts, puppets and clay figures. Images are displayed in a rapid succession, usually 24, 25, 30, or 60 frames per second. Computer animation processes generating animated images with the general term computer-generated imagery (CGI). 3D animation uses computer graphics, while 2D animation is used for stylistic, low bandwidth and faster real-time renderings. History Early examples of attempts to capture the phenomenon of motion into a still drawing can be found in paleolithic cave paintings, where animals are often depicted with multiple legs in superimposed positions, clearly attempting to convey the perception of motion.An earthen goblet discovered at the site of the 5,200-year-old Shahr-e Sūkhté (Burnt City) in southeastern Iran, depicts what could possibly be the world's oldest example of animation. The artifact bears five sequential images depicting a Persian Desert Ibex jumping up to eat the leaves of a tree.Ancient Chinese records contain several mentions of devices that were said to "give an impression of movement" to human or animal figures, these accounts are unclear and may only refer to the actual movement of the figures through space. They may, of course, refer to Chinese shadow puppets.In the 19th century, the phenakistoscope (1832), zoetrope (1834) and praxinoscope (1877) were introduced. A thaumatrope (1824) is a simple toy with a small disk with different pictures on each side; a bird in a cage and is attached to two pieces of strings. The phenakistoscope was invented simultaneously by Belgian Joseph Plateau and Austrian Simon von Stampfer in 1831. The phenakistoscope consists of a disk with a series of images, drawn on radi evenly space around the center of the disk.John Barnes Linnett patented the first flip book in 1868 as the kineograph. The common flip book were early animation devices that produced an illusion of movement from a series of sequential drawings, animation did not develop further until the advent of motion picture film and cinematography in the 1890s.The cinématographe was a projector, printer, and camera in one machine that allowed moving pictures to be shown successfully on a screen which was invented by history's earliest filmmakers, Auguste and Louis Lumière, in 1894. The first animated projection (screening) was created in France, by Charles-Émile Reynaud, who was a French science teacher. Reynaud created the Praxinoscope in 1877 and the Théâtre Optique in December 1888. On 28 October 1892, he projected the first animation in public, Pauvre Pierrot, at the Musée Grévin in Paris. This film is also notable as the first known instance of film perforations being used. His films were not photographed, they were drawn directly onto the transparent strip. In 1900, more than 500,000 people had attended these screenings.The first film that was recorded on standard picture film and included animated sequences was the 1900 Enchanted Drawing, which was followed by the first entirely animated film - the 1906 Humorous Phases of Funny Faces by J. Stuart Blackton, who, because of that, is considered the father of American animation.In Europe, the French artist, Émile Cohl, created the first animated film using what came to be known as traditional animation creation methods - the 1908 Fantasmagorie. The film largely consisted of a stick figure moving about and encountering all manner of morphing objects, a wine bottle that transforms into a flower. There were also sections of live action in which the animator's hands would enter the scene. The film was created by drawing each frame on paper and then shooting each frame onto negative film, which gave the picture a blackboard look.The author of the first puppet-animated film (The Beautiful Lukanida (1912)) was the Russian-born (ethnically Polish) director Wladyslaw Starewicz, known as Ladislas Starevich.More detailed hand-drawn animation, requiring a team of animators drawing each frame manually with detailed backgrounds and characters, were those directed by Winsor McCay, a successful newspaper cartoonist, including the 1911 Little Nemo, the 1914 Gertie the Dinosaur, and the 1918 The Sinking of the Lusitania. Gertie the Dinosaur was an early example of the character development in drawn animation.During the 1910s, the production of animated short films typically referred to as "cartoons", became an industry of its own and cartoon shorts were produced for showing in movie theaters. The most successful producer at the time was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process which dominated the animation industry for the rest of the decade.El Apóstol (Spanish: "The Apostle") was a 1917 Argentine animated film utilizing cutout animation, and the world's first animated feature film. Unfortunately, a fire that destroyed producer Federico Valle's film studio incinerated the only known copy of El Apóstol, and it is now considered a lost film.In 1958, Hanna-Barbera released Huckleberry Hound, the first half hour television program to feature only in animation. Terrytoons released Tom Terrific that same year. Television significantly decreased public attention to the animated shorts being shown in theaters.Computer animation has become popular since Toy Story (1995), the first feature-length animated film completely made using this technique.In 2008, the animation market was worth US$68.4 billion. Animation as an art and industry continues to thrive as of the mid-2010s because well-made animated projects can find audiences across borders and in all four quadrants. Animated feature-length films returned the highest gross margins (around 52%) of all film genres in the 2004–2013 timeframe. Techniques  Traditional animation Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century. The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper. To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings. The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.The traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system. Various software programs are used to color the drawings and simulate camera movement and effects. The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media with digital video. The "look" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years. Some animation producers have used the term "tradigital" (a play on the words "traditional" and "digital") to describe cel animation which makes extensive use of computer technologies.Examples of traditionally animated feature films include Pinocchio (United States, 1940), Animal Farm (United Kingdom, 1954), and The Illusionist (British-French, 2010). Traditionally animated films which were produced with the aid of computer technology include The Lion King (US, 1994), The Prince of Egypt (US, 1998), Akira (Japan, 1988), Spirited Away (Japan, 2001), The Triplets of Belleville (France, 2003), and The Secret of Kells (Irish-French-Belgian, 2009).Full animation refers to the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement, having a smooth animation. Fully animated films can be made in a variety of styles, from more realistically animated works those produced by the Walt Disney studio (The Little Mermaid, Beauty and the Beast, Aladdin, The Lion King) to the more 'cartoon' styles of the Warner Bros. animation studio. Many of the Disney animated features are examples of full animation, as are non-Disney works, The Secret of NIMH (US, 1982), The Iron Giant (US, 1999), and Nocturna (Spain, 2007). Fully animated films are animated at 24 frames per second, with a combination of animation on ones and twos, meaning that drawings can be held for one frame out of 24 or two frames out of 24.Limited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or "skippy" movement animation. Limited animation uses fewer drawings per second, thereby limiting the fluidity of the animation. This is a more economic technique. Pioneered by the artists at the American studio United Productions of America, limited animation can be used as a method of stylized artistic expression, as in Gerald McBoing-Boing (US, 1951), Yellow Submarine (UK, 1968), and certain anime produced in Japan. Its primary use, however, has been in producing cost-effective animated content for media for television (the work of Hanna-Barbera, Filmation, and other TV animation studios) and later the Internet (web cartoons).Rotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame. The source film can be directly copied from actors' outlines into animated drawings, as in The Lord of the Rings (US, 1978), or used in a stylized and expressive manner, as in Waking Life (US, 2001) and A Scanner Darkly (US, 2006). Some other examples are Fire and Ice (US, 1983), Heavy Metal (1981), and Aku no Hana (2013).Live-action/animation is a technique combining hand-drawn characters into live action shots or live action actors into animated shots. One of the earlier uses was in Koko the Clown when Koko was drawn over live action footage. Other examples include Who Framed Roger Rabbit (US, 1988), Space Jam (US, 1996) and Osmosis Jones (US, 2001). Stop motion animation Stop-motion animation is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement. There are many different types of stop-motion animation, usually named after the medium used to create the animation. Computer software is widely available to create this type of animation; however, traditional stop motion animation is usually less expensive and time-consuming to produce than current computer animation.Puppet animation typically involves stop-motion puppet figures interacting in a constructed environment, in contrast to real-world interaction in model animation. The puppets generally have an armature inside of them to keep them still and steady to constrain their motion to particular joints. Examples include The Tale of the Fox (France, 1937), The Nightmare Before Christmas (US, 1993), Corpse Bride (US, 2005), Coraline (US, 2009), the films of Jiří Trnka and the adult animated sketch-comedy television series Robot Chicken (US, 2005–present).Puppetoon, created using techniques developed by George Pal, are puppet-animated films which typically use a different version of a puppet for different frames, rather than simply manipulating one existing puppet.Clay animation, or Plasticine animation (often called claymation, which, however, is a trademarked name), uses figures made of clay or a similar malleable material to create stop-motion animation. The figures may have an armature or wire frame inside, similar to the related puppet animation (below), that can be manipulated to pose the figures. Alternatively, the figures may be made entirely of clay, in the films of Bruce Bickford, where clay creatures morph into a variety of different shapes. Examples of clay-animated works include The Gumby Show (US, 1957–1967) Morph shorts (UK, 1977–2000), Wallace and Gromit shorts (UK, as of 1989), Jan Švankmajer's Dimensions of Dialogue (Czechoslovakia, 1982), The Trap Door (UK, 1984). Films include Wallace & Gromit: The Curse of the Were-Rabbit, Chicken Run and The Adventures of Mark Twain.Strata-cut animation, Strata-cut animation is most commonly a form of clay animation in which a long bread-like "loaf" of clay, internally packed tight and loaded with varying imagery, is sliced into thin sheets, with the animation camera taking a frame of the end of the loaf for each cut, eventually revealing the movement of the internal images within.Cutout animation is a type of stop-motion animation produced by moving two-dimensional pieces of material paper or cloth. Examples include Terry Gilliam's animated sequences from Monty Python's Flying Circus (UK, 1969–1974); Fantastic Planet (France/Czechoslovakia, 1973) ; Tale of Tales (Russia, 1979), The pilot episode of the adult television sitcom series (and sometimes in episodes) of South Park (US, 1997) and the music video Live for the moment, from Verona Riots band (produced by Alberto Serrano and Nívola Uyá, Spain 2014).Silhouette animation is a variant of cutout animation in which the characters are backlit and only visible as silhouettes. Examples include The Adventures of Prince Achmed (Weimar Republic, 1926) and Princes et princesses (France, 2000).Model animation refers to stop-motion animation created to interact with and exist as a part of a live-action world. Intercutting, matte effects and split screens are often employed to blend stop-motion characters or objects with live actors and settings. Examples include the work of Ray Harryhausen, as seen in films, Jason and the Argonauts (1963), and the work of Willis H. O'Brien on films, King Kong (1933).Go motion is a variant of model animation that uses various techniques to create motion blur between frames of film, which is not present in traditional stop-motion. The technique was invented by Industrial Light & Magic and Phil Tippett to create special effect scenes for the film The Empire Strikes Back (1980). Another example is the dragon named "Vermithrax" from Dragonslayer (1981 film).Object animation refers to the use of regular inanimate objects in stop-motion animation, as opposed to specially created items.Graphic animation uses non-drawn flat visual graphic material (photographs, newspaper clippings, magazines, etc.), which are sometimes manipulated frame-by-frame to create movement. At other times, the graphics remain stationary, while the stop-motion camera is moved to create on-screen action.Brickfilm are a subgenre of object animation involving using Lego or other similar brick toys to make an animation. These have had a recent boost in popularity with the advent of video sharing sites, YouTube and the availability of cheap cameras and animation software.Pixilation involves the use of live humans as stop motion characters. This allows for a number of surreal effects, including disappearances and reappearances, allowing people to appear to slide across the ground, and other effects. Examples of pixilation include The Secret Adventures of Tom Thumb and Angry Kid shorts. Computer animation Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer. 2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact. 3D animation can create images that seem real to the viewer. 2D animation 2D animation figures are created or edited on the computer using 2D bitmap graphics or created and edited using 2D vector graphics. This includes automated computerized versions of traditional animation techniques, interpolated morphing, onion skinning and interpolated rotoscoping.2D animation has many applications, including analog computer animation, Flash animation, and PowerPoint animation. Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.Final line advection animation is a technique used in 2D animation, to give artists and animators more influence and control over the final product as everything is done within the same department. Speaking about using this approach in Paperman, John Kahrs said that "Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm." 3D animation 3D animation is digitally modeled and manipulated by an animator. The animator usually starts by creating a 3D polygon mesh to manipulate. A mesh typically includes many vertices that are connected by edges and faces, which give the visual appearance of form to a 3D object or 3D environment. Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices. This process is called rigging and can be used in conjunction with keyframes to create movement.Other techniques can be applied, mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects, fire and water simulations. These techniques fall under the category of 3D dynamics. 3D terms Cel-shaded animation is used to mimic traditional animation using computer software. Shading looks stark, with less blending of colors. Examples include Skyland (2007, France), The Iron Giant (1999, United States), Futurama (Fox, 1999) Appleseed Ex Machina (2007, Japan), The Legend of Zelda: The Wind Waker (2002, Japan)Machinima – Films created by screen capturing in video games and virtual worlds.Motion capture is used when live-action actors wear special suits that allow computers to copy their movements into CG characters. Examples include Polar Express (2004, US), Beowulf (2007, US), A Christmas Carol (2009, US), The Adventures of Tintin (film) (2011, US) kochadiiyan (2014, India).Photo-realistic animation is used primarily for animation that attempts to resemble real life, using advanced rendering that mimics in detail skin, plants, water, fire, clouds, etc. Examples include Up (2009, US), How to Train Your Dragon (2010, US), Ice Age (2002, US). Mechanical animation Animatronics is the use of mechatronics to create machines which seem animate rather than robotic.Audio-Animatronics and Autonomatronics is a form of robotics animation, combined with 3-D animation, created by Walt Disney Imagineering for shows and attractions at Disney theme parks move and make noise (generally a recorded speech or song). They are fixed to whatever supports them. They can sit and stand, and they cannot walk. An Audio-Animatron is different from an android-type robot in that it uses prerecorded movements and sounds, rather than responding to external stimuli. In 2009, Disney created an interactive version of the technology called Autonomatronics.Linear Animation Generator is a form of animation by using static picture frames installed in a tunnel or a shaft. The animation illusion is created by putting the viewer in a linear motion, parallel to the installed picture frames. The concept and the technical solution were invented in 2007 by Mihai Girlovan in Romania.Chuckimation is a type of animation created by the makers of the television series Action League Now! in which characters/props are thrown, or chucked from off camera or wiggled around to simulate talking by unseen hands.Puppetry is a form of theatre or performance animation that involves the manipulation of puppets. It is very ancient and is believed to have originated 3000 years BC. Puppetry takes many forms, they all share the process of animating inanimate performing objects. Puppetry is used in almost all human societies both as entertainment – in performance – and ceremonially in rituals, celebrations, and carnivals. Most puppetry involves storytelling.Zoetrope is a device that produces the illusion of motion from a rapid succession of static pictures. The term zoetrope is from the Greek words ζωή (zoē), meaning "alive, active", and τρόπος (tropos), meaning "turn", with "zoetrope" taken to mean "active turn" or "wheel of life". Other animation styles, techniques, and approaches Hydrotechnics: a technique that includes lights, water, fire, fog, and lasers, with high-definition projections on mist screens.Drawn on film animation: a technique where footage is produced by creating the images directly on film stock, for example by Norman McLaren, Len Lye and Stan Brakhage.Paint-on-glass animation: a technique for making animated films by manipulating slow drying oil paints on sheets of glass, for example by Aleksandr Petrov.Erasure animation: a technique using traditional 2D media, photographed over time as the artist manipulates the image. For example, William Kentridge is famous for his charcoal erasure films, and Piotr Dumała for his auteur technique of animating scratches on plaster.Pinscreen animation: makes use of a screen filled with movable pins that can be moved in or out by pressing an object onto the screen. The screen is lit from the side so that the pins cast shadows. The technique has been used to create animated films with a range of textural effects difficult to achieve with traditional cel animation.Sand animation: sand is moved around on a back- or front-lighted piece of glass to create each frame for an animated film. This creates an interesting effect when animated because of the light contrast.Flip book: a flip book (sometimes, especially in British English, called a flick book) is a book with a series of pictures that vary gradually from one page to the next, so that when the pages are turned rapidly, the pictures appear to animate by simulating motion or some other change. Flip books are often illustrated books for children, they also be geared towards adults and employ a series of photographs rather than drawings. Flip books are not always separate books, they appear as an added feature in ordinary books or magazines, often in the page corners. Software packages and websites are also available that convert digital video files into custom-made flip books.Character animationMulti-sketchingSpecial effects animation Production The creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, with certain unique aspects. One thing live-action and animated feature-length films do have in common is that they are both extremely labor-intensive and have high production costs.The most important difference is that once a film is in the production phase, the marginal cost of one more shot is higher for animated films than live-action films. It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation). It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene if that scene fails to effectively advance the plot of the film. Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes will make sense as a whole. While live-action films are now also storyboarded, they enjoy more latitude to depart from storyboards (i.e., real-time improvisation).Another problem unique to animation is the necessity of ensuring that the style of an animated film is consistent from start to finish, even as films have grown longer and teams have grown larger. Animators, like all artists, necessarily have their own individual styles, but must subordinate their individuality in a consistent way to whatever style was selected for a particular film. Since the early 1980s, feature-length animated films have been created by teams of about 500 to 600 people, of whom 50 to 70 are animators. It is relatively easy for two or three artists to match each other's styles; it is harder to keep dozens of artists synchronized with one another.This problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before animation begins. Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles. On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.Unlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film. In the early 1960s, animation studios began hiring professional screenwriters to write screenplays (while also continuing to use story departments) and screenplays had become commonplace for animated films by the late 1980s. Criticism Criticism of animation has been common in media and cinema since its inception. With its popularity, a large amount of criticism has arisen, especially animated feature-length films. Many concerns of cultural representation, psychological effects on children have been brought up around the animation industry, which has remained rather politically unchanged and stagnant since its inception into mainstream culture.Certain under-representation of women has been criticized in animation films and the industry. Awards As with any other form of media, animation too has instituted awards for excellence in the field. The original awards for animation were presented by the Academy of Motion Picture Arts and Sciences for animated shorts from the year 1932, during the 5th Academy Awards function. The first winner of the Academy Award was the short Flowers and Trees, a production by Walt Disney Productions. The Academy Award for a feature-length animated motion picture was only instituted for the year 2001, and awarded during the 74th Academy Awards in 2002. It was won by the film Shrek, produced by DreamWorks and Pacific Data Images. Disney/Pixar have produced the most films either to win or be nominated for the award. The list of both awards can be obtained here:Academy Award for Best Animated FeatureAcademy Award for Best Animated Short FilmSeveral other countries have instituted an award for best animated feature film as part of their national film awards: Africa Movie Academy Award for Best Animation (since 2008), BAFTA Award for Best Animated Film (since 2006), César Award for Best Animated Film (since 2011), Golden Rooster Award for Best Animation (since 1981), Goya Award for Best Animated Film (since 1989), Japan Academy Prize for Animation of the Year (since 2007), National Film Award for Best Animated Film (since 2006). Also since 2007, the Asia Pacific Screen Award for Best Animated Feature Film has been awarded at the Asia Pacific Screen Awards. Since 2009, the European Film Awards have awarded the European Film Award for Best Animated Film.The Annie Award is another award presented for excellence in the field of animation. Unlike the Academy Awards, the Annie Awards are only received for achievements in the field of animation and not for any other field of technical and artistic endeavor. They were re-organized in 1992 to create a new field for Best Animated feature. The 1990s winners were dominated by Walt Disney, however, newer studios, led by Pixar & DreamWorks, have now begun to consistently vie for this award. The list of awardees is as follows:Annie Award for Best Animated FeatureAnnie Award for Best Animated Short SubjectAnnie Award for Best Animated Television Production See also 12 basic principles of animationAnimated war filmAnimation departmentAnimation softwareArchitectural animationAvar (animation variable)Computer-generated imageryIndependent animationInternational Tournée of AnimationList of motion picture topicsModel sheetMotion graphic designSociety for Animation StudiesTradigital artWire-frame model Notes  References  Citations  Bibliography  Online  External links Animation at DMOZThe making of an 8-minute cartoon short"Animando", a 12-minute film demonstrating 10 different animation techniques (and teaching how to use them).
A ship is a large watercraft that travels the world's oceans and other sufficiently deep waterways, carrying passengers or goods, or in support of specialized missions, such as defense, research and fishing. Historically, a "ship" was a sailing vessel with at least three square-rigged masts and a full bowsprit. Ships are generally distinguished from boats, based on size, shape and load capacity.Ships have been important contributors to human migration and commerce. They have supported the spread of colonization and the slave trade, but have also served scientific, cultural, and humanitarian needs. After the 16th century, new crops that had come from and to the Americas via the European seafarers significantly contributed to the world population growth. Ship transport is responsible for the largest portion of world commerce.As of 2016, there were more than 49,000 merchant ships, totaling almost 1.8 billion dead weight tons. Of these 28% were oil tankers, 43% were bulk carriers, and 13% were container ships. Military forces operate vessels for naval warfare and to transport and support forces ashore. As of 2016, among the world's 104 navies, Korean People's Navy of North Korea had the most surface vessels (967), followed by People's Liberation Army Navy of China (714), the United States Navy (415), Islamic Republic of Iran Navy (398), and Russian Navy (352). The top 50 navies had a median fleet of 88 surface vessels each, according to various sources. Nomenclature There is no universal definition of what distinguishes a ship from a boat. Ships can usually be distinguished from boats based on size and the ship's ability to operate independently for extended periods. A legal definition of ship from Indian case law is a vessel that carries goods by sea. A common notion is that a ship can carry a boat, but not vice versa. A US Navy rule of thumb is that ships heel towards the outside of a sharp turn, whereas boats heel towards the inside because of the relative location of the center of mass versus the center of buoyancy. American and British 19th Century maritime law distinguished "vessels" from other craft; ships and boats fall in one legal category, whereas open boats and rafts are not considered vessels.In the Age of Sail, a full-rigged ship was a sailing vessel with at least three square-rigged masts and a full bowsprit; other types of vessel were also defined by their sailplan, e.g. barque, brigantine, etc.A number of large vessels are usually referred to as boats. Submarines are a prime example. Other types of large vessel which are traditionally called boats are Great Lakes freighters, riverboats, and ferryboats. Though large enough to carry their own boats and heavy cargoes, these vessels are designed for operation on inland or protected coastal waters.In most maritime traditions ships have individual names, and modern ships may belong to a ship class often named after its first ship. In English, a ship is traditionally referred to as "she", even if named after a man, but this is not universal usage; some journalistic style guides advise using "it" as referring to ships with female pronouns can be seen as offensive and outdated. History  Prehistory and antiquity The first known vessels date back about 10,000 years ago, but could not be described as ships. The first navigators began to use animal skins or woven fabrics as sails. Affixed to the top of a pole set upright in a boat, these sails gave early ships range. This allowed men to explore widely, allowing for the settlement of Oceania for example (about 3,000 years ago).By around 3000 BC, Ancient Egyptians knew how to assemble wooden planks into a hull. They used woven straps to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. The Greek historian and geographer Agatharchides had documented ship-faring among the early Egyptians: "During the prosperous period of the Old Kingdom, between the 30th and 25th centuries B. C., the river-routes were kept in order, and Egyptian ships sailed the Red Sea as far as the myrrh-country." Sneferu's ancient cedar wood ship Praise of the Two Lands is the first reference recorded (2613 BC) to a ship being referred to by name.The ancient Egyptians were perfectly at ease building sailboats. A remarkable example of their shipbuilding skills was the Khufu ship, a vessel 143 feet (44 m) in length entombed at the foot of the Great Pyramid of Giza around 2500 BC and found intact in 1954.It is known that ancient Nubia/Axum traded with India, and there is evidence that ships from Northeast Africa may have sailed back and forth between India/Sri Lanka and Nubia trading goods and even to Persia, Himyar and Rome. Aksum was known by the Greeks for having seaports for ships from Greece and Yemen.Elsewhere in Northeast Africa, the Periplus of the Red Sea reports that Somalis, through their northern ports such as Zeila and Berbera, were trading frankincense and other items with the inhabitants of the Arabian Peninsula well before the arrival of Islam as well as with then Roman-controlled Egypt.A panel found at Mohenjodaro depicted a sailing craft. Vessels were of many types; their construction is vividly described in the Yukti Kalpa Taru, an ancient Indian text on shipbuilding. This treatise gives a technical exposition on the techniques of shipbuilding. It sets forth minute details about the various types of ships, their sizes, and the materials from which they were built. The Yukti Kalpa Taru sums up in a condensed form all the available information. The Yukti Kalpa Taru gives sufficient information and dates to prove that, in ancient times, Indian shipbuilders had a good knowledge of the materials which were used in building ships. In addition to describing the qualities of the different types of wood and their suitability for shipbuilding, the Yukti Kalpa Taru gives an elaborate classification of ships based on their size.The oldest discovered sea faring hulled boat is the Late Bronze Age Uluburun shipwreck off the coast of Turkey, dating back to 1300 BC.The Phoenicians, the first to sail completely around Africa, and Greeks gradually mastered navigation at sea aboard triremes, exploring and colonizing the Mediterranean via ship. Around 340 BC, the Greek navigator Pytheas of Massalia ventured from Greece to Western Europe and Great Britain. In the course of the 2nd century BC, Rome went on to destroy Carthage and subdue the Hellenistic kingdoms of the eastern Mediterranean, achieving complete mastery of the inland sea, that they called Mare Nostrum. The monsoon wind system of the Indian Ocean was first sailed by Greek navigator Eudoxus of Cyzicus in 118 BC.In China, by the time of the Zhou Dynasty ship technologies such as stern mounted rudders were developed, and by the Han Dynasty, a well kept naval fleet was an integral part of the military. Ship technology advanced to the point where by the medieval period, water tight compartments were developed.The Swahili people had various extensive trading ports dotting the coast of medieval East Africa and Great Zimbabwe had extensive trading contacts with Central Africa, and likely also imported goods brought to Africa through the Southeast African shore trade of Kilwa in modern-day Tanzania.It is known by historians that at its height the Mali Empire built a large naval fleet under Emperor Mansa Musa in the late 13th and early 14th century. Arabic sources describe what some consider to be visits to the New World by a Mali fleet in 1311.Before the introduction of the compass, celestial navigation was the main method for navigation at sea. In China, early versions of the magnetic compass were being developed and used in navigation between 1040 and 1117. The true mariner's compass, using a pivoting needle in a dry box, was developed in Europe no later than 1300. Renaissance Until the Renaissance, navigational technology remained comparatively primitive. This absence of technology did not prevent some civilizations from becoming sea powers. Examples include the maritime republics of Genoa and Venice, Hanseatic League, and the Byzantine navy. The Vikings used their knarrs to explore North America[citation needed][dubious], trade in the Baltic Sea and plunder many of the coastal regions of Western Europe.Towards the end of the 14th century, ships like the carrack began to develop towers on the bow and stern. These towers decreased the vessel's stability, and in the 15th century, the caravel, designed by the Portuguese, based on the Arabic qarib which could sail closer to the wind, became more widely used. The towers were gradually replaced by the forecastle and sterncastle, as in the carrack Santa María of Christopher Columbus. This increased freeboard allowed another innovation: the freeing port, and the artillery associated with it.In the 16th century, the use of freeboard and freeing ports became widespread on galleons.At this time, ships were developing in Asia in much the same way as Europe. Japan used defensive naval techniques in the Mongol invasions of Japan in 1281. It is likely that the Mongols of the time took advantage of both European and Asian shipbuilding techniques. During the 15th century, China's Ming Dynasty assembled one of the largest and most powerful naval fleets in the world for the diplomatic and power projection voyages of Zheng He. Elsewhere in Japan in the 15th century, one of the world's first iron-clads, "Tekkōsen" (鉄甲船), literally meaning "iron ships", was also developed. In Japan, during the Sengoku era from the fifteenth to 17th century, the great struggle for feudal supremacy was fought, in part, by coastal fleets of several hundred boats, including the atakebune. In Korea, in the early 15th century during the Joseon era, "Geobukseon"(거북선), was developed. The "turtle ship", as it was called is recognized as the first armored ship in the world.During the Age of the Ajuran, the Somali sultanates and republics of Merca, Mogadishu, Barawa, Hobyo and their respective ports flourished, enjoying a lucrative foreign commerce with ships sailing to and coming from Arabia, India, Venetia, Persia, Egypt, Portugal and as far away as China. In the 16th century, Duarte Barbosa noted that many ships from the Kingdom of Cambaya in what is modern-day India sailed to Mogadishu with cloth and spices, for which they in return received gold, wax and ivory. Barbosa also highlighted the abundance of meat, wheat, barley, horses, and fruit on the coastal markets, which generated enormous wealth for the merchants.Middle Age Swahili Kingdoms are known to have had trade port bullship and trade routes with the Islamic world and Asia and were described by Greek historians as "metropolises". Famous African trade ports such as Mombasa, Zanzibar, and Kilwa were known to Chinese sailors such as Zheng He and medieval Islamic historians such as the Berber Islamic voyager Abu Abdullah ibn Battua. In the 14th century AD, King Abubakari I, the brother of King Mansa Musa of the Mali Empire, is thought to have had a great armada of ships sitting on the coast of West Africa. This is corroborated by ibn Battuta himself who recalls several hundred Malian ships off the coast. This has led to great speculation, with historical evidence, that it is possible that Malian sailors may have reached the coast of Pre-Columbian America under the rule of Abubakari II, nearly two hundred years before Christopher Columbus and that black traders may have been in the Americas before Columbus. Fifty years before Christopher Columbus, Chinese navigator Zheng He traveled the world at the head of what was for the time a huge armada. The largest of his ships had nine masts, were 130 metres (430 ft) long and had a beam of 55 metres (180 ft). His fleet carried 30,000 men aboard 70 vessels, with the goal of bringing glory to the Chinese emperor.At the same time Zheng He made his expedition, Portuguese explorer Gil Eanes sailed on a square-rigged caravel beyond Cape Bojador the end of what was then considered the known world opening the route to deep sea exploration, continental sea communication technology and the spherical earth principle.The carrack and then the caravel were developed in Portugal. After Columbus, European exploration rapidly accelerated, and many new trade routes were established. In 1498, by reaching India, Vasco da Gama proved that the access to the Indian Ocean from the Atlantic was possible. These explorations in the Atlantic and Indian Oceans were soon followed by France, England and the Netherlands, who explored the Portuguese and Spanish trade routes into the Pacific Ocean, reaching Australia in 1606 and New Zealand in 1642. In the 17th century Dutch and Spanish explorers such as Abel Tasman and Luís Vaz de Torres explored the coasts of Australia, while in the 18th century it was British explorer James Cook who mapped much of Polynesia. Specialization and modernization Parallel to the development of warships, ships in service of marine fishery and trade also developed in the period between antiquity and the Renaissance.Maritime trade was driven by the development of shipping companies with significant financial resources. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway up to and past the early days of the industrial revolution. Flat-bottomed and flexible scow boats also became widely used for transporting small cargoes. Mercantile trade went hand-in-hand with exploration, self-financed by the commercial benefits of exploration.During the first half of the 18th century, the French Navy began to develop a new type of vessel known as a ship of the line, featuring seventy-four guns. This type of ship became the backbone of all European fighting fleets. These ships were 56 metres (184 ft) long and their construction required 2,800 oak trees and 40 kilometres (25 mi) of rope; they carried a crew of about 800 sailors and soldiers.During the 19th century the Royal Navy enforced a ban on the slave trade, acted to suppress piracy, and continued to map the world. A clipper was a very fast sailing ship of the 19th century. The clipper routes fell into commercial disuse with the introduction of steam ships with better fuel efficiency, and the opening of the Suez and Panama Canals.Ship designs stayed fairly unchanged until the late 19th century. The industrial revolution, new mechanical methods of propulsion, and the ability to construct ships from metal triggered an explosion in ship design. Factors including the quest for more efficient ships, the end of long running and wasteful maritime conflicts, and the increased financial capacity of industrial powers created an avalanche of more specialized boats and ships. Ships built for entirely new functions, such as firefighting, rescue, and research, also began to appear.In light of this, classification of vessels by type or function can be difficult. Even using very broad functional classifications such as fishery, trade, military, and exploration fails to classify most of the old ships. This difficulty is increased by the fact that the terms such as sloop and frigate are used by old and new ships alike, and often the modern vessels sometimes have little in common with their predecessors. 21st century In 2007, the world's fleet included 34,882 commercial vessels with gross tonnage of more than 1,000 tons, totaling 1.04 billion tons. These ships carried 7.4 billion tons of cargo in 2006, a sum that grew by 8% over the previous year. In terms of tonnage, 39% of these ships are tankers, 26% are bulk carriers, 17% container ships and 15% were other types.In 2002, there were 1,240 warships operating in the world, not counting small vessels such as patrol boats. The United States accounted for 3 million tons worth of these vessels, Russia 1.35 million tons, the United Kingdom 504,660 tons and China 402,830 tons. The 20th century saw many naval engagements during the two world wars, the Cold War, and the rise to power of naval forces of the two blocs. The world's major powers have recently used their naval power in cases such as the United Kingdom in the Falkland Islands and the United States in Iraq.The size of the world's fishing fleet is more difficult to estimate. The largest of these are counted as commercial vessels, but the smallest are legion. Fishing vessels can be found in most seaside villages in the world. As of 2004, the United Nations Food and Agriculture Organization estimated 4 million fishing vessels were operating worldwide. The same study estimated that the world's 29 million fishermen caught 85,800,000 tonnes (84,400,000 long tons; 94,600,000 short tons) of fish and shellfish that year. Types of ships Because ships are constructed using the principles of naval architecture that require same structural components, their classification is based on their function such as suggested by Paulet and Presles, which requires modification of the components. The categories accepted in general by naval architects are:High-speed craft – Multihulls including wave piercers, small-waterplane-area twin hull (SWATH), surface effect ships and hovercraft, hydrofoil, wing in ground effect craft (WIG).Off shore oil vessels – Platform supply vessel, pipe layers, accommodation and crane barges, non and semi-submersible drilling rigs, production platforms, floating production storage and offloading units.Fishing vesselsMotorised fishing trawlers, trap setters, seiners, longliners, trollers & factory ships.Traditional sailing and rowed fishing vessels and boats used for handline fishingHarbour work craftCable layersTugboats, dredgers, salvage vessels, tenders, Pilot boats.Floating dry docks, floating cranes, lightership.Dry cargo ships – tramp freighters, bulk carriers, cargo liners, container vessels, barge carriers, Ro-Ro ships, refrigerated cargo ships, timber carriers, livestock & light vehicle carriers.Liquid cargo ships – Oil tankers, liquefied gas carriers, chemical carriers.Passenger vesselsLiners, cruise and Special Trade Passenger (STP) shipsCross-channel, coastal and harbour ferries.Luxury & cruising yachtsSail training and multi-masted shipsRecreational boats and craft – rowed, masted and motorised craftSpecial-purpose vessels – weather and research vessels, deep sea survey vessels, and icebreakers.Submersibles – industrial exploration, scientific research, tourist and hydrographic survey.WarshipsSurface combatant- deep and shallow draftSubmarinesSome of these are discussed in the following sections. Freshwater Freshwater shipping may occur on lakes, rivers and canals. Ships designed for those venues may be specially adapted to the widths and depths of specific waterways. Examples of freshwater waterways that are navigable in part by large vessels include the Danube, Mississippi, Rhine, Yangtze and Amazon Rivers, and the Great Lakes. Great Lakes Lake freighters, also called lakers, are cargo vessels that ply the Great Lakes. The most well-known is SS Edmund Fitzgerald, the latest major vessel to be wrecked on the Lakes. These vessels are traditionally called boats, not ships. Visiting ocean-going vessels are called "salties." Because of their additional beam, very large salties are never seen inland of the Saint Lawrence Seaway. Because the smallest of the Soo Locks is larger than any Seaway lock, salties that can pass through the Seaway may travel anywhere in the Great Lakes. Because of their deeper draft, salties may accept partial loads on the Great Lakes, "topping off" when they have exited the Seaway. Similarly, the largest lakers are confined to the Upper Lakes (Superior, Michigan, Huron, Erie) because they are too large to use the Seaway locks, beginning at the Welland Canal that bypasses the Niagara River.Since the freshwater lakes are less corrosive to ships than the salt water of the oceans, lakers tend to last much longer than ocean freighters. Lakers older than 50 years are not unusual, and as of 2005, all were over 20 years of age.SS St. Marys Challenger, built in 1906 as William P Snyder, was the oldest laker still working on the Lakes until its conversion into a barge starting in 2013. Similarly, E.M. Ford, built in 1898 as Presque Isle, was sailing the lakes 98 years later in 1996. As of 2007 E.M. Ford was still afloat as a stationary transfer vessel at a riverside cement silo in Saginaw, Michigan. Seagoing commercial vessels Commercial vessels or merchant ships can be divided into four broad categories: fishing, cargo ships, passenger ships, and special-purpose ships. The UNCTAD review of maritime transport categorizes ships as: oil tankers, bulk (and combination) carriers, general cargo ships, container ships, and "other ships", which includes "liquefied petroleum gas carriers, liquefied natural gas carriers, parcel (chemical) tankers, specialized tankers, reefers, offshore supply, tugs, dredgers, cruise, ferries, other non-cargo". General cargo ships include "multi-purpose and project vessels and roll-on/roll-off cargo".Modern commercial vessels are typically powered by a single propeller driven by a diesel or, less usually, gas turbine engine., but until the mid-19th century they were predominantly square sail rigged. The fastest vessels may use pump-jet engines. Most commercial vessels have full hull-forms to maximize cargo capacity. Hulls are usually made of steel, although aluminum can be used on faster craft, and fiberglass on the smallest service vessels. Commercial vessels generally have a crew headed by a captain, with deck officers and marine engineers on larger vessels. Special-purpose vessels often have specialized crew if necessary, for example scientists aboard research vessels.Fishing boats are generally small, often little more than 30 meters (98 ft) but up to 100 metres (330 ft) for a large tuna or whaling ship. Aboard a fish processing vessel, the catch can be made ready for market and sold more quickly once the ship makes port. Special purpose vessels have special gear. For example, trawlers have winches and arms, stern-trawlers have a rear ramp, and tuna seiners have skiffs. In 2004, 85,800,000 tonnes (84,400,000 long tons; 94,600,000 short tons) of fish were caught in the marine capture fishery. Anchoveta represented the largest single catch at 10,700,000 tonnes (10,500,000 long tons; 11,800,000 short tons). That year, the top ten marine capture species also included Alaska pollock, Blue whiting, Skipjack tuna, Atlantic herring, Chub mackerel, Japanese anchovy, Chilean jack mackerel, Largehead hairtail, and Yellowfin tuna. Other species including salmon, shrimp, lobster, clams, squid and crab, are also commercially fished. Modern commercial fishermen use many methods. One is fishing by nets, such as purse seine, beach seine, lift nets, gillnets, or entangling nets. Another is trawling, including bottom trawl. Hooks and lines are used in methods like long-line fishing and hand-line fishing. Another method is the use of fishing trap.Cargo ships transport dry and liquid cargo. Dry cargo can be transported in bulk by bulk carriers, packed directly onto a general cargo ship in break-bulk, packed in intermodal containers as aboard a container ship, or driven aboard as in roll-on roll-off ships. Liquid cargo is generally carried in bulk aboard tankers, such as oil tankers which may include both crude and finished products of oil, chemical tankers which may also carry vegetable oils other than chemicals and LPG/LNG tankers, although smaller shipments may be carried on container ships in tank containers.Passenger ships range in size from small river ferries to very large cruise ships. This type of vessel includes ferries, which move passengers and vehicles on short trips; ocean liners, which carry passengers from one place to another; and cruise ships, which carry passengers on voyages undertaken for pleasure, visiting several places and with leisure activities on board, often returning them to the port of embarkation. Riverboats and inland ferries are specially designed to carry passengers, cargo, or both in the challenging river environment. Rivers present special hazards to vessels. They usually have varying water flows that alternately lead to high speed water flows or protruding rock hazards. Changing siltation patterns may cause the sudden appearance of shoal waters, and often floating or sunken logs and trees (called snags) can endanger the hulls and propulsion of riverboats. Riverboats are generally of shallow draft, being broad of beam and rather square in plan, with a low freeboard and high topsides. Riverboats can survive with this type of configuration as they do not have to withstand the high winds or large waves that are seen on large lakes, seas, or oceans.Fishing vessels are a subset of commercial vessels, but generally small in size and often subject to different regulations and classification. They can be categorized by several criteria: architecture, the type of fish they catch, the fishing method used, geographical origin, and technical features such as rigging. As of 2004, the world's fishing fleet consisted of some 4 million vessels. Of these, 1.3 million were decked vessels with enclosed areas and the rest were open vessels. Most decked vessels were mechanized, but two-thirds of the open vessels were traditional craft propelled by sails and oars. More than 60% of all existing large fishing vessels were built in Japan, Peru, the Russian Federation, Spain or the United States of America. Special purpose vessels A weather ship was a ship stationed in the ocean as a platform for surface and upper air meteorological observations for use in marine weather forecasting. Surface weather observations were taken hourly, and four radiosonde releases occurred daily. It was also meant to aid in search and rescue operations and to support transatlantic flights. Proposed as early as 1927 by the aviation community, the establishment of weather ships proved to be so useful during World War II that the International Civil Aviation Organization (ICAO) established a global network of weather ships in 1948, with 13 to be supplied by the United States. This number was eventually negotiated down to nine.The weather ship crews were normally at sea for three weeks at a time, returning to port for 10-day stretches. Weather ship observations proved to be helpful in wind and wave studies, as they did not avoid weather systems like other ships tended to for safety reasons. They were also helpful in monitoring storms at sea, such as tropical cyclones. The removal of a weather ship became a negative factor in forecasts leading up to the Great Storm of 1987. Beginning in the 1970s, their role became largely superseded by weather buoys due to the ships' significant cost. The agreement of the use of weather ships by the international community ended in 1990. The last weather ship was Polarfront, known as weather station M ("Mike"), which was put out of operation on 1 January 2010. Weather observations from ships continue from a fleet of voluntary merchant vessels in routine commercial operation. Naval vessels Naval vessels are those used by a navy for military purposes. There have been many types of naval vessel. Modern naval vessels can be broken down into three categories: surface warships, submarines, and support and auxiliary vessels.Modern warships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines and amphibious assault ships. The distinction between cruisers, destroyers, frigates, and corvettes is not rigorous; the same vessel may be described differently in different navies. Battleships were used during the Second World War and occasionally since then (the last battleships were removed from the U.S. Naval Vessel Register in March 2006), but were made obsolete by the use of carrier-borne aircraft and guided missiles.Most military submarines are either attack submarines or ballistic missile submarines. Until the end of World War II the primary role of the diesel/electric submarine was anti-ship warfare, inserting and removing covert agents and military forces, and intelligence-gathering. With the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to effectively hunt each other. The development of submarine-launched nuclear and cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster munitions to nuclear weapons.Most navies also include many types of support and auxiliary vessel, such as minesweepers, patrol boats, offshore patrol vessels, replenishment ships, and hospital ships which are designated medical treatment facilities.Fast combat vessels such as cruisers and destroyers usually have fine hulls to maximize speed and maneuverability. They also usually have advanced marine electronics and communication systems, as well as weapons. Architecture Some components exist in vessels of any size and purpose. Every vessel has a hull of sorts. Every vessel has some sort of propulsion, whether it's a pole, an ox, or a nuclear reactor. Most vessels have some sort of steering system. Other characteristics are common, but not as universal, such as compartments, holds, a superstructure, and equipment such as anchors and winches. Hull For a ship to float, its weight must be less than that of the water displaced by the ship's hull. There are many types of hulls, from logs lashed together to form a raft to the advanced hulls of America's Cup sailboats. A vessel may have a single hull (called a monohull design), two in the case of catamarans, or three in the case of trimarans. Vessels with more than three hulls are rare, but some experiments have been conducted with designs such as pentamarans. Multiple hulls are generally parallel to each other and connected by rigid arms.Hulls have several elements. The bow is the foremost part of the hull. Many ships feature a bulbous bow. The keel is at the very bottom of the hull, extending the entire length of the ship. The rear part of the hull is known as the stern, and many hulls have a flat back known as a transom. Common hull appendages include propellers for propulsion, rudders for steering, and stabilizers to quell a ship's rolling motion. Other hull features can be related to the vessel's work, such as fishing gear and sonar domes.Hulls are subject to various hydrostatic and hydrodynamic constraints. The key hydrostatic constraint is that it must be able to support the entire weight of the boat, and maintain stability even with often unevenly distributed weight. Hydrodynamic constraints include the ability to withstand shock waves, weather collisions and groundings.Older ships and pleasure craft often have or had wooden hulls. Steel is used for most commercial vessels. Aluminium is frequently used for fast vessels, and composite materials are often found in sailboats and pleasure craft. Some ships have been made with concrete hulls. Propulsion systems Propulsion systems for ships fall into three categories: human propulsion, sailing, and mechanical propulsion. Human propulsion includes rowing, which was used even on large galleys. Propulsion by sail generally consists of a sail hoisted on an erect mast, supported by stays and spars and controlled by ropes. Sail systems were the dominant form of propulsion until the 19th century. They are now generally used for recreation and competition, although experimental sail systems, such as the turbosails, rotorsails, and wingsails have been used on larger modern vessels for fuel savings.Mechanical propulsion systems generally consist of a motor or engine turning a propeller, or less frequently, an impeller or wave propulsion fins. Steam engines were first used for this purpose, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships. Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels (see NS Savannah).In addition to traditional fixed and controllable pitch propellers there are many specialized variations, such as contra-rotating and nozzle-style propellers. Most vessels have a single propeller, but some large vessels may have up to four propellers supplemented with transverse thrusters for maneuvring at ports. The propeller is connected to the main engine via a propeller shaft and, in case of medium- and high-speed engines, a reduction gearbox. Some modern vessels have a diesel-electric powertrain in which the propeller is turned by an electric motor powered by the ship's generators. Steering systems For ships with independent propulsion systems for each side, such as manual oars or some paddles, steering systems may not be necessary. In most designs, such as boats propelled by engines or sails, a steering system becomes necessary. The most common is a rudder, a submerged plane located at the rear of the hull. Rudders are rotated to generate a lateral force which turns the boat. Rudders can be rotated by a tiller, manual wheels, or electro-hydraulic systems. Autopilot systems combine mechanical rudders with navigation systems. Ducted propellers are sometimes used for steering.Some propulsion systems are inherently steering systems. Examples include the outboard motor, the bow thruster, and the Z-drive. Holds, compartments, and the superstructure Larger boats and ships generally have multiple decks and compartments. Separate berthings and heads are found on sailboats over about 25 feet (7.6 m). Fishing boats and cargo ships typically have one or more cargo holds. Most larger vessels have an engine room, a galley, and various compartments for work. Tanks are used to store fuel, engine oil, and fresh water. Ballast tanks are equipped to change a ship's trim and modify its stability.Superstructures are found above the main deck. On sailboats, these are usually very low. On modern cargo ships, they are almost always located near the ship's stern. On passenger ships and warships, the superstructure generally extends far forward. Equipment Shipboard equipment varies from ship to ship depending on such factors as the ship's era, design, area of operation, and purpose. Some types of equipment that are widely found include:Masts can be the home of antennas, navigation lights, radar transponders, fog signals, and similar devices often required by law.Ground tackle includes equipment such as mooring winches, windlasses, and anchors. Anchors are used to moor ships in shallow water. They are connected to the ship by a rope or chain. On larger vessels, the chain runs through a hawsepipe.Cargo equipment such as cranes and cargo booms are used to load and unload cargo and ship's stores.Safety equipment such as lifeboats, liferafts, and survival suits are carried aboard many vessels for emergency use. Design considerations  Hydrostatics Ships float in the water at a level where mass of the displaced water equals the mass of the vessel, such that the downwards force of gravity equals the upward force of buoyancy. As a vessel is lowered into the water its weight remains constant but the corresponding weight of water displaced by its hull increases. If the vessel's mass is evenly distributed throughout, it floats evenly along its length and across its beam (width). A vessel's stability is considered in both this hydrostatic sense as well as a hydrodynamic sense, when subjected to movement, rolling and pitching, and the action of waves and wind. Stability problems can lead to excessive pitching and rolling, and eventually capsizing and sinking. Hydrodynamics The advance of a vessel through water is resisted by the water. This resistance can be broken down into several components, the main ones being the friction of the water on the hull and wave making resistance. To reduce resistance and therefore increase the speed for a given power, it is necessary to reduce the wetted surface and use submerged hull shapes that produce low amplitude waves. To do so, high-speed vessels are often more slender, with fewer or smaller appendages. The friction of the water is also reduced by regular maintenance of the hull to remove the sea creatures and algae that accumulate there. Antifouling paint is commonly used to assist in this. Advanced designs such as the bulbous bow assist in decreasing wave resistance.A simple way of considering wave-making resistance is to look at the hull in relation to its wake. At speeds lower than the wave propagation speed, the wave rapidly dissipates to the sides. As the hull approaches the wave propagation speed, however, the wake at the bow begins to build up faster than it can dissipate, and so it grows in amplitude. Since the water is not able to "get out of the way of the hull fast enough", the hull, in essence, has to climb over or push through the bow wave. This results in an exponential increase in resistance with increasing speed.This hull speed is found by the formula:                              knots                ≈        1.34        ×                              L                          ft                                            {\displaystyle {\mbox{knots}}\approx 1.34\times {\sqrt {L{\mbox{ft}}}}}  or, in metric units:                              knots                ≈        2.5        ×                              L                          m                                            {\displaystyle {\mbox{knots}}\approx 2.5\times {\sqrt {L{\mbox{m}}}}}  where L is the length of the waterline in feet or meters.When the vessel exceeds a speed/length ratio of 0.94, it starts to outrun most of its bow wave, and the hull actually settles slightly in the water as it is now only supported by two wave peaks. As the vessel exceeds a speed/length ratio of 1.34, the hull speed, the wavelength is now longer than the hull, and the stern is no longer supported by the wake, causing the stern to squat, and the bow rise. The hull is now starting to climb its own bow wave, and resistance begins to increase at a very high rate. While it is possible to drive a displacement hull faster than a speed/length ratio of 1.34, it is prohibitively expensive to do so. Most large vessels operate at speed/length ratios well below that level, at speed/length ratios of under 1.0.For large projects with adequate funding, hydrodynamic resistance can be tested experimentally in a hull testing pool or using tools of computational fluid dynamics.Vessels are also subject to ocean surface waves and sea swell as well as effects of wind and weather. These movements can be stressful for passengers and equipment, and must be controlled if possible. The rolling movement can be controlled, to an extent, by ballasting or by devices such as fin stabilizers. Pitching movement is more difficult to limit and can be dangerous if the bow submerges in the waves, a phenomenon called pounding. Sometimes, ships must change course or speed to stop violent rolling or pitching.How it has been convincingly shown in scientific studies of the 21st century, controllability of some vessels decreases dramatically in some cases that are conditioned by effects of the bifurcation memory. This class of vessels includes ships with high manoeuvring capabilities, aircraft and controlled underwater vehicles designed to be unstable in steady-state motion that are interesting in terms of applications. These features must be considered in designing ships and in their control in critical situations. Lifecycle A ship will pass through several stages during its career. The first is usually an initial contract to build the ship, the details of which can vary widely based on relationships between the shipowners, operators, designers and the shipyard. Then, the design phase carried out by a naval architect. Then the ship is constructed in a shipyard. After construction, the vessel is launched and goes into service. Ships end their careers in a number of ways, ranging from shipwrecks to service as a museum ship to the scrapyard. Design A vessel's design starts with a specification, which a naval architect uses to create a project outline, assess required dimensions, and create a basic layout of spaces and a rough displacement. After this initial rough draft, the architect can create an initial hull design, a general profile and an initial overview of the ship's propulsion. At this stage, the designer can iterate on the ship's design, adding detail and refining the design at each stage.The designer will typically produce an overall plan, a general specification describing the peculiarities of the vessel, and construction blueprints to be used at the building site. Designs for larger or more complex vessels may also include sail plans, electrical schematics, and plumbing and ventilation plans.As environmental laws are becoming more strict, ship designers need to create their design in such a way that the ship, when it nears its end-of-term, can be disassembled or disposed easily and that waste is reduced to a minimum. Construction Ship construction takes place in a shipyard, and can last from a few months for a unit produced in series, to several years to reconstruct a wooden boat like the frigate Hermione, to more than 10 years for an aircraft carrier. During World War II, the need for cargo ships was so urgent that construction time for Liberty Ships went from initially eight months or longer, down to weeks or even days. Builders employed production line and prefabrication techniques such as those used in shipyards today.Hull materials and vessel size play a large part in determining the method of construction. The hull of a mass-produced fiberglass sailboat is constructed from a mold, while the steel hull of a cargo ship is made from large sections welded together as they are built.Generally, construction starts with the hull, and on vessels over about 30 meters (98 ft), by the laying of the keel. This is done in a drydock or on land. Once the hull is assembled and painted, it is launched. The last stages, such as raising the superstructure and adding equipment and accommodation, can be done after the vessel is afloat.Once completed, the vessel is delivered to the customer. Ship launching is often a ceremony of some significance, and is usually when the vessel is formally named. A typical small rowboat can cost under US$100, $1,000 for a small speedboat, tens of thousands of dollars for a cruising sailboat, and about $2,000,000 for a Vendée Globe class sailboat. A 25 meters (82 ft) trawler may cost $2.5 million, and a 1,000-person-capacity high-speed passenger ferry can cost in the neighborhood of $50 million. A ship's cost partly depends on its complexity: a small, general cargo ship will cost $20 million, a Panamax-sized bulk carrier around $35 million, a supertanker around $105 million and a large LNG carrier nearly $200 million. The most expensive ships generally are so because of the cost of embedded electronics: a Seawolf-class submarine costs around $2 billion, and an aircraft carrier goes for about $3.5 billion. Repair and conversion Ships undergo nearly constant maintenance during their career, whether they be underway, pierside, or in some cases, in periods of reduced operating status between charters or shipping seasons.Most ships, however, require trips to special facilities such as a drydock at regular intervals. Tasks often done at drydock include removing biological growths on the hull, sandblasting and repainting the hull, and replacing sacrificial anodes used to protect submerged equipment from corrosion. Major repairs to the propulsion and steering systems as well as major electrical systems are also often performed at dry dock.Vessels that sustain major damage at sea may be repaired at a facility equipped for major repairs, such as a shipyard. Ships may also be converted for a new purpose: oil tankers are often converted into floating production storage and offloading units. End of service Most ocean-going cargo ships have a life expectancy of between 20 and 30 years. A sailboat made of plywood or fiberglass can last between 30 and 40 years. Solid wooden ships can last much longer but require regular maintenance. Carefully maintained steel-hulled yachts can have a lifespan of over 100 years.As ships age, forces such as corrosion, osmosis, and rotting compromise hull strength, and a vessel becomes too dangerous to sail. At this point, it can be scuttled at sea or scrapped by shipbreakers. Ships can also be used as museum ships, or expended to construct breakwaters or artificial reefs.Many ships do not make it to the scrapyard, and are lost in fires, collisions, grounding, or sinking at sea. The Allies lost some 5,150 ships during World War II. Measuring ships One can measure ships in terms of overall length, length of the ship at the waterline, beam (breadth), depth (distance between the crown of the weather deck and the top of the keelson), draft (distance between the highest waterline and the bottom of the ship) and tonnage. A number of different tonnage definitions exist and are used when describing merchant ships for the purpose of tolls, taxation, etc.In Britain until Samuel Plimsoll's Merchant Shipping Act of 1876, ship-owners could load their vessels until their decks were almost awash, resulting in a dangerously unstable condition. Anyone who signed on to such a ship for a voyage and, upon realizing the danger, chose to leave the ship, could end up in jail. Plimsoll, a Member of Parliament, realised the problem and engaged some engineers to derive a fairly simple formula to determine the position of a line on the side of any specific ship's hull which, when it reached the surface of the water during loading of cargo, meant the ship had reached its maximum safe loading level. To this day, that mark, called the "Plimsoll Line", exists on ships' sides, and consists of a circle with a horizontal line through the centre. On the Great Lakes of North America the circle is replaced with a diamond. Because different types of water (summer, fresh, tropical fresh, winter north Atlantic) have different densities, subsequent regulations required painting a group of lines forward of the Plimsoll mark to indicate the safe depth (or freeboard above the surface) to which a specific ship could load in water of various densities. Hence the "ladder" of lines seen forward of the Plimsoll mark to this day. This is called the "freeboard mark" or "load line mark" in the marine industry. Ship pollution Ship pollution is the pollution of air and water by shipping. It is a problem that has been accelerating as trade has become increasingly globalized, posing an increasing threat to the world’s oceans and waterways as globalization continues. It is expected that, “...shipping traffic to and from the United States is projected to double by 2020." Because of increased traffic in ocean ports, pollution from ships also directly affects coastal areas. The pollution produced affects biodiversity, climate, food, and human health. However, the degree to which humans are polluting and how it affects the world is highly debated and has been a hot international topic for the past 30 years. Oil spills Oil spills have devastating effects on the environment. Crude oil contains polycyclic aromatic hydrocarbons (PAHs) which are very difficult to clean up, and last for years in the sediment and marine environment. Marine species constantly exposed to PAHs can exhibit developmental problems, susceptibility to disease, and abnormal reproductive cycles.By the sheer amount of oil carried, modern oil tankers must be considered something of a threat to the environment. An oil tanker can carry 2 million barrels (318,000 m3) of crude oil, or 84,000,000 US gallons (69,940,000 imp gal; 318,000,000 L). This is more than six times the amount spilled in the widely known Exxon Valdez incident. In this spill, the ship ran aground and dumped 10,800,000 US gallons (8,993,000 imp gal; 40,880,000 L) of oil into the ocean in March 1989. Despite efforts of scientists, managers, and volunteers, over 400,000 seabirds, about 1,000 sea otters, and immense numbers of fish were killed.The International Tanker Owners Pollution Federation has researched 9,351 accidental spills since 1974. According to this study, most spills result from routine operations such as loading cargo, discharging cargo, and taking on fuel oil. 91% of the operational oil spills were small, resulting in less than 7 tons per spill. Spills resulting from accidents like collisions, groundings, hull failures, and explosions are much larger, with 84% of these involving losses of over 700 tons.Following the Exxon Valdez spill, the United States passed the Oil Pollution Act of 1990 (OPA-90), which included a stipulation that all tankers entering its waters be double-hulled by 2015. Following the sinkings of Erika (1999) and Prestige (2002), the European Union passed its own stringent anti-pollution packages (known as Erika I, II, and III), which require all tankers entering its waters to be double-hulled by 2010. The Erika packages are controversial because they introduced the new legal concept of "serious negligence". Ballast water When a large vessel such as a container ship or an oil tanker unloads cargo, seawater is pumped into other compartments in the hull to help stabilize and balance the ship. During loading, this ballast water is pumped out from these compartments.One of the problems with ballast water transfer is the transport of harmful organisms. Meinesz believes that one of the worst cases of a single invasive species causing harm to an ecosystem can be attributed to a seemingly harmless jellyfish. Mnemiopsis leidyi, a species of comb jellyfish that inhabits estuaries from the United States to the Valdés peninsula in Argentina along the Atlantic coast, has caused notable damage in the Black Sea. It was first introduced in 1982, and thought to have been transported to the Black Sea in a ship’s ballast water. The population of the jellyfish shot up exponentially and, by 1988, it was wreaking havoc upon the local fishing industry. "The anchovy catch fell from 204,000 tonnes (225,000 short tons; 201,000 long tons) in 1984 to 200 tonnes (220 short tons; 197 long tons) in 1993; sprat from 24,600 tonnes (27,100 short tons; 24,200 long tons) in 1984 to 12,000 tonnes (13,200 short tons; 11,800 long tons) in 1993; horse mackerel from 4,000 tonnes (4,410 short tons; 3,940 long tons) in 1984 to zero in 1993." Now that the jellyfish have exhausted the zooplankton, including fish larvae, their numbers have fallen dramatically, yet they continue to maintain a stranglehold on the ecosystem. Recently the jellyfish have been discovered in the Caspian Sea. Invasive species can take over once occupied areas, facilitate the spread of new diseases, introduce new genetic material, alter landscapes and jeopardize the ability of native species to obtain food. "On land and in the sea, invasive species are responsible for about 137 billion dollars in lost revenue and management costs in the U.S. each year."Ballast and bilge discharge from ships can also spread human pathogens and other harmful diseases and toxins potentially causing health issues for humans and marine life alike. Discharges into coastal waters, along with other sources of marine pollution, have the potential to be toxic to marine plants, animals, and microorganisms, causing alterations such as changes in growth, disruption of hormone cycles, birth defects, suppression of the immune system, and disorders resulting in cancer, tumors, and genetic abnormalities or even death. Exhaust emissions Exhaust emissions from ships are considered to be a significant source of air pollution. “Seagoing vessels are responsible for an estimated 14 percent of emissions of nitrogen from fossil fuels and 16 percent of the emissions of sulfur from petroleum uses into the atmosphere.” In Europe ships make up a large percentage of the sulfur introduced to the air, “...as much sulfur as all the cars, lorries and factories in Europe put together.” “By 2010, up to 40% of air pollution over land could come from ships.” Sulfur in the air creates acid rain which damages crops and buildings. When inhaled sulfur is known to cause respiratory problems and increase the risk of a heart attack. Ship breaking Ship breaking or ship demolition is a type of ship disposal involving the breaking up of ships for scrap recycling, with the hulls being discarded in ship graveyards. Most ships have a lifespan of a few decades before there is so much wear that refitting and repair becomes uneconomical. Ship breaking allows materials from the ship, especially steel, to be reused.In addition to steel and other useful materials, however, ships (particularly older vessels) can contain many substances that are banned or considered dangerous in developed countries. Asbestos and polychlorinated biphenyls (PCBs) are typical examples. Asbestos was used heavily in ship construction until it was finally banned in most of the developed world in the mid 1980s. Currently, the costs associated with removing asbestos, along with the potentially expensive insurance and health risks, have meant that ship-breaking in most developed countries is no longer economically viable. Removing the metal for scrap can potentially cost more than the scrap value of the metal itself. In most of the developing world, however, shipyards can operate without the risk of personal injury lawsuits or workers' health claims, meaning many of these shipyards may operate with high health risks. Furthermore, workers are paid very low rates with no overtime or other allowances. Protective equipment is sometimes absent or inadequate. Dangerous vapors and fumes from burning materials can be inhaled, and dusty asbestos-laden areas around such breakdown locations are commonplace.Aside from the health of the yard workers, in recent years, ship breaking has also become an issue of major environmental concern. Many developing nations, in which ship breaking yards are located, have lax or no environmental law, enabling large quantities of highly toxic materials to escape into the environment and causing serious health problems among ship breakers, the local population and wildlife. Environmental campaign groups such as Greenpeace have made the issue a high priority for their campaigns. See also Admiralty lawAirshipChartering (shipping)Dynamic positioningEnvironmental impact of shippingFactory shipFerryFlag stateGlossary of nautical termsMarine electronicsMarine fuel managementMaritime historyMother shipNaval architectureNavyNuclear marine propulsionPropulsionSailingSailing shipSailorShip burialShip transportShipwreckSpaceshipTrain ferryVessel safety surveyWhalerModel shipsShip modelShip model basinShip replicaListsList of fictional shipsList of historical ship typesList of Panamax portsList of the world's largest cruise shipsList of world's largest ships by gross tonnageList of world's longest shipsLists of shipsLists of shipwrecksShip sizesAframaxCapesizeChinamaxHandymaxHandysizeMaersk Triple E classMalaccamaxPanamaxQ-MaxSeawaymaxSuezmaxUltra Large Crude CarrierValemaxVLCC Notes  References Anzovin, Steven (2000). Famous First Facts (International Edition). H. W. Wilson Company. ISBN 0-8242-0958-3. Bowditch, Nathaniel (2002). The American Practical Navigator. Bethesda, MD: National Imagery and Mapping Agency. ISBN 0-939837-54-4. Central Intelligence Agency (2007). CIA World Factbook 2008. Skyhorse Publishing. ISBN 1-60239-080-0. Retrieved 2008-02-22. Chatterton, Edward Keble (1915). Sailing Ships and Their Story: The Story of Their Development from the Earliest Times to the Present Day. Philadelphia: J.B. Lippincott Company. Retrieved 2008-10-02. Cotterill, Charles Clement; Little, Edward Delanoy (1868). Ships and sailors, ancient and modern. London: Seeley, Jackson and Halliday. Cutler, Thomas J. (1999). The Bluejacket's Manual (Bluejacket's Manual, 22nd ed). Annapolis, Md: Naval Institute Press. ISBN 1-55750-065-7. Cutler, Thomas J. (December 2003). Dutton's Nautical Navigation (15th ed.). Annapolis, MD: Naval Institute Press. ISBN 978-1-55750-248-3. "Knock Nevis (7381154)". Miramar Ship Index. Retrieved 2016-05-17. (subscription required (help)). Encyclopædia Britannica (1911). "Navigation". In Chisholm, Hugh. Encyclopædia Britannica. 19 (11th ed.). Encyclopædia Britannica (1911). "Ship". In Chisholm, Hugh. Encyclopædia Britannica. 24 (11th ed.). pp. 881–889. Fisheries and Aquacultures Department (2007). "The Status of the Fishing Fleet". The State of World Fisheries and Aquaculture 2006. Rome: Food and Agriculture Organization of the United Nations. George, William (2005). Stability and Trim for the Ship's Officer. Centreville, MD: Cornell Maritime Press. ISBN 978-0-87033-564-8. Hayler, William B.; Keever, John M. (2003). American Merchant Seaman's Manual. Cornell Maritime Pr. ISBN 0-87033-549-9. Huber, Mark (2001). Tanker operations: a handbook for the person-in-charge (PIC). Cambridge, MD: Cornell Maritime Press. ISBN 0-87033-528-6. Lavery, Brian (2004). Ship: The Epic Story of Maritime Adventure (Smithsonian). New York: DK Publishing Inc. ISBN 0-7566-0496-6. Maloney, Elbert S. (December 2003). Chapman Piloting and Seamanship (64th ed.). New York: Hearst Communications. ISBN 1-58816-089-0. Office of Data and Economic Analysis (July 2006). "World Merchant Fleet 2001–2005" (PDF). United States Maritime Administration. Retrieved March 13, 2007. Overseas Shipholding Group (2008-02-22). "Overseas Shipholding Group Fleet List". Overseas Shipholding Group. Sawyer, L. A.; Mitchell, W. O. (1987). Sailing ship to supertanker: the hundred-year story of British Esso and its ships. Lavenham, Suffolk: Terence Dalton. ISBN 0-86138-055-X. Singh, Baljit (July 11, 1999). "The world's biggest ship". The Times (of India). Retrieved 2008-04-07. Turpin, Edward A.; McEwen, William A. (1980). Merchant Marine Officers' Handbook (4th ed.). Centreville, MD: Cornell Maritime Press. ISBN 0-87033-056-X. United Nations Conference on Trade and Development (UNCTAD) (2006). Review of Maritime Transport, 2006 (PDF). New York and Geneva: United Nations. United Nations Conference on Trade and Development (UNCTAD) (2007). Review of Maritime Transport, 2007 (PDF). New York and Geneva: United Nations. Stopford, Martin (1997). Maritime economics. New York: Routledge. ISBN 0-415-15309-3.  External links Ship partsNautical terms and ship partsTanker shipsShip sizesMalaccamaxShip sizes from handymax to ULCC
A computer is a device that can be instructed to carry out an arbitrary set of arithmetic or logical operations automatically. The ability of computers to follow a sequence of operations, called a program, make computers very applicable to a wide range of tasks. Such computers are used as control systems for a very wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer assisted design, but also in general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects millions of other computers.Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers has increased continuously and dramatically since then.Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved. Etymology According to the Oxford English Dictionary, the first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number." This usage of the term referred to a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.The Online Etymology Dictionary gives the first attested use of "computer" in the "1640s, [meaning] "one who calculates,"; this is an "... agent noun from compute (v.)". The Online Etymology Dictionary states that the use of the term to mean "calculating machine" (of any type) is from 1897." The Online Etymology Dictionary indicates that the "modern use" of the term, to mean "programmable digital electronic computer" dates from "... 1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine". History  Pre-20th century Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, circa 1000 AD.The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time–distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.In the 1770s Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers. First computing device Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.The machine was about a century ahead of its time. All the parts for his machine had to be made by hand — this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906. Analog computers During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (control systems) and aircraft (slide rule). Digital computers  Electromechanical By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was Turing complete. Vacuum tubes and digital electronic circuits Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first "automatic electronic digital computer". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.The U.S.-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors. Modern computers  Concept of modern computer The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper, On Computable Numbers. Turing proposed a simple device that he called "Universal Computing machine" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine. Stored programs Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report "Proposed Electronic Calculator" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube, the first random-access digital storage device. Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job. Transistors The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell. Integrated circuits The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel. Mobile computers become dominant With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so-called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013. Types Computers are typically classified based on their uses: Based on uses Analog computerDigital computerHybrid computer Based on sizes SmartphoneMicro computerPersonal computerLaptopMini ComputerMainframe computerSuper computer Hardware The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and "mice" input devices are all hardware. History of computing hardware  Other hardware topics A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits. Input devices When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand-operated input devices are:Computer keyboardDigital cameraDigital videoGraphics tabletImage scannerJoystickMicrophoneMouseOverlay keyboardTrackballTouchscreen Output devices The means through which computer gives output are known as output devices. Some examples of output devices are:Computer monitorPrinterPC speakerProjectorSound cardVideo card Control unit The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance.A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.The control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:Read the code for the next instruction from the cell indicated by the program counter.Decode the numerical code for the instruction into a set of commands or signals for each of the other systems.Increment the program counter so it points to the next instruction.Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code.Provide the necessary data to an ALU or register.If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation.Write the result from the ALU back to a memory location or to a register or perhaps an output device.Jump back to step (1).Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen. Central processing unit (CPU) The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor. Arithmetic logic unit (ALU) The ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices. Memory A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28  256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.Computer main memory comes in two principal varieties:random-access memory or RAMread-only memory or ROMRAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part. Input/output (I/O) I/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O. I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry. Multitasking While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn. One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time-sharing" since each program is allocated a "slice" of time in turn.Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss. Multiprocessing Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called "embarrassingly parallel" tasks. Software Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible) computer, it is sometimes called "firmware". Operating systems  Languages There are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications. Application Software  Programs The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors. Stored program architecture This section applies to most common RAM machine-based computers.In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second. Machine code In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler. Programming language Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques. Low-level languages Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC. High-level languages/third generation language Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles. Fourth generation languages These 4G languages are less procedural than 3G languages. The benefit of 4GL is that they provide ways to obtain information without requiring the direct help of a programmer. An example of a 4GL is SQL. Program design Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge. Bugs Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design. Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947. Firmware Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it. Networking and the Internet Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments. Misconceptions  Human computer A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information." Any device which processes information qualifies as a computer, especially if the processing is purposeful. Unconventional computing Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example. More realistically, modern computers are made out of transistors made of photolithographed semiconductors. Future There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly. Computer architecture paradigms There are many types of computer architectures:Quantum computer vs. Chemical computerScalar processor vs. Vector processorNon-Uniform Memory Access (NUMA) computersRegister machine vs. Stack machineHarvard architecture vs. von Neumann architectureCellular architectureOf all these abstract machines, a quantum computer holds the most promise for revolutionizing computing. Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity. Artificial intelligence A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Professions and organizations As the use of computers has spread throughout society, there are an increasing number of careers involving computers.The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature. See also  References  Notes  External links  Media related to Computers at Wikimedia Commons  Wikiversity has a quiz on this articleWarhol & The Computer
Cricket is a bat-and-ball game played between two teams of eleven players on a cricket field, at the centre of which is a rectangular 22-yard-long pitch with a wicket (a set of three wooden stumps) at each end. One team bats, attempting to score as many runs as possible, whilst their opponents field. Each phase of play is called an innings. After either ten batsmen have been dismissed or a fixed number of overs have been completed, the innings ends and the two teams then swap roles. The winning team is the one that scores the most runs, including any extras gained, during their innings.At the start of each game, two batsmen and eleven fielders enter the field of play. The play begins when a member of the fielding team, known as the bowler, delivers the ball from one end of the pitch to the other, towards the wicket at that end, in front of which stands one of the batsmen, known as the striker. The striker "takes guard" on a crease drawn on the pitch four feet in front of the wicket. His role is to prevent the ball from hitting the stumps by use of his bat, and simultaneously to strike it well enough to score runs. The other batsman, known as the non-striker, waits at the opposite end of the pitch near the bowler. A dismissed batsman must leave the field, and a teammate replaces him. The bowler's objectives are to prevent the scoring of runs and to dismiss the batsman. An over is a set of six deliveries bowled by the same bowler. The next over is bowled from the other end of the pitch by a different bowler.The most common forms of dismissal are bowled, when the bowler hits the stumps directly with the ball, leg before wicket, when the batsman prevents the ball from hitting the stumps with his body instead of his bat, and caught, when the batsman hits the ball into the air and it is intercepted by a fielder before touching the ground. Runs are scored by two main methods: either by hitting the ball hard enough for it to cross the boundary, or by the two batsmen swapping ends by each simultaneously running the length of the pitch in opposite directions whilst the fielders are retrieving the ball. If a fielder retrieves the ball quickly enough to put down the wicket with a batsman not having reached the crease at that end of the pitch, that batsman is dismissed (a run-out occurs). Adjudication is performed on the field by two umpires.The laws of cricket are maintained by the International Cricket Council (ICC) and the Marylebone Cricket Club (MCC). There are various formats ranging from Twenty20, played over a few hours with each team having a single innings of 20 overs (i.e. 120 deliveries), to Test cricket, played over five days with unlimited overs and the teams playing two innings apiece. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid object made of compressed leather enclosing a cork core.Although cricket's origins are uncertain, it is first recorded in south-east England in the 16th century. It spread globally with the expansion of the British Empire, leading to the first international matches in the mid-19th century. ICC, the game's governing body, has over 100 members, ten of which are full members who play Test cricket. The sport is followed primarily in Australasia, Britain, the Indian subcontinent, southern Africa and the West Indies. Women's cricket, which is organised and played separately, has also achieved international standard. Etymology A number of words have been suggested as sources for the term "cricket". In the earliest definite reference to the sport in 1598 it is called creckett. One possible source for the name is the Old English cricc or cryce meaning a crutch or staff. In Samuel Johnson's Dictionary, he derived cricket from "cryce, Saxon, a stick". In Old French, the word criquet seems to have meant a kind of club or stick.Given the strong medieval trade connections between south-east England and the County of Flanders when the latter belonged to the Duchy of Burgundy, the name may have been derived from the Middle Dutch (in use in Flanders at the time) krick(-e), meaning a stick (crook). Another possible source is the Middle Dutch word krickstoel, meaning a long low stool used for kneeling in church and which resembled the long low wicket with two stumps used in early cricket. According to Heiner Gillmeister, a European language expert of Bonn University, "cricket" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., "with the stick chase"). Dr Gillmeister believes that not only the name but the sport itself is of Flemish origin. History Cricket can definitely be traced back to Tudor times in early 16th-century England though there have been a number of claims, many of them spurious and/or lacking evidence, supporting earlier dates from 1301. The earliest definite reference to cricket being played comes from evidence given at a 1598 court case which mentions that "creckett" (sic) was played on common land in Guildford around 1550. The court in Guildford heard on Monday, 17 January 1597 (Julian date, equating to the year 1598 in the Gregorian calendar) from a 59-year-old coroner, John Derrick, who gave witness that when he was a scholar at the "Free School at Guildford", fifty years earlier, "hee and diverse of his fellows did runne and play [on the common land] at creckett and other plaies."It is believed that cricket was originally a children's game but references in 1611 indicate that adults had started playing it and the earliest known organised inter-parish or village cricket match was played around that time. In 1624, a player called Jasper Vinall died after he was struck on the head during a match between two parish teams in Sussex. During the 17th century, numerous references indicate the growth of cricket in the south-east of England. By the end of the century, it had become an organised activity being played for high stakes and it is believed that the first professionals appeared in the years following the Restoration in 1660. A newspaper report survives of "a great cricket match" with eleven players a side that was played for high stakes in Sussex in 1697, and this is the earliest known reference to a cricket match of such importance.The game underwent major development in the 18th century. Betting played a key part in that development with rich patrons forming their own "select XIs". Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury. The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season. Bowling underwent an evolution around 1760 when bowlers began to pitch the ball instead of rolling or skimming it towards the batsman. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old "hockey stick" shape. The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point. MCC quickly became the sport's premier club and the custodian of the Laws of cricket. New Laws introduced in the latter part of the 18th century included the three stump wicket and leg before wicket (lbw).The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial. Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839, which ultimately formed the official County Championship in 1890. Meanwhile, the British Empire had been instrumental in spreading the game overseas and by the middle of the 19th century it had become well established in India, North America, the Caribbean, South Africa, Australia and New Zealand. In 1844, the first-ever international match took place between the United States and Canada. In 1859, a team of English players went to North America on the first overseas tour.The first Australian team to tour overseas was a team of Aboriginal stockmen who travelled to England in 1868 to play matches against county teams. In 1862, an English team made the first tour of Australia. The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865.In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia. The rivalry between England and Australia gave birth to The Ashes in 1882 and this has remained Test cricket's most famous contest. Test cricket began to expand in 1888–89 when South Africa played England.The last two decades before the First World War have been called the "Golden Age of cricket". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.The inter-war years were dominated by one player: Australia's Don Bradman, statistically the greatest batsman of all time. Test cricket continued to expand during the 20th century with the addition of the West Indies, India and New Zealand before the Second World War and then Pakistan, Sri Lanka, Zimbabwe and Bangladesh in the post-war period. South Africa was banned from international cricket from 1970 to 1992 as part of the apartheid boycott.Cricket entered a new era in 1963 when English counties introduced the limited overs variant. As it was sure to produce a result, limited overs cricket was lucrative and the number of matches increased. The first Limited Overs International was played in 1971. The governing International Cricket Council (ICC) saw its potential and staged the first limited overs Cricket World Cup in 1975. In the 21st century, a new limited overs form, Twenty20, has made an immediate impact.While some English team games like hockey and football became international games, played all over the world, cricket remained a colonial game, limited to those countries that had once been a part of the British empire. The pre-industrial oddness of cricket made it a hard game to export. It took root only in countries that the British conquered and ruled. In these colonies cricket was established as a popular sport either by white settlers or by local elites who wanted to copy the habits of their colonial masters, as in India. Rules and game-play Cricket is a bat and ball game, played between two teams of eleven players each. One team bats, attempting to score runs, while the other bowls and fields the ball, attempting to restrict the scoring and dismiss the batsmen. The objective of the game is for a team to score more runs than its opponent. In some forms of cricket, it is also necessary to dismiss the opposition in order to win the match, which would otherwise be drawn. Format of the game A cricket match is divided into periods called innings (which ends with "s" in both singular and plural form). It is decided before the match whether the teams will have one innings or two innings each. During an innings one team fields and the other bats. The two teams switch between fielding and batting after each innings. All eleven members of the fielding team take the field, but only two members of the batting team (two batsmen) are on the field at any given time. The order of batsmen is usually announced just before the match, but it can be varied.A coin toss is held by the team captains (who are also players) just before the match starts: the winner decides whether to bat or field first.The cricket field is usually circular or oval in shape, with a rectangular pitch at the centre. The edge of the playing field is marked with a boundary, which could be a fence, part of the stands, a rope or a painted line.At each end of the pitch is a wooden target called a wicket; the two wickets are placed 22 yards (20 m) apart. The pitch is marked with painted lines: a bowling crease in line with the wicket, and a batting or popping crease four feet (122 cm) in front of it. The wicket is made of three vertical stumps supporting two small horizontal bails. A wicket is put down if at least one bail is dislodged, or one stump is knocked down (usually by the ball, but also if the batsman does it with his body, clothing or equipment). This is also described as breaking, knocking down, or hitting the wicket – though if the ball hits the wicket but does not dislodge a bail or stump then it is not considered to be down.At any instant each batsman "owns" a particular wicket (usually the one closer to him) and, except when actually batting, is safe when he is in his ground. This means that at least one part of his body or bat is touching the ground behind the popping crease. If his wicket is put down while the ball is live and he is out of his ground then he is dismissed, but the other batsman is safe.The two batsmen take positions at opposite ends of the pitch. One designated member of the fielding team, called the bowler, bowls the ball from one end of the pitch to the striking batsman at the other end. The batsman at the bowling end is called the non-striker, and stands to the side of his wicket, behind his crease. The batsman are allowed to step forward of their creases, though at some risk. Another member of the fielding team, the wicket keeper, is positioned behind the striker's wicket.The fielding team's other nine members stand outside the pitch, spread out across the field. The fielding captain often tactically changes their position between balls.There is always an umpire at each end of the pitch.The bowler usually retreats a few yards (metres) behind the wicket, runs towards it (his run-up), and then releases the ball over-hand as he reaches the bowling crease. (If he crosses the crease before he releases the ball, or if he flexes his elbow too much in a throw, then it is a no ball, the batsman cannot be dismissed, and the batting team gets a penalty or extra run. If the ball passes the far wicket out of reach of the batsman then it is called a wide, also with an extra run.) The ball can be bowled so that it bounces on the pitch, lands exactly on the crease (a yorker), or crosses the crease without bouncing (a full toss).A no ball or a wide does not count towards the six balls in the over.The batsman tries to prevent the ball from hitting the wicket by striking the ball with his bat. (This includes the handle of the bat, and his gloves.) If the bowler succeeds in putting down the wicket the batsman is dismissed and is said to be bowled out. If the batsman misses the ball, but any part of his body prevents it from reaching the wicket, then he is out leg before wicket, or "LBW".If the batsman hits the ball but it is caught by a fielder without bouncing then he is caught out. If it is caught by the bowler then he is caught and bowled; by the wicket keeper, caught behind.If the batsman is successful in striking the ball and it is not caught without bouncing, then the two batsmen may try to score points (runs) for their team. Both batsmen run the length of the pitch, exchanging positions, and grounding their bats behind the opposite crease. Each crossing and grounding by both batsmen is worth one run. The batsmen may attempt one run, or multiple runs, or elect not to run at all. By attempting runs, the batsmen risk dismissal. This happens if the fielding team retrieves the ball and hits either wicket with the ball (either by throwing it, or while holding it) before the batsman who owns that wicket reaches his ground behind the crease. The dismissed batsman is run out. Batsmen will sometimes start to run, change their minds, and return to their original positions.If the batsman hits the ball over the field boundary without the ball touching the field, the batting team scores six runs. If the ball touches the ground and then reaches the boundary, the batting team scores four runs. The batsmen might have started running before the ball reaches the boundary, but those runs do not count.If the batsman misses the ball they can still attempt extra runs : these are called byes. If the ball bounces off his body then it is called a leg bye.If the striking batsman leaves his ground and misses the ball, then the wicket keeper can catch it and put down the wicket – out stumped.In case of a no ball or a wide the batsman can choose to strike the ball, earning runs in addition to the fixed penalty. If he does so he can only be dismissed by being run out.When the batsmen have finished attempting their runs the ball is dead, and is returned to the bowler to be bowled again. The ball becomes live when he starts his run up. The bowler continues to bowl toward the same wicket, regardless of any switch of the batsmen's positions.A batsman may retire from an innings without being dismissed, usually after reaching a milestone like a hundred runs (a century).A dismissed batsman leaves the field, to be replaced by another batsman from the batting team. However, even though the wicket may have been put down, or the ball caught, the batsman is not actually dismissed until the fielding team appeal to the umpires for a decision, traditionally using the expression "How's that" (or "Howzat") (although often the batsman will immediately walk away without the need for an appeal). In some matches, particularly Test matches, either team may request a review by a third umpire who can use a decision review system (DRS), which includes TV replays and other electronic equipment such as Hawk-Eye, Hot Spot and the Snickometer.After a bowler has bowled six times (an over), another member of the fielding team is designated as the new bowler, the old bowler taking up a fielding position. The batsmen stay in place, and the new bowler bowls to the opposite wicket, so the roles of striker and non-striker reverse. The wicket keeper and the two umpires always change positions, as do many of the fielders, and play continues. Bowlers may (and usually do) bowl multiple times during an innings, but may not bowl two overs in succession.The innings is complete when 10 of the 11 members of the batting team have been dismissed (all out – although one always remaining "not out"), when a set number of overs has been played, or when the batting team declares that they have enough runs.The number of innings and the number of overs per innings vary depending on the format of the match. In a match which is not a limited overs format the umpires will often specify that the last session of the day will have a specified number of overs rather than continuing until a specified time (to avoid time wasting by either team).The match ends when all innings have been completed. The umpires can also call an end to the match in case of bad light or weather. But often the match ends immediately when one team to bat has completed its innings or both its innings, and the other team already has more runs. In four-innings games the last team may not even need to play its second innings: this team is said to win by an innings. If the winning team has not completed its last innings, and still has, for example, five batsmen who are not out or have not even batted, then they are said to "win by five wickets". If the last team to bat is all out and loses the match because it has say 50 fewer runs than the other team, then the winning team "wins by 50 runs". In the rare event that the two teams both complete their innings and they have the same number of runs, then it is a tie.In matches which are not limited overs there is also the possibility of a draw: the team with fewer runs still has batsmen on the field when the game ends, usually because time has run out. This has an impact on strategy: a team will often declare their innings closed when they have accumulated enough runs, in the hope that they will have enough time left to dismiss the other team and thus avoid a draw, but risking a loss if the other team scores enough runs. Pitch, wickets and creases  Playing surface Cricket is played on a grassy field. The Laws of Cricket do not specify the size or shape of the field, but it is often oval. In the centre of the field is a rectangular strip, known as the pitch.The pitch is a flat surface 10 feet (3.0 m) wide, with very short grass that tends to be worn away as the game progresses. At either end of the pitch, 22 yards (20 m) apart, are placed wooden targets, known as the wickets. These serve as a target for the bowling (also known as the fielding) side and are defended by the batting side, which seeks to accumulate runs. Stumps, bails and creases Each wicket on the pitch consists of three wooden stumps placed vertically, in line with one another. They are surmounted by two wooden crosspieces called bails; the total height of the wicket including bails is 28.5 inches (720 mm) and the combined width of the three stumps, including small gaps between them is 9 inches (230 mm).Four lines, known as creases, are painted onto the pitch around the wicket areas to define the batsman's "safe territory" and to determine the limit of the bowler's approach. These are called the "popping" (or batting) crease, the bowling crease and two "return" creases.The stumps are placed in line on the bowling creases and so these creases must be 22 yards (20 m) apart. A bowling crease is 8 feet 8 inches (2.64 m) long, with the middle stump placed dead centre. The popping crease has the same length, is parallel to the bowling crease and is 4 feet (1.2 m) in front of the wicket. The return creases are perpendicular to the other two; they are adjoined to the ends of the popping crease and are drawn through the ends of the bowling crease to a length of at least 8 feet (2.4 m).When bowling the ball, the bowler's back foot in his "delivery stride" must land within the two return creases while at least some part of his front foot must land on or behind the popping crease. If the bowler breaks this rule, the umpire calls "No ball".The importance of the popping crease to the batsman is that it marks the limit of his safe territory. He can be dismissed stumped or run out (see Dismissals below) if the wicket is broken while he is "out of his ground". Bat and ball The essence of the sport is that a bowler delivers the ball from his end of the pitch towards the batsman who, armed with a bat is "on strike" at the other end.The bat is made of wood (usually White Willow) and has the shape of a blade topped by a cylindrical handle. The blade must not be more than 4.25 inches (108 mm) wide and the total length of the bat not more than 38 inches (970 mm).The ball is a hard leather-seamed spheroid, with a circumference of 9 inches (230 mm). The hardness of the ball, which can be delivered at speeds of more than 90 miles per hour (140 km/h), is a matter for concern and batsmen wear protective clothing including pads (designed to protect the knees and shins), batting gloves for the hands, a helmet for the head and a box inside the trousers (to protect the crotch area). Some batsmen wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The ball has a "seam": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent, and helps the bowler propel it in a less predictable manner. During cricket matches, the quality of the ball deteriorates to a point where it is no longer usable, and during the course of this deterioration its behaviour in flight will change and thus influence the match. Players will therefore attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva is legal, even when the polishing is deliberately done on one side only to increase the ball's swing, while rubbing other substances into the ball, scratching the surface or picking at the seam is illegal ball tampering. Umpires and scorers The game on the field is regulated by two umpires, one of whom stands behind the wicket at the bowler's end, the other in a position called "square leg", a position 15–20 metres to the side of the "on strike" batsman. The main role of the umpires is to adjudicate on whether a ball is correctly bowled (not a no ball or a wide), when a run is scored, and whether a batsman is out (the fielding side must appeal to the umpire, usually with the phrase How's That?). Umpires also determine when intervals start and end, decide on the suitability of the playing conditions and can interrupt or even abandon the match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.Off the field and in televised matches, there is often a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test matches and limited overs internationals played between two ICC full members. These matches also have a match referee whose job is to ensure that play is within the Laws of cricket and the spirit of the game.The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire. For example, the umpire raises a forefinger to signal that the batsman is out (has been dismissed); he raises both arms above his head if the batsman has hit the ball for six runs. The scorers are required by the Laws of cricket to record all runs scored, wickets taken and overs bowled; in practice, they also note significant amounts of additional data relating to the game. Innings The innings (ending with 's' in both singular and plural form) is the term used for the collective performance of the batting side. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. Depending on the type of match being played, each team has either one or two innings.The main aim of the bowler, supported by his fielders, is to dismiss the batsman. A batsman when dismissed is said to be "out" and that means he must leave the field of play and be replaced by the next batsman on his team. When ten batsmen have been dismissed (i.e. are out), then the whole team is dismissed and the innings is over. The last batsman, the one who has not been dismissed, is not allowed to continue alone as there must always be two batsmen "in". This batsman is termed "not out".An innings can end early for three reasons: because the batting side's captain has chosen to "declare" the innings closed (which is a tactical decision), or because the batting side has achieved its target and won the game, or because the game has ended prematurely due to bad weather or running out of time. In each of these cases the team's innings ends with two "not out" batsmen, unless the innings is declared closed at the fall of a wicket and the next batsman has not joined in the play.In limited overs cricket, there might be two batsmen still "not out" when the last of the allotted overs has been bowled. Overs The bowler bowls the ball in sets of six deliveries (or "balls") and each set of six balls is called an over. This name came about because the umpire calls "Over!" when six balls have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batsmen do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs. The batsmen do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice versa. (Sometimes when one of the two batsmen is a much stronger player than the other, he tries to score a single run from the last ball of the over so that he can remain "on strike".) The umpires also change positions so that the one who was at square leg now stands behind the wicket at the non-striker's end and vice versa.The only occasion where one bowler bowls consecutive overs is in the Super Over directly after the final over of a tied game of a limited-overs match. Team structure A team consists of eleven players. Depending on his or her primary skills, a player may be classified as a specialist batsman or bowler. A well-balanced team usually has five or six specialist batsmen and four or five specialist bowlers. Teams nearly always include a specialist wicket-keeper because of the importance of this fielding position. Each team is headed by a captain who is responsible for making tactical decisions such as determining the batting order, the placement of fielders and the rotation of bowlers.A player who excels in both batting and bowling is known as an all-rounder. One who excels as a batsman and wicket-keeper is known as a "wicket-keeper/batsman", sometimes regarded as a type of all-rounder. True all-rounders are rare as most players focus on either batting or bowling skills. Bowling The bowler reaches his delivery stride by means of a "run-up", although some bowlers with a very slow delivery take no more than a couple of steps before bowling. A fast bowler needs momentum and takes quite a long run-up, running very fast as he does so.The fastest bowlers can deliver the ball at a speed of over 90 miles per hour (140 km/h) and they sometimes rely on sheer speed to try and defeat the batsman, who is forced to react very quickly. Other fast bowlers rely on a mixture of speed and guile. Some fast bowlers make use of the seam of the ball so that it "curves" or "swings" in flight. This type of delivery can deceive a batsman into mistiming his shot so that the ball just touches the edge of the bat and can then be "caught behind" by the wicketkeeper or a slip fielder.At the other end of the bowling scale is the "spinner" who bowls at a relatively slow pace and relies entirely on guile to deceive the batsman. A spinner will often "buy his wicket" by "tossing one up" (in a slower, higher parabolic path) to lure the batsman into making a poor shot. The batsman has to be very wary of such deliveries as they are often "flighted" or spun so that the ball will not behave quite as he expects and he could be "trapped" into getting himself out.In between the pacemen and the spinners are the "medium pacers" who rely on persistent accuracy to try and contain the rate of scoring and wear down the batsman's concentration.All bowlers are classified according to their looks or style. The classifications, as with much cricket terminology, can be very confusing. Hence, a bowler could be classified as LF, meaning he is a left arm fast bowler; or as LBG, meaning he is a right arm spin bowler who bowls deliveries that are called a "leg break" and a "Googly".During the bowling action the elbow may be held at any angle and may bend further, but may not straighten out. If the elbow straightens illegally then the square-leg umpire may call no-ball: this is known as "throwing" or "chucking", and can be difficult to detect. The current laws allow a bowler to straighten his arm 15 degrees or less. Fielding All eleven players on the fielding side take the field together. One of them is the wicket-keeper (or "keeper") who operates behind the wicket being defended by the batsman on strike. Wicket-keeping is normally a specialist occupation and his primary job is to gather deliveries that the batsman does not hit, so that the batsmen cannot run byes. He wears special gloves (he is the only fielder allowed to do so), a box over the groin, and pads to cover his lower legs. He is the only player who can get a batsman out stumped.Apart from the one currently bowling, the other nine fielders are tactically deployed by the team captain in chosen positions around the field.The captain is the most important member of the fielding side as he determines all the tactics including who should bowl (and how); and he is responsible for "setting the field", though usually in consultation with the bowler.In all forms of cricket, if a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of him. The substitute cannot bowl, act as a captain or keep wicket. The substitute leaves the field when the injured player is fit to return. Batting At any one time, there are two batsmen in the playing area. One takes station at the striker's end to defend the wicket as above and to score runs if possible. His partner, the non-striker, is at the end where the bowler is operating.Batsmen come in to bat in a batting order, decided by the team captain usually after consulting the team coach, though the captain is not bound to consult the coach. The first two batsmen – the "openers" – usually face the hostile bowling from fresh fast bowlers with a new ball. The top batting positions are usually given to the most competent batsmen in the team, and the team's bowlers – who are typically, but not always, less skilled as batsmen – typically bat last. The pre-announced batting order is not mandatory; when a wicket falls any player who has not yet batted may be sent in next.If a batsman "retires" (usually due to injury) and cannot return, he is actually "not out" and his retirement does not count as a dismissal, though in effect he has been dismissed because his innings is over. Substitute batsmen are not allowed.A skilled batsman can use a wide array of "shots" or "strokes" in both defensive and attacking mode. The idea is to hit the ball to best effect with the flat surface of the bat's blade. If the ball touches the side of the bat it is called an "edge". Batsmen do not always seek to hit the ball as hard as possible, and a good player can score runs just by making a deft stroke with a turn of the wrists or by simply "blocking" the ball but directing it away from fielders so that he has time to take a run.There is a wide variety of shots played in cricket. The batsman's repertoire includes strokes named according to the style of swing and the direction aimed: e.g., "cut", "drive", "hook", "pull".A batsman is not required to play a shot; in the event that he believes the ball will not hit his wicket and there is no opportunity to score runs, he can "leave" the ball to go through to the wicketkeeper. Equally, he does not have to attempt a run when he hits the ball with his bat. He can deliberately use his leg to block the ball and thereby "pad it away", but this is risky because of the leg before wicket rule.In the event of an injured batsman being fit to bat but not to run, the umpires and the fielding captain could previously allow another member of the batting side to be a runner. The runner's only task was to run between the wickets instead of the incapacitated batsman, and he was required to wear and carry exactly the same equipment as the batsman. As of 2011 the ICC outlawed the use of runners as they felt this was being abused. Runs The batsman on strike (i.e. the "striker") must prevent the ball hitting the wicket, and try to score runs by hitting the ball with his bat so that he and his partner have time to run from one end of the pitch to the other before the fielding side can return the ball. To register a run, both runners must touch the ground behind the crease with either their bats or their bodies (the batsmen carry their bats as they run). Each completed run increments the score.More than one run can be scored from a single hit: hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more. To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batsmen do not need to run.Hits for five are unusual and generally rely on the help of "overthrows" by a fielder returning the ball. If an odd number of runs is scored by the striker, the two batsmen have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.The decision to attempt a run is ideally made by the batsman who has the better view of the ball's progress, and this is communicated by calling: "yes", "no" and "wait" are often heard.Running is a calculated risk because if a fielder breaks the wicket with the ball while the nearest batsman is out of his ground (i.e. he does not have part of his body or bat in contact with the ground behind the popping crease), the batsman is run out.A team's score is reported in terms of the number of runs scored and the number of batsmen that have been dismissed. For example, if five batsmen are out and the team has scored 224 runs, they are said to have scored 224 for the loss of 5 wickets (commonly shortened to "224 for five" and written 224/5 or, in Australia, "five for 224" and 5/224). Extras Additional runs can be gained by the batting team as extras (called "sundries" in Australia) due to errors made by the fielding side. This is achieved in four ways:No ball: a penalty of one extra that is conceded by the bowler if he breaks the rules of bowling either by (a) using an inappropriate arm action; (b) overstepping the popping crease; (c) having a foot outside the return crease. In addition, the bowler has to re-bowl the ball. In limited overs matches, a no ball is called if the bowling team's field setting fails to comply with the restrictions. In shorter formats of the game (20–20, ODI) the free hit rule has been introduced. The ball following a front foot no-ball will be a free-hit for the batsman, whereby he is safe from losing his wicket except for being run-out.Wide: a penalty of one extra that is conceded by the bowler if he bowls so that the ball is out of the batsman's reach; as with a no ball, a wide must be re-bowled. If a wide ball crosses the boundary, five runs are awarded to the batting side (one run for the wide, and four for the boundary).Bye: extra(s) awarded if the batsman misses the ball and it goes past the wicketkeeper to give the batsmen time to run in the conventional way (note that one mark of a good wicketkeeper is one who restricts the tally of byes to a minimum).Leg bye: extra(s) awarded if the ball hits the batsman's body, but not his bat, while attempting a legitimate shot, and it goes away from the fielders to give the batsmen time to run in the conventional way.When the bowler has bowled a no ball or a wide, his team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again and hence the batting side has the opportunity to score more runs from this extra ball. The batsmen have to run (i.e., unless the ball goes to the boundary for four) to claim byes and leg byes but these only count towards the team total, not to the striker's individual total for which runs must be scored off the bat. Dismissals There are eleven ways in which a batsman can be dismissed: five relatively common and six extremely rare. The common forms of dismissal are "bowled", "caught", "leg before wicket" (lbw), "run out", and (somewhat less common) "stumped". Rare methods are "hit wicket", "hit the ball twice", "obstructed the field", "handled the ball" and "timed out" – these are almost unknown in the professional game. The eleventh – retired out – is not treated as an on-field dismissal but rather a retrospective one for which no fielder is credited.If the dismissal is obvious (for example when "bowled" and in most cases of "caught") the batsman will voluntarily leave the field without the umpire needing to dismiss them. Otherwise before the umpire will award a dismissal and declare the batsman to be out, a member of the fielding side (generally the bowler) must "appeal". This is invariably done by asking (or shouting) "how's that?" – normally reduced to howzat? If the umpire agrees with the appeal, he will raise a forefinger and say "Out!". Otherwise he will shake his head and say "Not out". Appeals are particularly loud when the circumstances of the claimed dismissal are unclear, as is always the case with lbw and often with run outs and stumpings.Bowled: the bowler has hit the wicket with the delivery and the wicket has "broken" with at least one bail being dislodged (if the ball hits the wicket without dislodging a bail it is not out).Caught: the batsman has hit the ball with his bat, or with his hand which was holding the bat, and the ball has been caught before it has touched the ground by a member of the fielding side.Leg before wicket (lbw): the ball has hit the batsman's body (including his clothing, pads etc. but not the bat, nor a hand holding the bat) when it would have gone on to hit the stumps. This rule exists mainly to prevent the batsman from guarding his wicket with his legs instead of the bat. To be given out lbw, the ball must not bounce outside leg stump or strike the batsmen outside the line of leg-stump. It may bounce outside off-stump. The batsman may only be dismissed lbw by a ball striking him outside the line of off-stump if he has not made a genuine attempt to play the ball with his bat.Run out: a member of the fielding side has broken or "put down" the wicket with the ball while the nearest batsman was out of his ground; this occurs either by an accurate throw to the wicket, or more commonly by a throw to the wicketkeeper or other fielder standing near the wicket, while the batsmen are attempting a run, although a batsman can be given out "run out" even when he is not attempting a run; he merely needs to be out of his ground.Stumped is similar except that it is done by the wicketkeeper after the batsman has missed the bowled ball and has stepped out of his ground, and is not attempting a run. A batsman can be run out on a No ball, but cannot be stumped.Hit wicket: a batsman is out hit wicket if he dislodges one or both bails with his bat, person, clothing or equipment in the act of receiving a ball, or in setting off for a run having just received a ball.Hit the ball twice is very unusual and was introduced as a safety measure to counter dangerous play and protect the fielders. The batsman may legally play the ball a second time only to stop the ball hitting the wicket after he has already played it. "Hit" does not necessarily refer to the batsman's bat.Obstructing the field: another unusual dismissal which tends to involve a batsman deliberately getting in the way (physically and/or verbally) of a fielder.Handled the ball: a batsman must not deliberately touch the ball with his hand, for example to protect his wicket. Note that the batsman's hand or glove counts as part of the bat while the hand is holding the bat, so batsmen are frequently caught off their gloves (i.e. the ball hits, and is deflected by, the glove and can then be caught).Timed out; means that the next batsman was not ready to receive a delivery within three minutes of the previous one being dismissed.Retired out: a batsman retires without the umpire's permission, and does not have the permission of the opposition captain to resume their innings.In the vast majority of cases, it is the striker who is out when a dismissal occurs. If the non-striker is dismissed it is usually by being run out, but he could also be dismissed for obstructing the field, handling the ball or being timed out.A batsman may leave the field without being dismissed. If injured or taken ill the batsman may temporarily retire, and be replaced by the next batsman. This is recorded as retired hurt or retired ill. The retiring batsman is not out, and may resume the innings later if he recovers. An unimpaired batsman may retire, and this is treated as being dismissed retired out; no player is credited with the dismissal. Batsmen cannot be out bowled, caught, leg before wicket, stumped or hit wicket off a no ball. They cannot be out bowled, caught, leg before wicket, or hit the ball twice off a wide. Some of these modes of dismissal can occur without the bowler bowling a delivery. The batsman who is not on strike may be run out by the bowler if he leaves his crease before the bowler bowls, and a batsman can be out obstructing the field or retired out at any time. Timed out is, by its nature, a dismissal without a delivery. With all other modes of dismissal, only one batsman can be dismissed per ball bowled. Innings closed An innings is closed when:Ten of the eleven batsmen are out (have been dismissed); in this case, the team is said to be "all out"The team has only one batsman left who can bat, one or more of the remaining players being unavailable owing to injury, illness or absence; again, the team is said to be "all out"The team batting last reaches the score required to win the matchThe predetermined number of overs has been bowled (in a one-day match only, commonly 50 overs; or 20 in Twenty20)A captain declares his team's innings closed while at least two of his batsmen are not out (this does not apply in one-day limited over matches) Results If the team that bats last is all out having scored fewer runs than their opponents, the team is said to have "lost by n runs" (where n is the difference between the number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have "won by n wickets", where n is the number of wickets left to fall. For instance a team that passes its opponents' score having only lost six wickets would have won "by four wickets".In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have won by an innings and n runs, and does not need to bat again: n is the difference between the two teams' aggregate scores.If the team batting last is all out, and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.If the match has only a single innings per side, then a maximum number of deliveries for each innings is often imposed. Such a match is called a "limited overs" or "one-day" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth-Lewis method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a "no-result" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible; for example, wet weather. Distinctive elements  Individual focus For a team sport, cricket places individual players under unusual scrutiny and pressure. Bowler, batsman, and fielder all act essentially independently of each other. While team managements can signal to a bowler or batsman to pursue certain tactics, the execution of the play itself is a series of solitary acts. Cricket is more similar to baseball than many other team sports in this regard: while the individual focus in cricket is slightly mitigated by the importance of the batting partnership and the practicalities of running, it is enhanced by the fact that a batsman's innings may continue for several hours. Spirit of the Game Cricket is a unique game where in addition to the laws, the players must abide by the "Spirit of the Game". The standard of sportsmanship has historically been considered so high that the phrase "it's just not cricket" was coined in the 19th century to describe unfair or underhanded behaviour in any walk of life. In the last few decades though, with financial incentives, high-level cricket has become increasingly fast-paced and competitive, increasing the use of appealing and sledging, although players are still expected to abide by the umpires' rulings without argument, and for the most part they do. Beginning in 2001, the MCC has held an annual lecture named after Colin Cowdrey on the spirit of the game. Even in the modern game fielders are known to signal to the umpire that a boundary was hit, despite what could have been considered a spectacular save (though sometimes they might be found out by the TV replays anyway). In addition to this, some batsmen have been known to "walk" when they think they are out even if the umpire does not declare them out. This is a high level of sportsmanship, as sometimes a batsman could take advantage of incorrect umpiring decisions. Influence of weather Cricket is a sport played predominantly in the drier periods of the year. But, even so, the weather is a key factor in many cricket matches.Cricket cannot be played in wet weather. Dampness affects the bounce of the ball on the wicket and is a risk to all players involved in the game. Many grounds have facilities to cover the cricket pitch (or the wicket). Covers can be in the form of tarpaulins laid over the wicket, elevated covers on wheels (acting like an umbrella) or even hover covers which form an airtight seal around the wicket. However, most grounds do not have the facilities to cover the outfield. This means that in the event of heavy rain, a match may be cancelled, abandoned or suspended due to an unsafe outfield.Another factor in cricket is the amount of light available. At grounds without floodlights (or in game formats which disallow the use of floodlights), umpires can stop play in the event of bad light as it becomes too difficult for the batsmen (and in extreme cases, fielders) to see the ball coming at them.The sight-screens give a white background which help batsmen pick out the red ball (or a black background for a white ball).The umpires always have the final decision on weather-related issues. Uniqueness of each field Unlike those of most sports, cricket playing fields can vary significantly in size and shape. While the dimensions of the pitch and infield are specifically regulated, the Laws of Cricket do not specify the size or shape of the field. The field boundaries are sometimes painted and sometimes marked by a rope. Pitch and outfield variations can have a significant effect on how balls behave and are fielded as well as on batting. Pitches vary in consistency, and thus in the amount of bounce, spin, and seam movement available to the bowler. Hard pitches are usually good to bat on because of high but even bounce. Dry pitches tend to deteriorate for batting as cracks often appear, and when this happens to the pitch, spinners can play a key role. Damp pitches, or pitches covered in grass (termed "green" pitches), allow good fast bowlers to extract extra bounce. Such pitches tend to offer help to fast bowlers throughout the match, but become better for batting as the game goes on. While players of other outdoor sports deal with similar variations of field surface and stadium covering, the size and shape of their fields are much more standardised. Other local factors, such as altitude and climate, can also significantly affect play. These physical variations create a distinctive set of playing conditions at each ground. A given ground may acquire a reputation as batsman friendly or bowler friendly if one or the other discipline notably benefits from its unique mix of elements. The absence of a standardised field affects not only how particular games play out, but the nature of team makeup and players' statistical records. Types of matches Cricket is a multi-faceted sport with multiple formats, varying playing standard and level of formality and the desired time that the match should last. A pertinent division in terms of professional cricket is between matches limited by time in which the teams have two innings apiece, and those limited by number of overs, in which they have a single innings each. The former, known as first-class cricket, has a duration of three to five days (there have been examples of "timeless" matches too); the latter, known as limited overs cricket because each team bowls a limit of typically 50 or 20 overs, has a planned duration of one day only (a match can be extended if necessary due to bad weather, etc.).Typically, two-innings matches have at least six hours of playing time each day. Limited overs matches often last six hours or more. There are usually formal intervals on each day for lunch and tea with brief informal breaks for drinks. There is also a short interval between innings.Amateur cricketers rarely play matches that last longer than a single day; these may loosely be divided into declaration matches, in which a specified maximum time or number of overs is assigned to the game in total and the teams swap roles only when the batting team is either completely dismissed or declares; and limited overs matches, in which a specified maximum number of overs is assigned for each team's innings individually. These will vary in length between 30 and 60 overs per side at the weekend and the ever-popular 20 over format during the evenings. Other forms of cricket, such as indoor cricket and garden cricket remain popular.Historically, a form of cricket known as single wicket had been extremely successful and many of these contests in the 18th and 19th centuries qualify as important matches. In this form, although each team may have from one to six players, there is only one batsman at a time and he must face every delivery bowled while his innings lasts. Single wicket has rarely been played since limited overs cricket began. Test cricket Test cricket is the highest standard of first-class cricket. A Test match is an international fixture between teams representing those countries that are Full Members of the ICC.Although the term "Test match" was not coined until much later, Test cricket is deemed to have begun with two matches between Australia and England in the 1876–77 Australian season. Subsequently, eight other national teams have achieved Test status: South Africa (1889), West Indies (1928), New Zealand (1929), India (1932), Pakistan (1952), Sri Lanka (1982), Zimbabwe (1992) and Bangladesh (2000). Zimbabwe suspended its Test status in 2006 due to its inability to compete against other Test teams, and returned in 2011.Welsh players are eligible to play for England, which is in effect an England and Wales team. The West Indies team comprises players from numerous states in the Caribbean, notably Barbados, Guyana, Jamaica, Trinidad & Tobago, the Leeward Islands and the Windward Islands.Test matches between two teams are usually played in a group of matches called a "series". Matches last up to five days and a series normally consists of three to five matches. Test matches that are not finished within the allotted time are drawn. In the case of Test and first-class cricket: the possibility of a draw often encourages a team that is batting last and well behind to bat defensively, giving up any faint chance at a win to avoid a loss.Since 1882, most Test series between England and Australia have been played for a trophy known as The Ashes. Some other bilateral series have individual trophies too: for example, the Wisden Trophy is contested by England and West Indies; the Frank Worrell Trophy by Australia and West Indies and the Border-Gavaskar Trophy between India and Australia. Limited overs Standard limited overs cricket was introduced in England in the 1963 season in the form of a knockout cup contested by the first-class county clubs. In 1969, a national league competition was established. The concept was gradually introduced to the other leading cricket countries and the first limited overs international was played in 1971. In 1975, the first Cricket World Cup took place in England. Limited overs cricket has seen various innovations including the use of multi-coloured kit and floodlit matches using a white ball. A "one day match", named so because each match is scheduled for completion in a single day, is the common form of limited overs cricket played on an international level. In practice, matches sometimes continue on a second day if they have been interrupted or postponed by bad weather. The main objective of a limited overs match is to produce a definite result and so a conventional draw is not possible, but matches can be undecided if the scores are tied or if bad weather prevents a result. Each team plays one innings only and faces a limited number of overs, usually a maximum of 50. The Cricket World Cup is held in one day format and the last World Cup in 2015 was won by the co-hosts, Australia. The next World Cup will be hosted by England and Wales in 2019.Twenty20 is a new variant of limited overs itself with the purpose being to complete the match within about three hours, usually in an evening session. The original idea, when the concept was introduced in England in 2003, was to provide workers with an evening entertainment. It was commercially successful and has been adopted internationally. The inaugural Twenty20 World Championship was held in 2007 and won by India. Subsequent events have been held which were won by Pakistan (2009), England (2010), West Indies (2012), and Sri Lanka (2014). The next tournament is scheduled to be held in 2016. After the inaugural ICC World Twenty20 many domestic Twenty20 leagues were born. First of them was Indian Cricket League which was a rebel league since it was not authorized by BCCI. BCCI then formed its official league called the Indian Premier League. The official league went on to become a successful annual affair that attracted players and audience around the globe, while the Indian Cricket League has been disbanded. After the success of Indian Premier League, many other domestic leagues were formed globally. Recently the Twenty20 Champions League was formed as a tournament for domestic clubs of various countries. In this league, competition is between the top domestic teams from the senior cricketing nations. National championships First-class cricket includes Test cricket but the term is generally used to refer to the highest level of domestic cricket in those countries with full ICC membership, although there are exceptions to this. First-class cricket in England is played for the most part by the 18 county clubs which contest the County Championship. The concept of a champion county has existed since the 18th century but the official competition was not established until 1890. The most successful club has been Yorkshire County Cricket Club with 30 official titles.Australia established its national first-class championship in 1892–93 when the Sheffield Shield was introduced. In Australia, the first-class teams represent the various states. New South Wales has won the maximum number of titles with 45 to 2008.National championship trophies to be established elsewhere included the Ranji Trophy (India), Plunket Shield (New Zealand), Currie Cup (South Africa) and Shell Shield (West Indies). Some of these competitions have been updated and renamed in recent years.Domestic limited overs competitions began with England's Gillette Cup knockout in 1963. Countries usually stage seasonal limited overs competitions in both knockout and league format. In recent years, national Twenty20 competitions have been introduced, usually in knockout form though some incorporate mini-leagues. Club cricket Club cricket is a mainly amateur, but still formal, form of the sport of cricket, usually involving teams playing in competitions at weekends or in the evening. There is a great deal of variation in game format although the Laws of Cricket are always observed.Club cricket is frequently organised in a league or cup format. Games are limited by either time or overs. Limited overs games usually last between 20 and 60 overs per innings. A less common, but more traditional, format is limiting the game by time only. Games can range from a few hours in the evening to two days long. A modern innovation is the introduction of Twenty20 competitions, both as a format in the existing leagues and new leagues solely based on Twenty20, such as LastManStanding.Standards of play can vary from semi-professional to occasional recreational level and club cricket is often enjoyed as much for the social element as for the competition. Most clubs have their own ground to play on regularly, often including a field and pavilion or club house. An exception being 'Wandering Sides' who use other's grounds.Many leagues have been formed around the world of varying degrees of professionalism, the oldest being the Birmingham & District Premier League in the Birmingham area of England, founded in 1888. Other types of matches There are numerous variations of the sport played throughout the world that include indoor cricket, French cricket, beach cricket, Kwik cricket and all sorts of card games and board games that have been inspired by cricket. In these variants, the rules are often changed to make the game playable with limited resources or to render it more convenient and enjoyable for the participants.Indoor Cricket was first invented in 1970. It is similar to outdoor cricket except that is played in an indoor sports hall with 6 players per team. It is extremely popular in the UK with national championships and multiple independent leagues. Another less formal version of indoor cricket is played in a smaller arena with a soft ball and without pads was invented some years later and is commonly played in the Southern Hemisphere, and even has its own nominal international championships, including World Cups.In the UK, garden cricket is a popular version of the sport, played in gardens and on recreation grounds around the country by adults and children alike. Although a cricket bat and ball are generally used, other equipment such as pads and gloves are not. The exact rules will vary based on the number of participants and the available space.Families and teenagers play backyard cricket or tennis ball cricket in suburban yards or driveways, and the cities of India and Pakistan play host to countless games of "Gully Cricket" or "tape ball" in their long narrow streets. Sometimes the rules are improvised: e.g. it may be agreed that fielders can catch the ball with one hand after one bounce and claim a wicket; or if only a few people are available then everyone may field while the players take it in turns to bat and bowl. Tennis balls and homemade bats are often used, and a variety of objects may serve as wickets: for example, the batter's legs as in French cricket, which did not in fact originate in France, and is usually played by small children.In Kwik cricket, the bowler does not have to wait for the batsman to be ready before a delivery, leading to a faster, more exhausting game designed to appeal to children, which is often used in physical education lessons at UK schools. Another modification to increase the pace of the game is the "Tip and Run", "Tipity" Run, "Tipsy Run" or "Tippy-Go" rule, in which the batter must run when the ball touches the bat, even if it the contact is unintentional or minor. This rule, seen only in impromptu games, speeds the match up by removing the batsman's right to block the ball.In Samoa a form of cricket called Kilikiti is played in which hockey stick-shaped bats are used. In original English cricket, the hockey stick shape was replaced by the modern straight bat in the 1760s after bowlers began to pitch the ball instead of rolling or skimming it. In Estonia, teams gather over the winter for the annual Ice Cricket tournament. The game juxtaposes the normal summer pursuit with harsh, wintry conditions. Rules are otherwise similar to those for the six-a-side game. International structure The International Cricket Council (ICC), which has its headquarters in Dubai, is the international governing body of cricket. It was founded as the Imperial Cricket Conference in 1909 by representatives from England, Australia and South Africa, renamed the International Cricket Conference in 1965, and took up its current name in 1989.The ICC has 104 members: 10 Full Members that play official Test matches, 34 Associate Members, and 60 Affiliate Members. The ICC is responsible for the organisation and governance of cricket's major international tournaments, notably the Cricket World Cup. It also appoints the umpires and referees that officiate at all sanctioned Test matches, One Day International and Twenty20 Internationals. Each nation has a national cricket board which regulates cricket matches played in its country. The cricket board also selects the national squad and organises home and away tours for the national team. In the West Indies these matters are addressed by the West Indies Cricket Board which consists of members appointed by four national boards and two multi-national boards. Members  Full Members Full Members are the governing bodies for cricket in a country or associated countries. Full Members may also represent a geographical area. All Full Members have a right to send one representative team to play official Test matches. Also, all Full Member nations are automatically qualified to play ODIs and Twenty20 Internationals. West Indies cricket team does not represent one country instead an amalgamation of over 20 countries from the Caribbean. The English Cricket team represents both England and Wales.*Last Updated: 24 March 2017 AResigned May 1961, readmitted 10 July 1991. Top Associate and Affiliate Members All the associate and affiliate members are not qualified to play Test Cricket, however ICC grants One Day International status to its associate and affiliate members based on their success in the World Cricket League. The top six teams will be awarded One day international and Twenty20 International status, which will allow the associate and affiliate teams to be eligible to play the full members and play official ODI cricket.The associate and affiliate teams who currently hold ODI and T20I status: Cricket at international multi-sport events Cricket was played as part of the 1900 Summer Olympics, when England and France contested a two-day match. In 1998, cricket was played as part of the Commonwealth Games, on this occasion in the 50-over format. Twenty20 cricket was under consideration to be part of the 2010 Commonwealth Games, which were held in Delhi, but at the time the Board of Control for Cricket in India (BCCI), were not in favour of the short format of the game, and it was not included.Cricket was played in 2010 Asian Games in Guangzhou, China and 2014 Asian Games in Incheon, South Korea. India skipped both times. There was further calls for subsequent Commonwealth Games and Olympic Games. The Commonwealth Games Federation asked the ICC to participate in the 2014 and 2018 Commonwealth Games, but the ICC turned down the invitation. In 2010, the International Olympic Committee recognised cricket as a sport which could apply to be included in the Olympic Games, but in 2013 the ICC announced that it had no intentions to make such an application, primarily due to opposition from the BCCI. ESPNcricinfo suggested that the opposition might be based on the possible loss of income. In April 2016, ICC chief executive Dave Richardson said that Twenty20 cricket can have a chance of getting in for the 2024 Summer Games, but there must be collective support shown by the ICC's membership base, in particular from BCCI, in order for there to be a chance of inclusion. Statistics Organized cricket lends itself to statistics to a greater degree than many other sports. Each play is discrete and has a relatively small number of possible outcomes. At the professional level, statistics for Test cricket, one-day internationals, and first-class cricket are recorded separately. However, since Test matches are a form of first-class cricket, a player's first-class statistics will include his Test match statistics – but not vice versa. The Guide to Cricketers was a cricket annual edited by Fred Lillywhite between 1849 and his death in 1866. Wisden Cricketers' Almanack was founded in 1864 by the English cricketer John Wisden (1826–1884) as a competitor to The Guide to Cricketers. Its annual publication has continued uninterrupted to the present day, making it the longest running sports annual in history.Certain traditional statistics are familiar to most cricket fans. The basic batting statistics include:Innings (I): The number of innings in which the batsman actually batted.Not outs (NO): The number of times the batsman was not out at the conclusion of an innings they batted in.Runs (R): The number of runs scored.Highest score (HS/Best): The highest score ever made by the batsman.Batting average (Ave): The total number of runs divided by the total number of innings in which the batsman was out. Ave  Runs/[I – NO] (also Avge or Avg.)Centuries (100): The number of innings in which the batsman scored one hundred runs or more.Half-centuries (50): The number of innings in which the batsman scored fifty to ninety-nine runs (centuries do not count as half-centuries as well).Balls faced (BF): The total number of balls received, including no balls but not including wides.Strike rate (SR): The number of runs scored per 100 balls faced. (SR  [100 * Runs]/BF)Run rate (RR): Is the number of runs a batsman (or the batting side) scores in an over of six balls.The basic bowling statistics include:Overs (O): The number of overs bowled.Balls (B): The number of balls bowled. Overs is more traditional, but balls is a more useful statistic because the number of balls per over has varied historically.Maiden overs (M): The number of maiden overs (overs in which the bowler conceded zero runs) bowled.Runs (R): The number of runs conceded.Wickets (W): The number of wickets taken.No balls (Nb): The number of no balls bowled.Wides (Wd): The number of wides bowled.Bowling average (Ave): The average number of runs conceded per wicket. (Ave  Runs/W)Strike rate (SR): The average number of balls bowled per wicket. (SR  Balls/W)Economy rate (Econ): The average number of runs conceded per over. (Econ  Runs/overs bowled). Scorecards A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks. The earliest known scorecards were printed in 1776 by Pratt, scorer to the Sevenoaks Vine Cricket Club, but it was many years before his invention was widely adopted. Scorecards were printed and sold at Lord's for the first time in 1846.The introduction of scoreboards revolutionised cricket by allowing spectators to keep track of the day's play. In 1848, Fred Lillywhite used a portable printing press at grounds to print updated scorecards. In 1858, the Kennington Oval introduced the first mobile scorebox, "a house on rollers with figures for telegraphing on each side". In 1881, the Melbourne Cricket Ground erected the first cricket scoreboard. The scoreboard, located at the western end of the ground, gave the batsman's name and method of dismissal. Culture  Influence on everyday life Cricket has had a broad impact on popular culture, both in the Commonwealth of Nations and elsewhere. Cricket has had an influence on the lexicon of these nations, especially the English language, with such phrases as "that's not cricket" (unfair), "had a good innings", "sticky wicket", and "bowled over". There have been many cricket films. The term "Bradmanesque" from Don Bradman's name has become a generic term for outstanding excellence, both within cricket and in the wider world.The amateur game has also been spread further afield by expatriates from the Test-playing nations. In the arts and popular culture Cricket is the subject of works by noted English poets, including William Blake and Lord Byron. Beyond a Boundary (1963), written by Trinidadian C.L.R. James, is often named the best book on any sport, ever written. In fiction, there is English author P. G. Wodehouse's 1909 novel, Mike.In the visual arts, notable cricket paintings include Albert Chevallier Tayler's Kent vs Lancashire at Canterbury (1907) and Russell Drysdale's The Cricketers (1948), which has been called "possibly the most famous Australian painting of the 20th century." French impressionist Camille Pissarro painted cricket on a visit to England in the 1890s. Francis Bacon, an avid cricket fan, captured a batsman in motion. Caribbean artist Wendy Nanan's cricket images are featured in a limited edition first day cover for Royal Mail's "World of Invention" stamp issue, celebrating the London Cricket Conference 1–3 March 2007, first international workshop of its kind and part of the celebrations leading up to the 2007 Cricket World Cup.There are many cricket video games, including EA Sports Cricket 07. Influence on other sports Cricket has close historical ties with Australian rules football and many players have competed at top levels in both sports. In 1858, prominent Australian cricketer Tom Wills called for the formation of a "foot-ball club" with "a code of laws" to keep cricketers fit during the off-season. The Melbourne Football Club was founded the following year, and Wills and three other members codified the first laws of the game. It is typically played on modified cricket fields.In the late 19th century, a former cricket player, English-born Henry Chadwick of Brooklyn, New York, was responsible for the "development of the box score, tabular standings, the annual baseball guide, the batting average, and most of the common statistics and tables used to describe baseball". The statistical record is so central to the game's "historical essence" that Chadwick came to be known as Father of Baseball, and was an early inductee in the Baseball Hall of Fame. See also  References  Bibliography Altham, H. S. (1962). A History of Cricket, Volume 1 (to 1914). George Allen & Unwin. Birley, Derek (1999). A Social History of English Cricket. Aurum. ISBN 1-85410-710-0. Bowen, Rowland (1970). Cricket: A History of its Growth and Development. Eyre & Spottiswoode. Major, John (2007). More Than A Game. HarperCollins. McCann, Tim (2004). Sussex Cricket in the Eighteenth Century. Sussex Record Society. Underdown, David (2000). Start of Play. Allen Lane.  External links International Cricket Council (ICC)"Cricket". Encyclopædia Britannica Online.Cricket at DMOZ
Parachuting, or skydiving, is a method of transiting from a high point to Earth with the aid of gravity, involving the control of speed during the descent with the use of a parachute. It may involve more or less free-fall, a time during which the parachute has not been deployed and the body gradually accelerates to terminal velocity.The first parachute jump in history was made by Andre-Jacques Garnerin, the inventor of the parachute, on October 22, 1797. Garnerin tested his contraption by leaping from a hydrogen balloon 3,200 feet above Paris. Garnerin's parachute bore little resemblance to today's parachute's however as it was not packed into any sort of container and did not feature a ripcord. The first intentional freefall jump with a ripcord-operated deployment was not made until over a century later by Leslie Irvin in 1919. While Georgia Broadwick made an earlier freefall in 1914 when her static line became entangled with her jump aircraft's tail assembly, her freefall descent was not planned. Broadwick cut her static line and deployed her parachute manually, only as a means of freeing her self from the aircraft to which she had become entangled.The military developed parachuting technology as a way to save aircrews from emergencies aboard balloons and aircraft in flight, and later as a way of delivering soldiers to the battlefield. Early competitions date back to the 1930s, and it became an international sport in 1952. Common uses Parachuting is performed as recreational activity, and a competitive sport which is widely considered an extreme sport due to the risks involved. Modern militaries utilize parachuting for the deployment of airborne forces and supplies, and special operations forces commonly employ parachuting, especially free-fall parachuting, as a method of insertion. Occasionally forest firefighters, known as "smokejumpers" in the United States, use parachuting as a means of rapidly inserting themselves near forest fires in especially remote or otherwise inaccessible areas.Manually exiting an aircraft and parachuting to safety has been widely used by aviators (especially military aviators and aircrew), and passengers to escape an aircraft that could not otherwise land safely. While this method of escape is relatively rare in modern times, it was commonly used in World War I by military aviators, and utilized extensively throughout the air wars of World War II. In modern times, the most common means of escape from an aircraft in distress is via an ejection seat. Said system is usually operated by the pilot, aircrew member, or passenger, by engaging an activation device manually. In most designs, this will lead to the seat being propelled out of and away from the aircraft carrying the occupant with it, by means of either an explosive charge or a rocket propulsion system. Once clear of the aircraft, the ejection seat will deploy a parachute, although some older models entrusted this step to manual activation by the seat's occupant. Safety Despite the perception of danger, fatalities are rare. About 21 skydivers are confirmed killed each year in the US, roughly one death for every 150,000 jumps (about 0.0007%).In the US and in most of the western world, skydivers are required to carry two parachutes. The reserve parachute must be periodically inspected and re-packed (whether used or not) by a certified parachute rigger (in the US, an FAA certificated parachute rigger). Many skydivers use an automatic activation device (AAD) that opens the reserve parachute at a pre-determined altitude if it detects that the skydiver is still in free fall. Depending on the country, AADs are often mandatory for new jumpers, and/or required for all jumpers regardless of their experience level. Most skydivers wear a visual altimeter, and an increasing number also use audible altimeters fitted to their helmets.Injuries and fatalities occurring under a fully functional parachute usually happen because the skydiver performed unsafe maneuvers or made an error in judgment while flying their canopy, typically resulting in a high-speed impact with the ground or other hazards on the ground. One of the most common sources of injury is a low turn under a high-performance canopy and while swooping. Swooping is the advanced discipline of gliding at high-speed parallel to the ground during landing.Changing wind conditions are another risk factor. In conditions of strong winds, and turbulence during hot days the parachutist can be caught in downdrafts close to the ground. Shifting winds can cause a crosswind or downwind landing which have a higher potential for injury due to the wind speed adding to the landing speed.Another risk factor is that of "canopy collisions", or collisions between two or more skydivers under fully inflated parachutes. Canopy collisions can cause the jumpers' inflated parachutes to entangle with each other, often resulting in a sudden collapse (deflation) of one or more of the involved parachutes. When this occurs, the jumpers often must quickly perform emergency procedures (if there is sufficient altitude to do so) to "cut-away" (jettison) from their main canopies and deploy their reserve canopies. Canopy collisions are particularly dangerous when occurring at altitudes too low to allow the jumpers adequate time to safely jettison their main parachutes and fully deploy their reserve parachutes.Equipment failure rarely causes fatalities and injuries. Approximately one in 750 deployments of a main parachute result in a malfunction. Ram-air parachutes typically spin uncontrollably when malfunctioning, and must be jettisoned before deploying the reserve parachute. Reserve parachutes are packed and deployed differently; they are also designed more conservatively and built and tested to more exacting standards so they are more reliable than main parachutes, but the real safety advantage comes from the probability of an unlikely main malfunction multiplied by the even less likely probability of a reserve malfunction. This yields an even smaller probability of a double malfunction although the possibility of a main malfunction that cannot be cutaway causing a reserve malfunction is a very real risk.Parachuting disciplines such as BASE jumping or those that involve equipment such as wing suit flying and sky surfing have a higher risk factor due to the lower mobility of the jumper and the greater risk of entanglement. For this reason, these disciplines are generally practiced by experienced jumpers.Depictions in commercial films – notably Hollywood action movies – usually overstate the dangers of the sport. Often, the characters in such films are shown performing feats that are physically impossible without special effects assistance. In other cases, their practices would cause them to be grounded or shunned at any safety-conscious drop zone or club. USPA member drop zones in the US and Canada are required to have an experienced jumper act as a "safety officer" (in Canada DSO – Drop Zone Safety Officer; in the U.S. S&TA – Safety and Training Advisor) who is responsible for dealing with jumpers who violate rules, regulations, or otherwise act in a fashion deemed unsafe by the appointed individual.In many countries, either the local regulations or the liability-conscious prudence of the drop zone owners require that parachutists must have attained the age of majority before engaging in the sport.The first skydive performed without a parachute was by stuntman Gary Connery on 23 May 2012 at 732 m. Most common injuries Due to the hazardous nature of skydiving, the greatest of precautions are taken to avoid parachuting injuries and death. For first time solo-parachutists, this includes anywhere from 4 to 8 hours of ground instruction. Since the majority of parachute injuries occur upon landing (approximately 85%), the greatest emphasis within ground training is usually on the proper parachute landing fall (PLF), which seeks to orient the body as to evenly disperse the impact through flexion of several large, insulating muscles (such as the medial gastrocnemius, tibialis anterior, rectus femoris, vastus medialis, biceps femoris, and semitendinosus ), as opposed to individual bones, tendons, and ligaments which break and tear more easily.Parachutists, especially those flying smaller sport canopies, often land with dangerous amounts of kinetic energy, and for this reason, improper landings are the cause of more than 30% of all skydiving related injuries and deaths. Often, injuries sustained during parachute landing are caused when a single outstretched limb, such as a hand or foot, is extended separately from the rest of the body, causing it to sustain forces disproportional to the support structures within. This tendency is displayed in the following chart, which shows the significantly higher proportion of wrist and ankle injuries among the 186 injured in a 110,000 parachute jump study:Due to the possibility of fractures (commonly occurring on the tibia and the ankle mortise), it is recommended that parachutists wear supportive footwear. Supportive footwear prevents inward and outward ankle rolling, allowing the PLF to safely transfer impact energy through the true ankle joint, and dissipate it via the medial gastrocnemius and tibialis anterior muscles. Weather Parachuting in poor weather, especially with thunderstorms, high winds, and dust devils can be a dangerous activity. Reputable drop zones will suspend normal operations during inclement weather. In the United States, the USPA's Basic Safety Requirements prohibit solo student skydivers from jumping in winds exceeding 14 mph while using ram-air equipment. However, maximum ground winds are unlimited for licensed skydivers. Visibility As parachuting is an aviation activity under the visual flight rules, it is generally illegal to jump in or through clouds, according to the relevant rules governing the airspace, such as FAR105 in the US or Faldskærmsbestemmelser (Parachuting Ordinances) in Denmark. Jumpers and pilots of the dropping aircraft similarly bear responsibility of following the other VFR elements, in particular ensuring that the air traffic at the moment of jump does not create a hazard. Canopy collisions A collision with another canopy is a statistical hazard, and may be avoided by observing simple principles, including knowing upper wind speeds, the number of party members and exit groups, and having sufficient exit separation between jumpers. In 2013, 17% of all skydiving fatalities in the United States resulted from mid-air collisions. Training Skydiving can be practised without jumping. Vertical wind tunnels are used to practise for free fall ("indoor skydiving" or "bodyflight"), while virtual reality parachute simulators are used to practise parachute control.Beginning skydivers seeking training have the following options:Static lineInstructor-assisted deploymentAccelerated freefallTandem skydiving Parachute deployment At a sport skydiver's deployment altitude, the individual manually deploys a small pilot-chute which acts as a drogue, catching air and pulling out the main parachute or the main canopy. There are two principal systems in use: the "throw-out", where the skydiver pulls a toggle attached to the top of the pilot-chute stowed in a small pocket outside the main container: and the "pull-out", where the skydiver pulls a small pad attached to the pilot-chute which is stowed inside the container.Throw-out pilot-chute pouches are usually positioned at the bottom of the container – the B.O.C. deployment system – but older harnesses often have leg-mounted pouches. The latter are safe for flat-flying, but often unsuitable for freestyle or head-down flying.In a typical civilian sport parachute system, the pilot-chute is connected to a line known as the "bridle", which in turn is attached to a small deployment bag that contains the folded parachute and the canopy suspension lines, which are stowed with rubber bands. At the bottom of the container that holds the deployment bag is a closing loop which, during packing, is fed through the grommets of the four flaps that are used to close the container. At that point, a curved pin that is attached to the bridle is inserted through the closing loop. The next step involves folding the pilot-chute and placing it in a pouch (e.g., B.O.C pouch).Activation begins when the pilot-chute is thrown out. It inflates and creates drag, pulling the pin out of the closing loop and allowing the pilot-chute to pull the deployment bag from the container. The parachute lines are pulled loose from the rubber bands and extend as the canopy starts to open. A rectangular piece of fabric called the "slider" (which separates the parachute lines into four main groups fed through grommets in the four respective corners of the slider) slows the opening of the parachute and works its way down until the canopy is fully open and the slider is just above the head of the skydiver. The slider slows and controls the deployment of the parachute. Without a slider, the parachute would inflate fast, potentially damaging the parachute fabric and/or suspension lines, as well as causing discomfort, injury or even death of the jumper. During a normal deployment, a skydiver will generally experience a few seconds of intense deceleration, in the realm of 3 to 4 g, while the parachute slows the descent from 190 km/h (120 mph) to approximately 28 km/h (17 mph).If a skydiver experiences a malfunction of their main parachute which they cannot correct, they pull a "cut-away" handle on the front right-hand side of their harness (on the chest) which will release the main canopy from the harness/container. Once free from the malfunctioning main canopy, the reserve canopy can be activated manually by pulling a second handle on the front left harness. Some containers are fitted with a connecting line from the main to reserve parachutes – known as a reserve static line (RSL) – which pulls open the reserve container faster than a manual release could. Whichever method is used, a spring-loaded pilot-chute then extracts the reserve parachute from the upper half of the container. Disciplines and maneuvers  Accuracy One example of this is "Hit and Rock", a variant of accuracy landing devised to let people of varying skill levels compete for fun. "Hit and Rock" is originally from POPS (Parachutists Over Phorty Society). The object is to land as close as possible to the chair, remove the parachute harness, sprint to the chair, sit fully in the chair and rock back and forth at least one time. The contestant is timed from the moment that feet touch the ground until that first rock is completed. This event is considered a race. Angle Flying Angle Flying was presented for the first time in 2000 at the World Freestyle Competitions, the European Espace Boogie, and the Eloy Freefly Festival.The technique consists of flying diagonally with a determinate relation between angle and trajectory speed of the body, to obtain an air stream that allows for control of flight. The aim is to fly in formation at the same level and angle, and to be able to perform different aerial games, such as freestyle, three-dimensional flight formation with grip, or acrobatic free-flying. Base Jumping  Camera flying  Classic accuracy Classic accuracy is running with opened parachute, in individual or team contest. The aim is to touch down on a target whose center is 2 cm in diameter. The target can be a deep foam mattress or an air-filled landing pad. An electronic recording pad of 32 cm in diameter is set in the middle. It measures score in 1 cm increments up to 16 cm and displays result just after landing.The first part of any competition take place over 8 rounds. Then in the individual competition, after this 8 selective rounds, the top 25% jump a semi-final round. After semi-final round, the top 50% are selected for the final round. The competitor with the lowest cumulative score is declared the winner.Competitors jump in teams of 5 maximum, exiting the aircraft at 1000 or 1200 meters and opening their parachutes sequentially to allow each competitor a clear approach to the target.This sport is unpredictable because weather conditions play a very important part. So classic accuracy requires high adaptability to aerology and excellent steering control.It is also the most interesting discipline for spectator due to the closeness of action (a few metres) and the possibility to be practised everywhere (sport ground, stadium, urban place...). Today, classic accuracy is the most practiced (in competition) discipline of skydiving in the world. Cross-country A cross-country jump is a skydive where the participants open their parachutes immediately after jumping, with the intention of covering as much ground under canopy as possible. Usual distance from jump run to the dropzone can be as much as several miles.There are two variations of a cross-country jump:The more popular one is to plan the exit point upwind of the drop zone. A map and information about the wind direction and velocity at different altitudes are used to determine the exit point. This is usually set at a distance from where all the participants should be able to fly back to the drop zone.The other variation is to jump out directly above the drop zone and fly down wind as far as possible. This increases the risks of the jump substantially, as the participants must be able to find a suitable landing area before they run out of altitude.Two-way radios and cell-phones are often used to make sure everyone has landed safely, and, in case of a landing off the drop zone, to find out where the parachutist is so that ground crew can pick them up. Formation skydiving Formation Skydiving (FS) was born in California, USA during the 1960‘s. The first documented skydiving formation occurred over Arvin, California in March 1964 when Mitch Poteet, Don Henderson, Andy Keech and Lou Paproski successfully formed a 4-man star formation, photographed by Bob Buquor. This discipline was formerly referred to in the skydiving community as Relative Work, often abbreviated to RW, Relly or Rel. Freeflying  Night jumps Parachuting is not always restricted to daytime hours; experienced skydivers sometimes perform night jumps. For safety reasons, this requires more equipment than a usual daytime jump and in most jurisdictions, it requires both an advanced skydiving license (at least a B-License in the U.S.) and a meeting with the local safety official covering who will be doing what on the load. A lit altimeter (preferably accompanied with an audible altimeter) is a must. Skydivers performing night jumps often take flashlights up with them so that they can check their canopies have properly deployed.Visibility to other skydivers and other aircraft is also a consideration; FAA regulations require skydivers jumping at night to be wearing a light visible for three miles (5 km) in every direction, and to turn it on once they are under canopy. A chem-light(glowstick) is a good idea on a night jump.Night jumpers should be made aware of the dark zone, when landing at night. Above 30 meters (100 feet) jumpers flying their canopy have a good view of the landing zone normally because of reflected ambient light/moon light. Once they get close to the ground, this ambient light source is lost, because of the low angle of reflection. The lower they get, the darker the ground looks. At about 100 feet and below it may seem that they are landing in a black hole. Suddenly it becomes very dark, and the jumper hits the ground soon after. This ground rush should be explained to, and anticipated by, the first time night jumper. Recommendations should be made to the jumper to utilize a canopy that is larger than they typically use on a day jump and to attempt to schedule their first night jump as close to a full moon as possible to make it easier to see the ground.While more dangerous than regular skydiving and more difficult to schedule, two night jumps are required by the USPA for a jumper to obtain their D license. Pond swooping Pond swooping is a form of competitive parachuting wherein canopy pilots attempt to touch down and glide across a small body of water, and onto the shore. Events provide lighthearted competition, rating accuracy, speed, distance and style. Points and peer approval are reduced when a participant "chows", or fails to reach shore and sinks into the water. Swoop ponds are not deep enough to drown in under ordinary circumstances, their main danger being from the concussive force of an incorrectly executed maneuver. In order to gain distance, swoopers increase their speed by executing a "hook turn", wherein both speed and difficulty increase with the angle of the turn. Hook turns are most commonly measured in increments of 90 degrees. As the angle of the turn increases, both horizontal and vertical speed are increased, such that a misjudgement of altitude or imprecise manipulation of the canopy's control structures (front risers, rear risers, and toggles) can lead to a high speed impact with the pond or Earth. Prevention of injury is the main reason why a pond is used for swooping rather than a grass landing area. Skysurfing  Space Ball This is when skydivers have a ball which weighs 100-200 grams and release it in free fall. The ball maintains the same fall rate as the skydivers. The skydivers can pass the ball around to each other whilst in free fall. At a predetermined altitude, the "ball master" will catch the ball and hold on to it to ensure it does not impact the ground. Style Style can be considered as sprint of parachuting. This individual discipline is played in free fall.The idea is to take maximum speed and complete a pre-designated series of maneuvers as fast and cleanly as possible (speed can exceed 400 km/h / 250 mph)Jumps are filmed using a ground based camera (with an exceptional lens to record the performance).Performance is timed (from the start of the maneuver until its completion) and then judged in public at the end of the jump. Competition includes 4 qualifying rounds and a final for the top 8. Competitors jump from a height of 2200 m to 2500 m.They rush into an acceleration stage for 15 to 20 seconds and then run their series of maneuvers benefiting to the maximum of the stored speed.Those series consist of Turns and Back-Loops to achieve in a pre-designated order. The incorrect performance of the maneuvers gives rise to penalties that are added at runtime.The performance of the athlete is defined in seconds and hundredths of a second. The competitor with the lowest cumulative time is declared the winner.Notice the complete sequence is performed by leading international experts in just over 6 seconds, penalties included. Stuff jumps Thanks to large unpopulated areas to jump over, 'stuff' jumps become possible. Also known as "zoo jumps", in these jumps the skydivers jump out with some object. Rubber raft jumps are popular; where the jumpers sit in a rubber raft. Cars, bicycles, motorcycles, vacuum cleaners, water tanks, and inflatable companions have also been thrown out the back of an aircraft. At a certain altitude, the jumpers break off from the object and deploy their parachutes, leaving it to smash into the ground at terminal velocity. Swoop and chug A tradition at many drop zones is the swoop and chug. As parachutists land from the last load of the day, other skydivers often hand the landing skydivers a beer that is customarily chugged in the landing area. This is sometimes timed as a friendly competition but is usually an informal, untimed, kick-off for the night's festivities. Tracking Tracking is where skydivers take a body position to achieve a high forward speed, allowing them to cover a great distance over the ground. Tracking is also used at the end of group jumps to achieve separation from other jumpers before parachute deployment. The tracking position involves sweeping the arms out to the side of the body and straightening the legs with the toes pointed. Tunnel flying Using a vertical wind tunnel to simulate free fall has become a discipline of its own and is not only used for training but has its own competitions, teams, and figures. Wingsuit flying 'Wingsuit flying' or 'wingsuiting' is the sport of flying through the air using a wingsuit, which adds surface area to the human body to enable a significant increase in lift. The common type of wingsuit creates an extra surface area with fabric between the legs and under the arms. Organizations National parachuting associations exist in many countries, many affiliated with the Fédération Aéronautique Internationale (FAI), to promote their sport. In most cases, national representative bodies, as well as local drop zone operators, require that participants carry certification attesting to their training their level of experience in the sport, and their proven competence. Anyone who cannot produce such bona-fides is treated as a student, requiring close supervision.The sole organization in the United States is the United States Parachute Association (USPA), which issues licenses and ratings, governs skydiving, and represents skydiving to government agencies. USPA publishes the Skydivers Information Manual (SIM) and many other resources. In Canada, the Canadian Sport Parachuting Association is the lead organization. In South Africa, the sport is managed by the Parachute Association of South Africa, and in the United Kingdom by the British Parachute Association. In Brazil the CNP (Comapnhia Nacional de Paraquedismo) sets in Boituva, where many records have been broken and where it is known for being the 2nd largest center in the world and the largest in the Southern Hemisphere.Within the sport, associations promote safety, technical advances, training-and-certification, competition and other interests of their members. Outside their respective communities, they promote their sport to the public and often intercede with government regulators.Competitions are organized at regional, national and international levels in most these disciplines. Some of them offer amateur competition.Many of the more photogenic/videogenic variants also enjoy sponsored events with prize money for the winners.The majority of jumpers tend to be non-competitive, enjoying the opportunity to skydive with their friends on weekends and holidays. The atmosphere of their gatherings is relaxed, sociable and welcoming to newcomers. Skydiving events, called "boogies", are arranged at local, national and international scale each year, which attracting both young jumpers and their elders – Parachutists Over Phorty (POPs), Skydivers Over Sixty (SOS) and even older groups. Drop zones In parachuting, a drop zone or DZ is the area above and around a location where a parachutist freefalls and expects to land. It is usually situated beside a small airport, often sharing the facility with other general aviation activities. There is generally a landing area designated for parachute landings. Drop zone staff includes the DZO (drop zone operator or owner), manifestors, pilots, instructors, coaches, cameramen, packers, riggers and other general staff. Equipment Costs in the sport are not trivial. As new technological advances or performance enhancements are introduced, equipment tends to increase in price. Similarly, the average skydiver carries more equipment than in earlier years, with safety devices (such as an AAD) contributing to a significant portion of the cost.A full set of brand-new equipment can easily cost as much as a new motorcycle or half a small car. The market is not large enough to permit the steady lowering of prices that is seen with some other equipment like computers.A new main canopy for the experienced parachutist can cost between $2,000 and $3,000 US  Higher performance and Tandem Parachutes cost significantly more, whilst large docile student parachutes often cost less.In many countries, the sport supports a used-equipment market. For beginners, that is the preferred way to acquire "gear", and has two advantages because users can:Try types of parachutes (there are many) to learn which style they prefer, before paying the price for new equipment.Acquire a complete system and all the peripheral items in a short time and at a reduced cost.Novices generally start with parachutes that are large and docile relative to the jumper's body weight. As they improve in skill and confidence, they can graduate to smaller, faster, more responsive parachutes. An active jumper might change parachutes several times in the space of a few years while retaining his or her first harness/container and peripheral equipment.Older jumpers, especially those who jump only on weekends in summer, sometimes tend in the other direction, selecting slightly larger, more gentle parachutes that do not demand youthful intensity and reflexes on each jump. They may be adhering to the maxim that: "There are old jumpers and there are bold jumpers, but there are no old, bold jumpers." (Pilots have much the same saying.)Most parachuting equipment is ruggedly designed and is enjoyed by several owners before being retired. Purchasers are always advised to have any potential purchases examined by a qualified parachute rigger. A rigger is trained to spot signs of damage or misuse. Riggers also keep track of industry product and safety bulletins, and can, therefore, determine if a piece of equipment is up-to-date and serviceable. Records On 24 October 2014, Alan Eustace achieved the highest parachute jump in history, jumping from 135,890 feet(41,422 m) and drogue-falling for 4 and a half minutes  The previous height record was set on 14 October 2012 by Felix Baumgartner who still holds records for the longest and fastest free-fall by breaking the speed of sound achieving Mach 1.25 jumping from 127,852 feet (38,970 m) as part of the Red Bull Stratos project. U.S. Air Force Captain Joe W. Kittinger, the 4th highest jumper (102,800 feet (31,330 m), 16 August 1960), served as mission control for Baumgartner.World's record for the most tandem parachute jumps in a 24-hour period is 103. This record was set in 2009 by Chip Bowlin and Kristine Gould.World's largest formation in free-fall: 8 February 2006 in Udon Thani, Thailand (400 linked persons in freefall).World's largest female-only formation: Jump for the Cause, 181 women from 26 countries who jumped from nine planes at 17,000 feet (5150 meters), in 2009.World's largest head down formation (vertical formation): 31 July 2015 at Skydive Chicago in Ottawa, Illinois, U.S. (164 linked skydivers in head to Earth attitude):Largest female head down formation (vertical formation): 30 November 2013 at Skydive Arizona in Eloy, Arizona, U.S. (63 linked skydivers in head to Earth attitude).European record: 13 August 2010, Włocławek, Poland. Polish skydivers broke a record when 102 people created a formation in the air during the Big Way Camp Euro 2010. The skydive was their fifteenth attempt at breaking the record.World's largest canopy formation: 100, set on 21 November 2007 in Lake Wales, Florida, U.S.Largest wingsuit formation: 22 September 2012, Perris Valley, California, U.S. (100 wingsuit jumpers).In 1929, U.S. Army Sergeant R. W. Bottriell held the world's record for most parachute jumps with 500. At that number, Bottriell stopped parachuting and became a ground instructor.Australian stunt parachutist, Captain Vincent Taylor, received the unofficial record for a lowest-level jump in 1929 when he jumped off a bridge over the San Francisco Bay whose center section had been raised to 135 feet (41 meters).Don Kellner holds the record for the most parachute jumps, with a total of over 42,000 jumps.Cheryl Stearns (U.S.) holds the record for the most parachute descents by a woman, with a total of 20,000 in August 2014, as well as the most parachute jumps made in a 24-hour period by a woman—352 jumps from 8–9 November 1995.Erin Hogan became the world's youngest sky diver as of 2002, when she tandem jumped at age 5.Bill Dause holds the record for the most accumulated freefall time with over 420 hours (30,000+ jumps).Jay Stokes holds the record for most parachute descents in a single day at 640.The Oldest Male Tandem skydiver is Armand Gendreau, born 24 June 1913. He made a tandem parachute jump above Notre-Dame-de-Lourdes, Québec, Canada, on 27 June 2014 at the age of 101 years 3 days.The Oldest Female Tandem skydiver is Estrid Geertsen, born 1 August 1904. She made a tandem parachute jump on 30 September 2004 from an altitude of 4,000 m (13,100 ft) over Roskilde, Denmark, at the age of 100 years 60 days.On 14 October 2012, after seven years of planning, the Red Bull Stratos mission came into dramatic climax. At 9.28 a.m. local time (3:28 p.m. GMT), Felix Baumgartner (Austria) lifted off from Roswell, New Mexico, USA. Destination: the edge of space. Within 3 hours, Felix would be back on earth after achieving the highest jump altitude, the highest freefall and the highest speed in freefall. He also became the first skydiver to break the sound barrier.The Oldest Solo United States skydiver is Milburn Hart who is 96 years old from Seattle, Washington. He made the solo jump in February 2005.On 11 April 2016 Verdun Hayes, aged 100, from Somerton, Somerset, became the oldest ever UK sky diver.Largest all-blind skydiving formation: 2, with Dan Rossi and John "BJ" Fleming on 13 September 2003.The oldest civilian parachute club in the WORLD is The Irish Parachute Club, founded in 1956 by Freddie Bond, located in Clonbullogue, Co. Offaly, Ireland.The oldest civilian parachute club in the USA is The Peninsula Skydivers Skydiving Club, founded in 1962 by Hugh Bacon Bergeron, located in West Point, VA, See also Banzai skydivingDolly ShepherdParachute landing fallParatrooperSpace divingSpeed skydivingSpace GamesThe First School of Modern SkyFlying Notes Malone, Jo (June, 2000). Birth of Freefly. Skydive the Mag. External links 
A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld computing devices. Specialized video games such as arcade games, in which the video game components are housed in a large, typically coin-operated chassis, while common in the 1980s in video arcades, have gradually declined due to the widespread availability of affordable home video game consoles (e.g., PlayStation 4, Xbox One and Nintendo Wii U) and video games on desktop and laptop computers and smartphones.The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, and buttons, or even, with the Kinect sensor, a person's hands and body. Players typically view the game on a video screen or television or computer monitor, or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and, in the 2010s, voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets. In the 2010s, the video game industry is of increasing commercial importance, with growth driven particularly by the emerging Asian markets and mobile games, which are played on smartphones. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV. History Early games used interactive electronic devices with various display formats. The earliest example is from 1947—a "Cathode ray tube Amusement Device" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen. Other early examples include: The Nimrod computer at the 1951 Festival of Britain; OXO a tic-tac-toe Computer game by Alexander S. Douglas for the EDSAC in 1952; Tennis for Two, an electronic interactive game engineered by William Higinbotham in 1958; Spacewar!, written by MIT students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961; and the hit ping pong-style Pong, a 1972 game by Atari. Each game used different means of display: NIMROD used a panel of lights to play the game of Nim, OXO used a graphical display to play tic-tac-toe Tennis for Two used an oscilloscope to display a side view of a tennis court, and Spacewar! used the DEC PDP-1's vector display to have two spaceships battle each other.In 1971, Computer Space, created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips. The game was featured in the 1973 science fiction film Soylent Green. Computer Space was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television. These were followed by two versions of Atari's Pong; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity. The commercial success of Pong led numerous other companies to develop Pong clones and their own systems, spawning the video game industry.A flood of Pong clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game Space Invaders, marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market. The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores. The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby. Space Invaders was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales. This helped Atari recover from their earlier losses, and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System, which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles. Overview  Platforms The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate. The term "system" is also commonly used. The distinctions below are not always clear and there may be games that bridge one or more platforms. In addition to personal computers, there are other devices which have the ability to play games but are not dedicated video game machines, such as smartphones, PDAs and graphing calculators. PC In common use a "PC game" refers to a form of media that involves a player interacting with a personal computer connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game in different hardwares, also the openness allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of modifications ("mods"), open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others. Console A "console game" is played on a specialized electronic device that connects to a common television set or composite video monitor, unlike PCs, which can run all sorts of computer programs, a console is a dedicated video game platform manufactured by a specific company. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include XBox and Playstation. Handheld A "handheld" gaming device is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardwares. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games. Arcade "Arcade game" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them. Web browser The web browser has also established itself as platform in its own right in the 2000s, while providing a cross-platform environment for video games designed to be played on a wide spectrum of hardware from personal computers and tablet computers to smartphones. This in turn has generated new terms to qualify classes of web browser-based games. These games may be identified based on the website that they appear, such as with "Facebook" games. Others are named based on the programming platform used to develop them, such as Java and Flash games. Genres A video game, like most other forms of media, may be categorized into genres. Video game genres are used to categorize video games based on their gameplay interaction rather than visual or narrative differences. A video game genre is defined by a set of gameplay challenges and are classified independent of their setting or game-world content, unlike other works of fiction such as films or books. For example, a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space.Because genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with titles specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as massively multiplayer online role-playing games, or, more commonly, MMORPGs. It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games. Classifications  Casual games Casual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system. Examples of genres within this category are match three, hidden object, time management, puzzle or many of the tower defense style games. Casual games are generally available through app stores and online retailers such as PopCap, Zylom and GameHouse or provided for free play through web portals such as Newgrounds. While casual games are most commonly played on personal computers, phones or tablets, they can also be found on many of the on-line console system download services (e.g., the PlayStation Network, WiiWare or Xbox Live). Serious games Serious games are games that are designed primarily to convey information or a learning experience to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning programs, etc.) and the primary distinction would appear to be based on the title's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule. Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exercise games, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.One of the longest-running serious games franchises would be Microsoft Flight Simulator first published in 1982 under that name. The United States military uses virtual reality based simulations, such as VBS1 for training exercises, as do a growing number of first responder roles (e.g., police, firefighters, EMTs). One example of a non-game environment utilized as a platform for serious game development would be the virtual world of Second Life, which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs and businesses (e.g., IBM, Cisco Systems) for meetings and training.Tactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point. For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include Darfur is Dying, Finding Zoe, and In The Balance. All of these games bring awareness to important issues and events in an intelligent and well thought out manner. Educational games On 23 September 2009, U.S. President Barack Obama launched a campaign called "Educate to Innovate" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments. This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup. Both of these examples are events that bring a focus to relevant and important current issues that are able to be addressed in the sense of video games to educate and spread knowledge in a new form of media. www.NobelPrize.org uses games to entice the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun to play video game. There are many different types and styles of educational games all the way from counting to spelling to games for kids and games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game. Controllers Video game can use several types of input devices to translate human actions to a game, the most common game controllers are keyboard and mouse for "PC games, consoles usually come with specific gamepads, handheld consoles have built in buttons. Other game controllers are commonly used for specific games like racing wheels, light guns or dance pads. Digital cameras can also be used as game controllers capturing movements of the body of the player.As technology continues to advance, more can be added onto the controller to give the player a more immersive experience when playing different games. There are some controllers that have presets so that the buttons are mapped a certain way to make playing certain games easier. Along with the presets, a player can sometimes custom map the buttons to better accommodate their play style. On keyboard and mouse, different actions in the game are already preset to keys on the keyboard. Most games allow the player to change that so that the actions are mapped to different keys that are more to their liking. The companies that design the controllers are trying to make the controller visually appealing and also feel comfortable in the hands of the consumer.An example of a technology that was incorporated into the controller was the touchscreen. It allows the player to be able to interact with the game differently than before. The person could move around in menus easier and they are also able to interact with different objects in the game. They can pick up some objects, equip others, or even just move the objects out of the players path. Another example is motion sensor where a persons movement is able to be captured and put into a game. Some motion sensor games are based off of where the controller is. Th reason for that is because there is a signal that is sent ffrom the controller to the console or computer so that the actions being done can create certian movements in the game. Other type of motion sensor games are webcam style where the person can move around infront of it and the actions done are repeated in a character of the game you are playing as. Development Video game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the "one-man shop" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some cellphones and PDAs).With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products. Downloadable content A phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with Grand Theft Auto IV (The Lost and Damned and The Ballad of Gay Tony), or Bethesda with Fallout 3 and its expansions. New gameplay modes can also become available, for instance, Call of Duty and its zombie modes, a multiplayer mode for Mushroom Wars or a higher difficulty level for Metro: Last Light. Smaller packages of DLC are also common, ranging from better in-game weapons (Dead Space, Just Cause 2), character outfits (LittleBigPlanet, Minecraft), or new songs to perform (SingStar, Rock Band, Guitar Hero). Expansion Pack A variation of downloadable content is expansion packs. Unlike DLC, expansion packs add a whole section to the game that either already existed in the game's code or was recently developed after the game had already been released. Expansions add new maps, missions, weapons, and other things that weren't previously accessible in the original game. An example of an expansion is Bungie's most recent game, Destiny, when they released the Rise of Iron expansion. The expansion added new weapons, new maps, higher levels, and also remade old missions so that the difficulty would be meet the new levels that were added to the characters. Expansions are added to the base game to help prolong the life of the game itself until the company is able to produce a sequel or a new game all together. Developers at times plan out their games life and already have the code for the expansion in the game but inaccessible by players and they would unlock the expansions as time went on to the players, sometimes at no extra cost and other times it costs extra to get the expansion. There are also some developers who make the game and then make the expansions as time goes on so that they could see what the players would like to have and what they can do to make the game better. There are also expansions that are set apart from the original game and are considered a stand-alone game, an example of that is Ubisoft's expansion Assassin's Creed IV: Black Flag Freedom's Cry which takes place control of a different character than that of the original game. Modifications Many games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve Corporation, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games. This allows for the kind of success seen by popular mods such as the Half-Life mod Counter-Strike. Cheating Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers, modification of game code by third parties, or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer. Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls. Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances. Glitches Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as patches) to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits. Easter eggs Easter eggs are hidden messages or jokes left in games by developers that are not part of the main game. Easter eggs are secret responses that occur as a result of an undocumented set of commands. The results can vary from a simple printed message or image, to a page of programmer credits or a small videogame hidden inside an otherwise serious piece of software. Videogame cheat codes are a specific type of Easter egg, in which entering a secret command will unlock special powers or new levels for the player. Theory Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from Star Trek, arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as Tron, eXistenZ and The Last Starfighter.Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game Tomb Raider, saying that "the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it." Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term "emergent narrative" has been used to describe how, in a simulated environment, storyline can be created simply by "what happens to the player." However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game. Emulation An emulator is a program that replicates the behavior of a video game console, allowing games to run on a different platform from the original hardware. Emulators exist for PCs, smartphones and consoles other than the original. Emulators are generally used to play old games, hack existing games, translate unreleased games in a specific region, or add enhanced features to games like improved graphics, speed up or down, bypass regional lockouts, or online multiplayer support.Some manufacturers have released official emulators for their own consoles. For example, the Virtual Console allows users to play games for old Nintendo consoles on the Wii, Wii U, and 3DS. Virtual Console is part of Nintendo's strategy for deterring video game piracy. In November 2015, Microsoft launched backwards compatibility of Xbox 360 games on Xbox One console via emulation. Also, Sony announced relaunching PS2 games on PS4 via emulation. According to Sony Computer Entertainment America v. Bleem, creating an emulator for a proprietary video game console is legal. However, Nintendo claims that emulators promote the distribution of illegally copied games. Social aspects  Demographics The November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between men and women. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB, almost 41% of PC gamers are women. Participation among African-Americans is even lower. One survey of over 2000 game developers returned responses from only 2.5% who identified as black.When comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:The average gamer is 30 years old and has been playing for 12 years. Eighty-two percent of gamers are 18 years of age or older.Forty-two percent of all players are women and women over 18 years of age are one of the industry's fastest growing demographics.Twenty-nine percent of game players are over the age of 50, an increase from nine percent in 1999.Sixty-five percent of gamers play games with other gamers in person.Fifty-five percent of gamers play games on their phones or handheld device.A 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group. A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group. Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games. Culture Video game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music. Multiplayer Video gaming has traditionally been a social experience. Multiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. Tennis for Two, arguably the first video game, was a two player game, as was its successor Pong. The first commercially available game console, the Magnavox Odyssey, had two controller inputs. Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; Eve Online set a record with 65,303 players on a single server in 2013. Behavioral effects It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. It has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.In Steven Johnson's book, Everything Bad Is Good for You, he argues that video games in fact demand far more from a player than traditional games like Monopoly. To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books. Some research suggests video games may even increase players' attention capacities.Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be "learning by doing" while playing video games while fostering creative thinking.The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people. According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to do problem solving. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups. In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects. A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games. Action gamers are not only better at ignoring distractions, but also at focusing on the main task. Objections to video games Like other media, such as rock music (notably heavy metal music and gangsta rap), video games have been the subject of objections, controversies and censorship, for instance because of depictions of violence, criminal activities, sexual themes, alcohol, tobacco and other drugs, propaganda, profanity or advertisements. Critics of video games include parents' groups, politicians, religious groups, scientists and other advocacy groups. Claims that some video games cause addiction or violent behavior continue to be made and to be disputed.There have been a number of societal and scientific arguments about whether the content of video games change the behavior and attitudes of a player, and whether this is reflected in video game culture overall. Since the early 1980s, advocates of video games have emphasized their use as an expressive medium, arguing for their protection under the laws governing freedom of speech and also as an educational tool. Detractors argue that video games are harmful and therefore should be subject to legislative oversight and restrictions. The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated. If a video game contains large amounts of violence it can cause a person to behave more aggressively. A study was done that showed that young people who have had a greater exposure to violence in video games ended up behaving more aggressively towards people in a social environment. Possible benefits In spite of the negative effects of video games, certain studies indicate that they may have value in terms of academic performance, perhaps because of the skills that are developed in the process. “When you play ... games you’re solving puzzles to move to the next level and that involves using some of the general knowledge and skills in maths, reading and science that you’ve been taught during the day,” said Alberto Posso an Associate Professor at the Royal Melbourne Institute of Technology, after analysing data from the results of standardized testing completed by over 12,000 high school students across Australia. As summarized by The Guardian, the study [published in the International Journal of Communication], "found that students who played online games almost every day scored 15 points above average in maths and reading tests and 17 points above average in science." However, the reporter added an important comment that was not provided by some of the numerous Web sites that published a brief summary of the Australian study study: "[the] methodology cannot prove that playing video games were the cause of the improvement." The Guardian also reported that a Columbia University study indicated that extensive video gaming by students in the 6 to 11 age group provided a greatly increased chance of high intellectual functioning and overall school competence.In an interview with CNN, Edward Castronova, a professor of Telecommunications at Indiana University Bloomington said he was not surprised by the outcome of the Australian study but also discussed the issue of causal connection. "Though there is a link between gaming and higher math and science scores, it doesn't mean playing games caused the higher scores. It could just be that kids who are sharp are looking for a challenge, and they don't find it on social media, and maybe they do find it on board games and video games," he explained.Video games have also been proven to raise self-esteem and build confidence. It gives people an opportunity to do things that they cannot do offline, and to discover new things about themselves. There is a social aspect to gaming as well – research has shown that a third of video game players make good friends online. As well as that, video games are also considered to be therapeutic as it helps to relieve stress. Although short term, studies have shown that children with developmental delays gain a temporary physical improvement in health when they interact and play video games on a regular, and consistent basis due to the cognitive benefits and the use of hand eye coordination Ratings and censorship  ESRB The Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated T for Teen if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an M for Mature rating, which means that no one under 17 should play it. There is a rated "A/O" games for "Adults Only" these games have massive violence or nudity. There are no laws that prohibit children from purchasing "M" rated games in the United States. Laws attempting to prohibit minors from purchasing "M" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment. However, many stores have opted to not sell such games to children anyway. Of course, video game laws vary from country to country. One of the most controversial games of all time, Manhunt 2 by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience. Video game manufacturers usually exercise tight control over the games that are made available on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, tablet computers or PDAs. PEGI Pan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material, it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age. Germany: BPjM and USK Stricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle (USK or Voluntary Certification of Entertainment Software) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is considered to be harmful to young people (for example because of extremely violent, pornographic or racist content), it may be referred to the BPjM (Bundesprüfstelle für jugendgefährdende Medien – Federal Verification Office for Child-Endangering Media) who may opt to place it on the Index upon which the game may not be sold openly or advertised in the open media. Such indexed titles are not "banned" and can still be legally obtained by adults, but it is considered a felony to supply these titles to a child. Commercial aspects  Game sales According to the market research firm SuperData, as of May 2015, the global games market was worth USD 74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play MMOs 8.7 billion, social games $7.9 billion, PC DLC 7.5 billion, and other categories $3 billion or less each.In the United States, also according to SuperData, the share of video games in the entertainment market grew from 5% in 1985 to 13% in 2015, becoming the third-largest market segment behind broadcast and cable television. The research firm anticipated that Asia would soon overtake North America as the largest video game market due to the strong growth of free-to-play and mobile games.Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more console games than computer games, with a strong preference for games catering to local tastes. Another key difference is that, despite the decline of arcades in the West, arcade games remain the largest sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China. Conventions Gaming conventions are an important showcase of the industry. The annual gamescom in Cologne in August is the world's leading expo for video games in attendance. The E3 in June in Los Angeles is also of global importance, but is an event for industry insiders only. The Tokyo Game Show in September is the main fair in Asia. Other notable conventions and trade fairs include Brasil Game Show in October, Paris Games Week in October–November, EB Games Expo (Australia) in October, KRI, ChinaJoy in July and the annual Game Developers Conference. Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples. eSports Short for electronic sports, are video game competitions played most by professional players individually or in teams that gained pupularity from the late 2000s, the most common genres are fighting, first-person shooter (FPS), multiplayer online battle arena (MOBA) and real-time strategy. There are certain games that are made for just competitive multiplayer purposes. With those type of games, players focus entirely one choosing the right character or obtaining the right equipment in the game to help them when facing other players. Tournaments are held so that people in the area or from different regions can play against other players of the same game and see who is the best. Major League Gaming (MLG) is a company that reports tournaments that are held across the country. The players that compete in these tournaments are given a rank depending on their skill level in the game that they choose to play in and face other players that play that game. The players that also compete are mostly called professional players for the fact that they have played the game they are competing in for many, long hours. Those players have been able to come up with different strategies for facing different characters. The professional players are able to pick a character to their liking and be able to master how to use that character very effectively. With strategy games, players tend to know how to get resources quick and are able to make quick decisions about where their troops are to be deployed and what kind of troops to create. Museums There are many video game museums around the world, including the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a 5,000-square-foot (460 m2) exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, D.C. has three video games on permanent display: Pac-Man, Dragon's Lair, and Pong.The Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on "The Art of Video Games". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum. See also List of video gamesHistory of video gamesOutline of video gamesVideo game addiction Notes  References  External links Video games bibliography by the French video game research association LudoscienceThe Virtual Museum of Computing (VMoC)
A central processing unit (CPU) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The computer industry has used the term "central processing unit" at least since the early 1960s. Traditionally, the term "CPU" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.The form, design and implementation of CPUs have changed over the course of their history, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory) and execution of instructions by directing the coordinated operations of the ALU, registers and other components.Most modern CPUs are microprocessors, meaning they are contained on a single integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called "cores"; in that context, one can speak of such single chips as "sockets". Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. There also exists the concept of virtual CPUs which are an abstraction of dynamical aggregated computational resources. History Early computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers". Since the term "CPU" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.The idea of a stored-program computer was already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled First Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Small-Scale Experimental Machine, a small prototype stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.Early CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also utilized a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.Relays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with. Transistor CPUs The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like vacuum tubes and relays. With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.In 1964, IBM introduced its System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called "microcode"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8.Transistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd. Small-scale integration CPUs During this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or "chip". At first only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.IBM's System/370 follow-on to the System/360 used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs but was eventually implemented with LSI components once these became practical. Large-scale integration CPUs Lee Boysel published influential articles, including a 1967 "manifesto", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). At the time, the only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a MOS process (i.e., PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar chips because bipolar junction transistors were so much faster than MOS chips; for example, Datapoint built processors out of transistor–transistor logic (TTL) chips until the early 1980s.People building high-speed computers wanted them to be fast, so in the 1970s they built the CPUs from small-scale integration (SSI) and medium-scale integration (MSI) 7400 series TTL gates. At the time, MOS ICs were so slow that they were considered useful only in a few niche applications that required low power.As the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the quantity of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits. Microprocessors Since the introduction of the first commercially available microprocessor, the Intel 4004 in 1970, and the first widely used microprocessor, the Intel 8080 in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. The 4004 has origins in the "Busicom Project" which dates back to 1968, with key figures in the 4004's creation including Busicom's Masatoshi Shima, Sharp's Tadashi Sasaki, and Intel's Marcian Hoff and Federico Faggin, whose fundamental inventions (Silicon Gate MOS ICs with self-aligned gates along with his new random logic design methodology) made its implementation a reality.Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term CPU is now applied almost exclusively to microprocessors. Several CPUs (denoted cores) can be combined in a single processing chip. Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity.While the complexity, size, construction, and general form of CPUs have changed enormously since 1950, it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As the aforementioned Moore's law continues to hold true, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model. Operation The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.After the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded, and executed simultaneously. This section describes what is generally referred to as the "classic RISC pipeline", which is quite common among the simple CPUs used in many electronic devices (often called microcontroller). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.Some instructions manipulate the program counter rather than producing result data directly; such instructions are generally called "jumps" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a "flags" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a "compare" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow. Fetch The first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by a program counter (PC), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below). Decode The instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by the circuitry known as the instruction decoder, the instruction is converted into signals that control other parts of the CPU.The way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a "field") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.In some CPU designs the instruction decoder is implemented as a hardwired, unchangeable circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions. Execute After the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, various parts of the CPU are electrically connected so they can perform all or part of the desired operation and then the action is completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.For example, if an addition instruction is to be executed, the arithmetic logic unit (ALU) inputs are connected to a pair of operand sources (numbers to be summed), the ALU is configured to perform an addition operation so that the sum of its operand inputs will appear at its output, and the ALU output is connected to storage (e.g., a register or memory) that will receive the sum. When the clock pulse occurs, the sum will be transferred to storage and, if the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set. Structure and implementation Hardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each basic operation is represented by a particular combination of bits, known as the machine language opcode; while executing instructions in a machine language program, the CPU decides which operation to perform by "decoding" the opcode. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.The actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Beside the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU). Control unit The control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit communicates with both the ALU and memory. Arithmetic logic unit The arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers or external memory, or they may be constants generated by the ALU itself.When all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose. Memory management unit Most high-end microprocessors (in desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don't include an MMU. Clock rate Most CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.To ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the "edges" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).However, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.One method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; that way, power requirements of the Xbox 360 are greatly reduced. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without utilizing a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers. Integer range Every CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a "high" or "low" voltage.Related to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called "word size", "bit width", "data path width", "integer precision", or "integer size". A CPU's integer size determines the range of integer values it can directly operate on. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.Integer range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.CPUs with larger word sizes require more circuitry and consequently are physically larger, cost more, and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68k instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.To gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating point values to facilitate greater accuracy and range in floating point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required. Parallelism The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as subscalar, operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle (IPC < 1).This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock cycle, IPC  1). However, the performance is nearly always subscalar (less than one instruction per clock cycle, IPC < 1).Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:instruction-level parallelism (ILP), which seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the utilization of on-die execution resources);task-level parallelism (TLP), which purposes to increase the number of threads or processes that a CPU can execute simultaneously.Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application. Instruction-level parallelism One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is the simplest form of a technique known as instruction pipelining, and is utilized in almost all modern general-purpose CPUs. Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs. Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units. In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously. In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, and out-of-order execution crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream—a case when a lot of data from the same type has to be processed—, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.In the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not accept one instruction per clock cycle. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.Both simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or ISA. The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity. Task-level parallelism Another strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).One technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.CPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.This reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PS3's 7-core Cell microprocessor. Data parallelism A less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as single instruction stream, multiple data stream (SIMD) and single instruction stream, single data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video, and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.Most early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications - like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX - were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples include Intel's SSE and the PowerPC-related AltiVec (also known as VMX). Virtual CPUs Cloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).A host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned out into resources pools with fine granularity. Performance The performance or speed of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform. Many reported IPS values have represented "peak" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called "benchmarks" for this purpose‍—‌such as SPECint‍—‌have been developed to attempt to measure the real effective performance in commonly used applications.Processing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called cores in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.Due to specific capabilities of modern CPUs, such as hyper-threading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware utilization gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual utilization of various parts of a CPU and provides various counters accessible to software; an example is Intel's Performance Counter Monitor technology. See also  Notes  References  External links How Microprocessors Work at HowStuffWorks.25 Microchips that shook the world – an article by the Institute of Electrical and Electronics Engineers.
The hardware of the Macintosh (or Mac) is produced solely by Apple Inc., who determines internal systems, designs, and prices. Apple directly sub-contracts hardware production to external OEM companies, maintaining a high degree of control over the end product. Apple buys certain components wholesale from third-party manufacturers. The current Mac product family uses Intel x86-64 processors. All Mac models ship with at least 1 GB RAM as standard. Current Mac computers use AMD Radeon or nVidia GeForce graphics cards and may include a dual-function DVD and CD burner, called the SuperDrive. Macs include two standard data transfer ports: USB and Thunderbolt. USB was introduced in the 1998 iMac G3 and is ubiquitous today; Thunderbolt is intended for high-performance devices such as external graphics cards. Processor architecture The original Macintosh used a Motorola 68000, a 16/32-bit (32-bit internal) CISC processor that ran at 8 MHz. The Macintosh Portable and PowerBook 100 both used a 16 MHz version. The Macintosh II featured a full 32-bit Motorola 68020 processor, but the Mac ROMs at the time contained software that only supported 24-bit memory addressing, therefore using only a fraction of the chip's memory addressing capabilities unless a software patch was applied. Macs with this limitation were referred to as not being “32-bit clean.” The successor Macintosh IIx introduced the Motorola 68030 processor, which added a memory management unit. The 68030 did not have a built-in floating point unit (FPU); thus, '030-based Macintoshes incorporated a separate unit—either the 68881 or 68882. Lower-cost models did without, although they incorporated an FPU socket, should the user decide to add one as an option. The first “32-bit clean” Macintosh that could use 32-bit memory addressing without a software patch was the IIci. In 1991, Apple released the first computers containing the Motorola 68040 processor, which contained the floating point unit in the main processor. Again, lower-cost models did not have FPUs, being based on the cut-down Motorola 68LC040 instead.After 1994 Apple used the PowerPC line of processors, starting with the PowerPC 601, which were later upgraded to the 603 and 603e and 604, 604e, and 604ev. In 1997, Apple introduced its first computer based on the significantly upgraded PowerPC G3 processor; this was followed in 1999 with the PowerPC G4. The last generation of PowerPC processor to be introduced was the 64-bit PowerPC 970FX ("G5"), introduced in 2003. During the transition to the PowerPC, Apple’s “Cognac” team wrote a 68030-to-PowerPC emulator that booted very early in OS loading. Initially the emulation was very slow, but later versions used a dynamic recompilation emulator which boosted performance by caching frequently used sections of translated code. The first version of the OS to ship with the earliest PowerPC systems was estimated to run 95% emulated. Later versions of the operating system increased the percentage of PowerPC native code until OS X brought it to 100% native.The PowerPC 604 processor introduced symmetric multiprocessing (SMP) to the Macintosh platform, with dual PowerPC 604e-equipped Power Macintosh 9500 and 9600 models. The G3 processor was not SMP-capable, but the G4 and G5 were, and Apple introduced many dual-CPU G4 and G5 Power Macs. The top of the range Power Macintosh G5 uses up to two dual core processors, for a total of four cores.On June 6, 2005, Steve Jobs announced that the company would begin transitioning the Macintosh line from PowerPC to Intel microprocessors (the transition was completed on August 7, 2006) and demonstrated a version of Mac OS X running on a computer powered by an Intel Pentium 4 CPU. Intel-powered Macs are able to run Macintosh software compiled for PowerPC processors using a dynamic translation system known as “Rosetta.” From OS 10.7 on, Rosetta is not an option.The first Macs with Intel processors were the iMac and the 15-inch MacBook Pro, both announced at the Macworld Conference and Expo in January 2006. Throughout the year the Mac mini was transitioned to the Intel architecture, with users having choice of either Core Solo or Core Duo CPUs. The iBook product line was phased out by the MacBook and on August 7, 2006, the Power Mac G5 was discontinued in favor of the Mac Pro, based on the new Intel Xeon "Woodcrest". The Xserve was also transitioned to an Intel Xeon "Woodcrest". In the second half of 2006 Apple launched new iMac and MacBook lines using the Core 2 Duo processor. Expandability and connectivity Apple detractors have always criticized the fact that Macs cannot be upgraded, as can most PCs. While most PC's use an ATX-formfactor logic board, power supply, and case, Apple has eschewed the popular standards as to give their design team maximum flexibility. However, Apple does use Intel processors, as well as industry-standard memory, drives, and peripherals.Historically, Macs were not designed to be taken apart. Ever since the original closed-box Macintosh in 1984, Apple has always preferred that upgrades take place outside the case. While PC users would open up their computer to install a second hard drive, Mac users would simply plug an external hard drive into their computer; this adds slight cost and the external hard drive performs more slowly, but is easier for the average user to perform.Due to the Macs' unique designs, most tasks that involve opening the computer are relegated to Apple-certified technicians; otherwise, the machine's warranty is null and void. However, Apple towers (such as the Mac Pro) permit access to all of the system's internals, allowing users to add or replace common items such as memory, drives, or expansion cards. Internal slots The earliest form of internal Macintosh expandability was the Processor Direct Slot (PDS), present from the SE onwards. It was basically a shortcut to the CPU socket, not a bus—which also meant that parts for the PDS slot were tied to a specific Macintosh model, with the notable exception of the LC PDS slot, which was standardized across the entire LC line. The PDS slot could be used for processor upgrades, Ethernet cards, the Apple IIe Card, or video cards. The last line of Macintoshes to have PDS slots was the first generation of the Power Macs.The first Macintosh to feature a bus for expansion was the Macintosh II, in the form of six NuBus (parallel 32-bit bus) slots. The NuBus was abandoned in favor of PCI in the second-generation Power Macs, and the G4 introduced 64-bit PCI slots as well as an AGP slot for video cards. The Power Mac G5 quickly introduced PCI-X slots, which were short-lived, as the final G5's and the Mac Pro use PCI Express for graphics and expansion.Out of the current models (as of August 2007), only the Mac Pro and Xserve feature PCIe slots and standard hard drive bays for easy upgradability. The PCIe slots allow addition of (for example) RAID controllers, video cards, or specialty audio cards. The MacBook Pro features a PCIe slot, in the form of a single ExpressCard/34 slot. Processors The Mac mini, iMac and Mac Pro all feature upgradeable Intel processors, although Apple does not officially support this.The Power Mac G3, as well as the very first Power Mac G4, had a socketed processor which could be upgraded. From then on, the Power Macs had their processor(s) on a daughtercard. All other Macs, including the Mac mini, most iMacs, and all of Apple's notebooks, have the processor permanently soldered to the logic board. Nevertheless, this did not stop companies such as Daystar and Sonnet from marketing processor upgrades for almost every system. Memory For memory, Apple has used standard SIMMs (30 and 72-pin), proprietary 168-pin DIMMs, and later, industry-standard SDRAM and DDR. Current Macs use either 1600 MHz, 1866 MHz or 2133 MHz DDR3 memory depending on the model.Only the 27-inch iMac and Mac Pro allow the user to upgrade the memory via an access door or removable panel. All other current Macs, including all portable Macs, have soldered-in memory. Disks The earliest Macintoshes used a proprietary serial port (a 19-pin D-subminiature connector) for external floppy or hard drives, until SCSI was introduced with the Macintosh Plus. SCSI remained the Macintosh drive medium of choice until the mid-1990s, when less expensive ATA drives were introduced, first on budget models, then across the whole range. Current Macs use Serial ATA for internal hard drives and optical drives, and FireWire or USB 2.0 for external drives.Only the Power Macs, Mac Pros, Xserves, and MacBook have user-accessible drive bays to allow one or more hard drives to be installed internally. All other machines have one dedicated space for one hard drive.All Macs have one optical drive, except the Mac Pro, which can optionally include two.Mac OS X, understands the Mac OS Standard and Mac OS Extended file systems. It is also capable of using disks formatted with Windows's FAT or NTFS file systems, as well as the Unix File System. Currently, Mac OS X Leopard betas have read-only support for ZFS, while paid members of Apple Developer Connection get access to an in-development read-write ZFS driver. Peripherals The very first Macs (the Macintosh and the Macintosh 512K) used proprietary connectors for the keyboard and mouse. The Apple Desktop Bus (ADB) was introduced with the Macintosh II and Macintosh SE. It was the standard input connector for keyboards and mice until USB was introduced with the iMac. The last Macintosh to have ADB was the Power Macintosh G3 (Blue & White), alongside the now-standard USB. Until February 2005, the PowerBook G4 and iBook G4 notebooks still used the ADB protocol to communicate with their built-in keyboards and trackpads, however they did not include any external ADB connectors.The majority of Mac computers have historically shipped with a single-button mouse. This changed in August 2005, when Apple released the four-button Mighty Mouse (a wireless version was made available on July 25, 2006) and began to ship it with new desktop Macs. As of late 2009, they have begun to also offer a Multi-Touch mouse, Magic Mouse, capable of receiving touch finger gestures. Starting with a new iMac G5 released in October 2005, Apple started to include built-in iSight cameras to appropriate models, and a media center interface called Front Row that can be operated by remote control for accessing media stored on the computer.Other legacy Macintosh peripheral connectors include the RS-432 serial ports, the GeoPort, and the AAUI port for networking.Since 2006, portable Macintoshes have used the MagSafe connection for their power cords. The cable attaches to the computer magnetically, rather than mechanically, so an unexpected yank on the cord will merely disconnect the cable, rather than send the delicate laptop flying. Networking Early Macs used the built-in serial ports for LocalTalk, which set up a fast (at the time) network between two machines. Later, a modified AUI port was added named Apple Attachment Unit Interface to provide a more user friendly version of 10BASE2 cabling and adapters, with Apple's version known as FriendlyNet. As this was an implementation of an Ethernet physical layer it was indifferent to the protocols used - and allowed connection of Macintosh machines using LocalTalk, TCP/IP, or other protocols assuming it was supported by programs. Eventually as Ethernet over twisted pair emerged as the dominate method for connecting computers, all Macs adopted the now familiar modular 8 pin modular jack as standard. Fibre Channel adapters are also available for the Mac Pro and the discontinued Xserve, generally for connection to large storage subsystems and/or high bandwidth multimedia applications.Apple introduced 802.11 wireless networking to the Mac in 1999, with AirPort technology built into the iBook. Three years later, it was updated to the 802.11g-compatible AirPort Extreme. With the exception of the desktop Mac Pro (available as user option), all current Macs feature 802.11n-capable AirPort Extreme cards as standard.Macs with IEEE 1394 support (a.k.a. FireWire) provide TCP/IP connectivity through this interface by default, allowing two machines to easily create a high-speed connection through a single FireWire cable. Video For connecting displays, Apple used a DA-15 connector on all models prior to the Blue and White Power Mac G3, which used a VGA connector. The original AGP-based Power Mac G4 used VGA, complemented by a DVI port; almost all later Macs, however, used the Apple Display Connector in addition to a VGA or DVI port, until the last revisions of the Power Mac G5 came standard with two DVI ports. Apple includes DVI-to-VGA adapters with its DVI-equipped computers. With the release of the unibody MacBook Pro, DVI was phased out in favor of DisplayPort.Smaller form-factor laptops, such as the iBooks and 12" PowerBook G4 and later generations of the Mac mini did not have enough space available to fit a full-size VGA or DVI connector. As such, these machines use a miniaturised variant, mini-VGA or mini-DVI, intended to be used with an adapter. However, even Mini-DVI was too large for the original MacBook Air. It instead was equipped with a micro-DVI port and came with an adapter. The MacBook Air was the only Mac to ever use this connector, until it too was phased out in favour of DisplayPort. The original DisplayPort connector was never used on Macs, Apple instead opting to create and standardise a miniature version of the connector: mini-DisplayPort. The Unibody Mac Mini is equipped with both a mini-DisplayPort and an HDMI output. It is the only Mac to ever ship with HDMI built-in. The Unibody Mac mini is shipped with an HDMI-to-DVI adapter.While not user-accessible, the 24-inch iMac features an Mobile PCI Express Module-formfactor video card; however, there are no upgrades available for it. Video cards can be replaced by the user in a Power Mac (which used PCI; later, AGP; finally, PCIe) or the Mac Pro (which has four PCIe slots). In all other Macs, the video card is integrated with the logic board and cannot be replaced.PowerPC-based Macs, for the most part, required compatible video cards. The current Intel-based Macs can use any EFI-compatible video card; normal PC video cards will work only if the user boots into Microsoft Windows. Some hackers, however, have found success "flashing" PC cards to work with Mac OS X in Apple's hardware. See also History of computing hardware (1960s-present)List of Macintosh models grouped by CPU typeList of Macintosh models by case typeApple KeyboardApple MouseiPodiPhone References 
Baseball is a bat-and-ball game played between two teams of nine players each, who take turns batting and fielding.The batting team attempts to score runs by hitting a ball that is thrown by the pitcher with a bat swung by the batter, then running counter-clockwise around a series of four bases: first, second, third, and home plate. A run is scored when a player advances around the bases and returns to home plate.Players on the batting team take turns hitting against the pitcher of the fielding team, which tries to prevent runs by getting hitters out in any of several ways. A player on the batting team who reaches a base safely can later attempt to advance to subsequent bases during teammates' turns batting, such as on a hit or by other means. The teams switch between batting and fielding whenever the fielding team records three outs. One turn batting for both teams, beginning with the visiting team, constitutes an inning. A game is composed of nine innings, and the team with the greater number of runs at the end of the game wins. Baseball has no game clock, although almost all games end in the ninth inning.Baseball evolved from older bat-and-ball games already being played in England by the mid-18th century. This game was brought by immigrants to North America, where the modern version developed. By the late 19th century, baseball was widely recognized as the national sport of the United States. Baseball is currently popular in North America and parts of Central and South America, the Caribbean, and East Asia, particularly Japan.In the United States and Canada, professional Major League Baseball (MLB) teams are divided into the National League (NL) and American League (AL), each with three divisions: East, West, and Central. The major league champion is determined by playoffs that culminate in the World Series. The top level of play is similarly split in Japan between the Central and Pacific Leagues and in Cuba between the West League and East League. History  Origins The evolution of baseball from older bat-and-ball games is difficult to trace with precision. A French manuscript from 1344 contains an illustration of clerics playing a game, possibly la soule, with similarities to baseball. Other old French games such as thèque, la balle au bâton, and la balle empoisonnée also appear to be related. Consensus once held that today's baseball is a North American development from the older game rounders, popular in Great Britain and Ireland. Baseball Before We Knew It: A Search for the Roots of the Game (2005), by David Block, suggests that the game originated in England; recently uncovered historical evidence supports this position. Block argues that rounders and early baseball were actually regional variants of each other, and that the game's most direct antecedents are the English games of stoolball and "tut-ball." It has long been believed that cricket also descended from such games, though evidence uncovered in early 2009 suggests that cricket may have been imported to England from Flanders.The earliest known reference to baseball is in a 1744 British publication, A Little Pretty Pocket-Book, by John Newbery. It contains a rhymed description of "base-ball" and a woodcut that shows a field set-up somewhat similar to the modern game—though in a triangular rather than diamond configuration, and with posts instead of ground-level bases. David Block discovered that the first recorded game of "Bass-Ball" took place in 1749 in Surrey, and featured the Prince of Wales as a player. William Bray, an English lawyer, recorded a game of baseball on Easter Monday 1755 in Guildford, Surrey. This early form of the game was apparently brought to Canada by English immigrants. Rounders was also brought to the USA by Canadians of both British and Irish ancestry. The first known American reference to baseball appears in a 1791 Pittsfield, Massachusetts, town bylaw prohibiting the playing of the game near the town's new meeting house. By 1796, a version of the game was well-known enough to earn a mention in a German scholar's book on popular pastimes. As described by Johann Gutsmuths, "englische Base-ball" involved a contest between two teams, in which "the batter has three attempts to hit the ball while at the home plate." Only one out was required to retire a side.By the early 1830s, there were reports of a variety of uncodified bat-and-ball games recognizable as early forms of baseball being played around North America. These games were often referred to locally as "town ball", though other names such as "round-ball" and "base-ball" were also used. Among the earliest examples to receive a detailed description—albeit five decades after the fact, in a letter from an attendee to Sporting Life magazine—took place in Beachville, Ontario, in 1838. There were many similarities to modern baseball, and some crucial differences: five bases (or byes); first bye just 18 feet (5.5 m) from the home bye; batter out if a hit ball was caught after the first bounce. The once widely accepted story that Abner Doubleday invented baseball in Cooperstown, New York, in 1839 has been conclusively debunked by sports historians.In 1845, Alexander Cartwright, a member of New York City's Knickerbocker Club, led the codification of the so-called Knickerbocker Rules. The practice, common to bat-and-ball games of the day, of "soaking" or "plugging"—effecting a putout by hitting a runner with a thrown ball—was barred. The rules thus facilitated the use of a smaller, harder ball than had been common. Several other rules also brought the Knickerbockers' game close to the modern one, though a ball caught on the first bounce was, again, an out and only underhand pitching was allowed. While there are reports that the New York Knickerbockers played games in 1845, the contest long recognized as the first officially recorded baseball game in U.S. history took place on June 19, 1846, in Hoboken, New Jersey: the "New York Nine" defeated the Knickerbockers, 23–1, in four innings (three earlier games have recently been discovered). With the Knickerbocker code as the basis, the rules of modern baseball continued to evolve over the next half-century. History of baseball in the United States  The game turns professional In the mid-1850s, a baseball craze hit the New York metropolitan area. By 1856, local journals were referring to baseball as the "national pastime" or "national game." A year later, sixteen area clubs formed the sport's first governing body, the National Association of Base Ball Players. In 1858 in Corona, Queens New York, at the Fashion Race Course, the first games of baseball to charge admission took place. The games, which took place between the all stars of Brooklyn, including players from the Brooklyn Atlantics, Excelsior of Brooklyn, Putnams and Eckford of Brooklyn, and the All Stars of New York (Manhattan), including players from the New York Knickerbockers, Gothams (predecessors of the San Francisco Giants), Eagles and Empire, are commonly believed to be the first all-star baseball games. In 1863, the organization disallowed putouts made by catching a fair ball on the first bounce. Four years later, it barred participation by African Americans. The game's commercial potential was developing: in 1869 the first fully professional baseball club, the Cincinnati Red Stockings, was formed and went undefeated against a schedule of semipro and amateur teams. The first professional league, the National Association of Professional Base Ball Players, lasted from 1871 to 1875; scholars dispute its status as a major league.The more formally structured National League was founded in 1876. As the oldest surviving major league, the National League is sometimes referred to as the "senior circuit." Several other major leagues formed and failed. In 1884, African American Moses Walker (and, briefly, his brother Welday) played in one of these, the American Association. An injury ended Walker's major league career, and by the early 1890s, a gentlemen's agreement in the form of the baseball color line effectively barred black players from the white-owned professional leagues, major and minor. Professional Negro leagues formed, but quickly folded. Several independent African American teams succeeded as barnstormers. Also in 1884, overhand pitching was legalized. In 1887, softball, under the name of indoor baseball or indoor-outdoor, was invented as a winter version of the parent game. Virtually all of the modern baseball rules were in place by 1893; the last major change—counting foul balls as strikes—was instituted in 1901. The National League's first successful counterpart, the American League, which evolved from the minor Western League, was established that year. The two leagues, each with eight teams, were rivals that fought for the best players, often disregarding each other's contracts and engaging in bitter legal disputes.A modicum of peace was eventually established, leading to the National Agreement of 1903. The pact formalized relations both between the two major leagues and between them and the National Association of Professional Base Ball Leagues, representing most of the country's minor professional leagues. The World Series, pitting the two major league champions against each other, was inaugurated that fall, albeit without express major league sanction: The Boston Americans of the American League defeated the Pittsburgh Pirates of the National League. The next year, the series was not held, as the National League champion New York Giants, under manager John McGraw, refused to recognize the major league status of the American League and its champion. In 1905, the Giants were National League champions again and team management relented, leading to the establishment of the World Series as the major leagues' annual championship event.As professional baseball became increasingly profitable, players frequently raised grievances against owners over issues of control and equitable income distribution. During the major leagues' early decades, players on various teams occasionally attempted strikes, which routinely failed when their jobs were sufficiently threatened. In general, the strict rules of baseball contracts and the reserve clause, which bound players to their teams even when their contracts had ended, tended to keep the players in check. Motivated by dislike for particularly stingy owner Charles Comiskey and gamblers' payoffs, real and promised, members of the Chicago White Sox conspired to throw the 1919 World Series. The Black Sox Scandal led to the formation of a new National Commission of baseball that drew the two major leagues closer together. The first major league baseball commissioner, Kenesaw Mountain Landis, was elected in 1920. That year also saw the founding of the Negro National League; the first significant Negro league, it would operate until 1931. For part of the 1920s, it was joined by the Eastern Colored League.Professional baseball was played in northeastern cities with a large immigrant-ethnic population; they gave strong support to the new sport. The Irish Catholics dominated in the late 19th century, comprising a third or more of the players and many of the top stars and managers. Historian Jerrold Casway argues that:Baseball for Irish kids was a shortcut to the American dream and to self-indulgent glory and fortune. By the mid-1880s these young Irish men dominated the sport and popularized a style of play that was termed heady, daring, and spontaneous.... Ed Delahanty personified the flamboyant, exciting spectator-favorite, the Casey-at-the-bat, Irish slugger. The handsome masculine athlete who is expected to live as large as he played. Rise of Ruth and racial integration Compared with the present, professional baseball in the early 20th century was lower-scoring and pitchers, the likes of Walter Johnson and Christy Mathewson, were more dominant. The "inside game," which demanded that players "scratch for runs", was played much more aggressively than it is today: the brilliant and often violent Ty Cobb epitomized this style. The so-called dead-ball era ended in the early 1920s with several changes in rule and circumstance that were advantageous to hitters. Strict new regulations governing the ball's size, shape and composition along with a new rule officially banning the spitball, along with other pitches that depended on the ball being treated or roughed-up with foreign substances after the death of Ray Chapman who was hit by a pitch in August 1920, coupled with superior materials available after World War I, resulted in a ball that traveled farther when hit. The construction of additional seating to accommodate the rising popularity of the game often had the effect of bringing the outfield fences closer in, making home runs more common. The rise of the legendary player Babe Ruth, the first great power hitter of the new era, helped permanently alter the nature of the game. The club with which Ruth set most of his slugging records, the New York Yankees, built a reputation as the majors' premier team. In the late 1920s and early 1930s, St. Louis Cardinals general manager Branch Rickey invested in several minor league clubs and developed the first modern "farm system". A new Negro National League was organized in 1933; four years later, it was joined by the Negro American League. The first elections to the Baseball Hall of Fame took place in 1936. In 1939 Little League Baseball was founded in Pennsylvania. By the late 1940s, it was the organizing body for children's baseball leagues across the United States.With America's entry into World War II, many professional players had left to serve in the armed forces. A large number of minor league teams disbanded as a result and the major league game seemed under threat as well. Chicago Cubs owner Philip K. Wrigley led the formation of a new professional league with women players to help keep the game in the public eye – the All-American Girls Professional Baseball League existed from 1943 to 1954. The inaugural College World Series was held in 1947, and the Babe Ruth League youth program was founded. This program soon became another important organizing body for children's baseball. The first crack in the unwritten agreement barring blacks from white-controlled professional ball occurred the previous year: Jackie Robinson was signed by the National League's Brooklyn Dodgers—where Branch Rickey had become general manager—and began playing for their minor league team in Montreal. In 1947, Robinson broke the major leagues' color barrier when he debuted with the Dodgers. Larry Doby debuted with the American League's Cleveland Indians the same year. Latin American players, largely overlooked before, also started entering the majors in greater numbers. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and black Cuban-born Minnie Miñoso, became the first Hispanic All-Stars.Facing competition as varied as television and football, baseball attendance at all levels declined. While the majors rebounded by the mid-1950s, the minor leagues were gutted and hundreds of semipro and amateur teams dissolved. Integration proceeded slowly: by 1953, only six of the 16 major league teams had a black player on the roster. That year, the Major League Baseball Players Association was founded. It was the first professional baseball union to survive more than briefly, but it remained largely ineffective for years. No major league team had been located west of St. Louis until 1958, when the Brooklyn Dodgers and New York Giants relocated to Los Angeles and San Francisco, respectively. The majors' final all-white bastion, the Boston Red Sox, added a black player in 1959. With the integration of the majors drying up the available pool of players, the last Negro league folded the following year. In 1961, the American League reached the West Coast with the Los Angeles Angels expansion team, and the major league season was extended from 154 games to 162. This coincidentally helped Roger Maris break Babe Ruth's long-standing single-season home run record, one of the most celebrated marks in baseball. Along with the Angels, three other new franchises were launched during 1961–62. With this, the first major league expansion in 60 years, each league now had ten teams. Attendance records and the age of steroids The players' union became bolder under the leadership of former United Steelworkers chief economist and negotiator Marvin Miller, who was elected executive director in 1966. On the playing field, major league pitchers were becoming increasingly dominant again. After the 1968 season, in an effort to restore balance, the strike zone was reduced and the height of the pitcher's mound was lowered from 15 to 10 inches (38.1 - 25.4 cm) . In 1969, both the National and American leagues added two more expansion teams, the leagues were reorganized into two divisions each, and a post-season playoff system leading to the World Series was instituted. Also that same year, Curt Flood of the St. Louis Cardinals made the first serious legal challenge to the reserve clause. The major leagues' first general players' strike took place in 1972. In another effort to add more offense to the game, the American League adopted the designated hitter rule the following year. In 1975, the union's power—and players' salaries—began to increase greatly when the reserve clause was effectively struck down, leading to the free agency system. In 1977, two more expansion teams joined the American League. Significant work stoppages occurred again in 1981 and 1994, the latter forcing the cancellation of the World Series for the first time in 90 years. Attendance had been growing steadily since the mid-1970s and in 1994, before the stoppage, the majors were setting their all-time record for per-game attendance.The addition of two more expansion teams after the 1993 season had facilitated another restructuring of the major leagues, this time into three divisions each. Offensive production—the number of home runs in particular—had surged that year, and again in the abbreviated 1994 season. After play resumed in 1995, this trend continued and non-division-winning wild card teams became a permanent fixture of the post-season. Regular-season interleague play was introduced in 1997 and the second-highest attendance mark for a full season was set. The next year, Mark McGwire and Sammy Sosa both surpassed Maris's decades-old single season home run record and two more expansion franchises were added. In 2000, the National and American leagues were dissolved as legal entities. While their identities were maintained for scheduling purposes (and the designated hitter distinction), the regulations and other functions—such as player discipline and umpire supervision—they had administered separately were consolidated under the rubric of Major League Baseball (MLB).In 2001, Barry Bonds established the current record of 73 home runs in a single season. There had long been suspicions that the dramatic increase in power hitting was fueled in large part by the abuse of illegal steroids (as well as by the dilution of pitching talent due to expansion), but the issue only began attracting significant media attention in 2002 and there was no penalty for the use of performance-enhancing drugs before 2004. In 2007, Bonds became MLB's all-time home run leader, surpassing Hank Aaron, as total major league and minor league attendance both reached all-time highs. Even though McGwire, Sosa, and Bonds—as well as many other players, including storied pitcher Roger Clemens—have been implicated in the steroid abuse scandal, their feats and those of other sluggers had become the major leagues' defining attraction. In contrast to the professional game's resurgence in popularity after the 1994 interruption, Little League enrollment was in decline: after peaking in 1996, it dropped 1 percent a year over the following decade. With more rigorous testing and penalties for performance-enhancing drug use a possible factor, the balance between bat and ball swung markedly in 2010, which became known as the "Year of the Pitcher". Runs per game fell to their lowest level in 18 years, and the strikeout rate was higher than it had been in half a century.Before the start of the 2012 season, MLB altered its rules to double the number of wild card teams admitted into the playoffs to two per league. The playoff expansion resulted in the addition of annual one-game playoffs between the wild card teams in each league. Baseball around the world Baseball, widely known as America's pastime, is well established in several other countries as well. The history of baseball in Canada has remained closely linked with that of the sport in the United States. As early as 1877, a professional league, the International Association, featured teams from both countries. While baseball is widely played in Canada and many minor league teams have been based in the country, the American major leagues did not include a Canadian club until 1969, when the Montreal Expos joined the National League as an expansion team. In 1977, the expansion Toronto Blue Jays joined the American League. The Blue Jays won the World Series in 1992 and 1993, the first and still the only club from outside the United States to do so. After the 2004 season, Major League Baseball relocated the Expos to Washington, D.C., where the team is now known as the Nationals.In 1847, American soldiers played what may have been the first baseball game in Mexico at Parque Los Berros in Xalapa, Veracruz. A few days after the Battle of Cerro Gordo, they used the "wooden leg captured (by the Fourth Illinois regiment) from General Santa Anna". The first formal baseball league outside of the United States and Canada was founded in 1878 in Cuba, which maintains a rich baseball tradition and whose national team has been one of the world's strongest since international play began in the late 1930s (all organized baseball in the country has officially been amateur since the Cuban Revolution). The Dominican Republic held its first islandwide championship tournament in 1912. Professional baseball tournaments and leagues began to form in other countries between the world wars, including the Netherlands (formed in 1922), Australia (1934), Japan (1936), Mexico (1937), and Puerto Rico (1938). The Japanese major leagues—the Central League and Pacific League—have long been considered the highest quality professional circuits outside of the United States. Japan has a professional minor league system as well, though it is much smaller than the American version—each team has only one farm club in contrast to MLB teams' four or five.After World War II, professional leagues were founded in many Latin American nations, most prominently Venezuela (1946) and the Dominican Republic (1955). Since the early 1970s, the annual Caribbean Series has matched the championship clubs from the four leading Latin American winter leagues: the Dominican Professional Baseball League, Mexican Pacific League, Puerto Rican Professional Baseball League, and Venezuelan Professional Baseball League. In Asia, South Korea (1982), Taiwan (1990), and China (2003) all have professional leagues.Many European countries have professional leagues as well, the most successful, other than the Dutch league, being the Italian league founded in 1948. Compared to those in Asia and Latin America, the various European leagues and the one in Australia historically have had no more than niche appeal. In 2004, Australia won a surprise silver medal at the Olympic Games. The Israel Baseball League, launched in 2007, folded after one season. The Confédération Européene de Baseball (European Baseball Confederation), founded in 1953, organizes a number of competitions between clubs from different countries, as well as national squads. Other competitions between national teams, such as the Baseball World Cup and the Olympic baseball tournament, were administered by the International Baseball Federation (IBAF) from its formation in 1938 until its 2013 merger with the International Softball Federation to create the current joint governing body for both sports, the World Baseball Softball Confederation (WBSC). By 2009, the IBAF had 117 member countries. Women's baseball is played on an organized amateur basis in many of the countries where it is a leading men's sport. Since 2004, the IBAF and now WBSC have sanctioned the Women's Baseball World Cup, featuring national teams.After being admitted to the Olympics as a medal sport beginning with the 1992 Games, baseball was dropped from the 2012 Summer Olympic Games at the 2005 International Olympic Committee meeting. It remained part of the 2008 Games. The elimination of baseball, along with softball, from the 2012 Olympic program enabled the IOC to consider adding two different sports, but none received the votes required for inclusion. While the sport's lack of a following in much of the world was a factor, more important was Major League Baseball's reluctance to have a break during the Games to allow its players to participate, as the National Hockey League now does during the Winter Olympic Games. Such a break is more difficult for MLB to accommodate because it would force the playoffs deeper into cold weather. Seeking reinstatement for the 2016 Summer Olympics, the IBAF proposed an abbreviated competition designed to facilitate the participation of top players, but the effort failed. Major League Baseball initiated the World Baseball Classic, scheduled to precede the major league season, partly as a replacement, high-profile international tournament. The inaugural Classic, held in March 2006, was the first tournament involving national teams to feature a significant number of MLB participants. The Baseball World Cup was discontinued after its 2011 edition in favor of an expanded World Baseball Classic. Rules and gameplay A game is played between two teams, each composed of nine players, that take turns playing offense (batting and baserunning) and defense (pitching and fielding). A pair of turns, one at bat and one in the field, by each team constitutes an inning. A game consists of nine innings (seven innings at the high school level and in doubleheaders in college and minor leagues). One team—customarily the visiting team—bats in the top, or first half, of every inning. The other team—customarily the home team—bats in the bottom, or second half, of every inning. The goal of the game is to score more points (runs) than the other team. The players on the team at bat attempt to score runs by circling or completing a tour of the four bases set at the corners of the square-shaped baseball diamond. A player bats at home plate and must proceed counterclockwise to first base, second base, third base, and back home in order to score a run. The team in the field attempts both to prevent runs from scoring and to record outs, which remove opposing players from offensive action until their turn in their team's batting order comes up again. When three outs are recorded, the teams switch roles for the next half-inning. If the score of the game is tied after nine innings, extra innings are played to resolve the contest. Many amateur games, particularly unorganized ones, involve different numbers of players and innings.The game is played on a field whose primary boundaries, the foul lines, extend forward from home plate at 45-degree angles. The 90-degree area within the foul lines is referred to as fair territory; the 270-degree area outside them is foul territory. The part of the field enclosed by the bases and several yards beyond them is the infield; the area farther beyond the infield is the outfield. In the middle of the infield is a raised pitcher's mound, with a rectangular rubber plate (the rubber) at its center. The outer boundary of the outfield is typically demarcated by a raised fence, which may be of any material and height (many amateur games are played on unfenced fields). Fair territory between home plate and the outfield boundary is baseball's field of play, though significant events can take place in foul territory, as well.There are three basic tools of baseball: the ball, the bat, and the glove or mitt:The baseball is about the size of an adult's fist, around 9 inches (23 centimeters) in circumference. It has a rubber or cork center, wound in yarn and covered in white cowhide, with red stitching.The bat is a hitting tool, traditionally made of a single, solid piece of wood. Other materials are now commonly used for nonprofessional games. It is a hard round stick, about 2.5 inches (6.4 centimeters) in diameter at the hitting end, tapering to a narrower handle and culminating in a knob. Bats used by adults are typically around 34 inches (86 centimeters) long, and not longer than 42 inches (106 centimeters).The glove or mitt is a fielding tool, made of padded leather with webbing between the fingers. As an aid in catching and holding onto the ball, it takes various shapes to meet the specific needs of different fielding positions.Protective helmets are also standard equipment for all batters.At the beginning of each half-inning, the nine players on the fielding team arrange themselves around the field. One of them, the pitcher, stands on the pitcher's mound. The pitcher begins the pitching delivery with one foot on the rubber, pushing off it to gain velocity when throwing toward home plate. Another player, the catcher, squats on the far side of home plate, facing the pitcher. The rest of the team faces home plate, typically arranged as four infielders—who set up along or within a few yards outside the imaginary lines between first, second, and third base—and three outfielders. In the standard arrangement, there is a first baseman positioned several steps to the left of first base, a second baseman to the right of second base, a shortstop to the left of second base, and a third baseman to the right of third base. The basic outfield positions are left fielder, center fielder, and right fielder. A neutral umpire sets up behind the catcher. Other umpires will be distributed around the field as well, though the number will vary depending on the level of play, amateur or children's games may only have an umpire behind the plate, while as many as six umpires can be used for important Major League Baseball games.Play starts with a batter standing at home plate, holding a bat. The batter waits for the pitcher to throw a pitch (the ball) toward home plate, and attempts to hit the ball with the bat. The catcher catches pitches that the batter does not hit—as a result of either electing not to swing or failing to connect—and returns them to the pitcher. A batter who hits the ball into the field of play must drop the bat and begin running toward first base, at which point the player is referred to as a runner (or, until the play is over, a batter-runner). A batter-runner who reaches first base without being put out (see below) is said to be safe and is now on base. A batter-runner may choose to remain at first base or attempt to advance to second base or even beyond—however far the player believes can be reached safely. A player who reaches base despite proper play by the fielders has recorded a hit. A player who reaches first base safely on a hit is credited with a single. If a player makes it to second base safely as a direct result of a hit, it is a double; third base, a triple. If the ball is hit in the air within the foul lines over the entire outfield (and outfield fence, if there is one), it is a home run: the batter and any runners on base may all freely circle the bases, each scoring a run. This is the most desirable result for the batter. A player who reaches base due to a fielding mistake is not credited with a hit—instead, the responsible fielder is charged with an error.Any runners already on base may attempt to advance on batted balls that land, or contact the ground, in fair territory, before or after the ball lands. A runner on first base must attempt to advance if a ball lands in play. If a ball hit into play rolls foul before passing through the infield, it becomes dead and any runners must return to the base they were at when the play began. If the ball is hit in the air and caught before it lands, the batter has flied out and any runners on base may attempt to advance only if they tag up or touch the base they were at when the play began, as or after the ball is caught. Runners may also attempt to advance to the next base while the pitcher is in the process of delivering the ball to home plate—a successful effort is a stolen base.A pitch that is not hit into the field of play is called either a strike or a ball. A batter against whom three strikes are recorded strikes out. A batter against whom four balls are recorded is awarded a base on balls or walk, a free advance to first base. (A batter may also freely advance to first base if the batter's body or uniform is struck by a pitch outside the strike zone, provided the batter does not swing and attempts to avoid being hit.) Crucial to determining balls and strikes is the umpire's judgment as to whether a pitch has passed through the strike zone, a conceptual area above home plate extending from the midpoint between the batter's shoulders and belt down to the hollow of the knee.A strike is called when one of the following happens:The batter lets a well-pitched ball (one within the strike zone) go through to the catcher.The batter swings at any ball (even one outside the strike zone) and misses, or foul tips it directly into the catcher's hands.The batter hits a foul ball—one that either initially lands in foul territory or initially lands within the diamond but moves into foul territory before passing first or third base. If there are already two strikes on the batter, a foul ball is not counted as a third strike; thus, a foul ball cannot result in the immediate strikeout of the batter. (There is an exception to this exception: a two-strike foul bunt is recorded as a third strike.)A ball is called when the pitcher throws a pitch that is outside the strike zone, provided the batter has not swung at it.While the team at bat is trying to score runs, the team in the field is attempting to record outs. Among the various ways a member of the batting team may be put out, five are most common:The strikeout: as described above, recorded against a batter who makes three strikes before putting the ball into play or being awarded a free advance to first base (see also uncaught third strike).The flyout: as described above, recorded against a batter who hits a ball in the air that is caught by a fielder, whether in fair territory or foul territory, before it lands, whether or not the batter has run.The ground out: recorded against a batter (in this case, batter-runner) who hits a ball that lands in fair territory which, before the batter-runner can reach first base, is retrieved by a fielder who touches first base while holding the ball or relays it to another fielder who touches first base while holding the ball.The force out: recorded against a runner who is required to attempt to advance—either because the runner is on first base and a batted ball lands in fair territory, or because the runner immediately behind on the basepath is thus required to attempt to advance—but fails to reach the next base before a fielder touches the base while holding the ball. The ground out is technically a special case of the force out.The tag out: recorded against a runner who is touched by a fielder with the ball or a glove holding the ball, while the runner is not touching a base.It is possible to record two outs in the course of the same play. This is called a double play. Even three outs in one play, a triple play, is possible, though this is very rare. Players put out or retired must leave the field, returning to their team's dugout or bench. A runner may be stranded on base when a third out is recorded against another player on the team. Stranded runners do not benefit the team in its next turn at bat as every half-inning begins with the bases empty of runners.An individual player's turn batting or plate appearance is complete when the player reaches base, hits a home run, makes an out, or hits a ball that results in the team's third out, even if it is recorded against a teammate. On rare occasions, a batter may be at the plate when, without the batter's hitting the ball, a third out is recorded against a teammate—for instance, a runner getting caught stealing (tagged out attempting to steal a base). A batter with this sort of incomplete plate appearance starts off the team's next turn batting; any balls or strikes recorded against the batter the previous inning are erased. A runner may circle the bases only once per plate appearance and thus can score at most a single run per batting turn. Once a player has completed a plate appearance, that player may not bat again until the eight other members of the player's team have all taken their turn at bat. The batting order is set before the game begins, and may not be altered except for substitutions. Once a player has been removed for a substitute, that player may not reenter the game. Children's games often have more liberal substitution rules.If the designated hitter (DH) rule is in effect, each team has a tenth player whose sole responsibility is to bat (and run). The DH takes the place of another player—almost invariably the pitcher—in the batting order, but does not field. Thus, even with the DH, each team still has a batting order of nine players and a fielding arrangement of nine players. Personnel  Player rosters Roster, or squad, sizes differ between different leagues and different levels of organized play. Major League Baseball teams maintain 25-player active rosters. A typical 25-man roster in a league without the DH rule, such as MLB's National League, features:eight position players—catcher, four infielders, three outfielders—who play on a regular basisfive starting pitchers who constitute the team's pitching rotation or starting rotationsix relief pitchers, including one specialist closer, who constitute the team's bullpen (named for the off-field area where pitchers warm up)one backup, or substitute, catchertwo backup infielderstwo backup outfieldersone specialist pinch hitter, or a second backup catcher, or a seventh relieverIn the American League and others with the DH rule, there will usually be nine offensive regulars (including the DH), five starting pitchers, seven or eight relievers, a backup catcher and two or three other reserves; the need for late inning pinch-hitters (usually in the pitcher's spot) is reduced by the DH. Other personnel The manager, or head coach of a team, oversees the team's major strategic decisions, such as establishing the starting rotation, setting the lineup, or batting order, before each game, and making substitutions during games—in particular, bringing in relief pitchers. Managers are typically assisted by two or more coaches; they may have specialized responsibilities, such as working with players on hitting, fielding, pitching, or strength and conditioning. At most levels of organized play, two coaches are stationed on the field when the team is at bat: the first base coach and third base coach, occupying designated coaches' boxes just outside the foul lines, assist in the direction of baserunners when the ball is in play, and relay tactical signals from the manager to batters and runners during pauses in play. In contrast to many other team sports, baseball managers and coaches generally wear their team's uniforms; coaches must be in uniform in order to be allowed on the playing field during a game.Any baseball game involves one or more umpires, who make rulings on the outcome of each play. At a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes. Additional umpires may be stationed near the other bases, thus making it easier to judge plays such as attempted force outs and tag outs. In Major League Baseball, four umpires are used for each game, one near each base. In the playoffs, six umpires are used: one at each base and two in the outfield along the foul lines. Strategy and tactics Many of the pre-game and in-game strategic decisions in baseball revolve around a fundamental fact: in general, right-handed batters tend to be more successful against left-handed pitchers and, to an even greater degree, left-handed batters tend to be more successful against right-handed pitchers. A manager with several left-handed batters in the regular lineup who knows the team will be facing a left-handed starting pitcher may respond by starting one or more of the right-handed backups on the team's roster. During the late innings of a game, as relief pitchers and pinch hitters are brought in, the opposing managers will often go back and forth trying to create favorable matchups with their substitutions: the manager of the fielding team trying to arrange same-handed pitcher-batter matchups, the manager of the batting team trying to arrange opposite-handed matchups. With a team that has the lead in the late innings, a manager may remove a starting position player—especially one whose turn at bat is not likely to come up again—for a more skillful fielder. Pitching and fielding tactics The tactical decision that precedes almost every play in a baseball game involves pitch selection. By gripping and then releasing the baseball in a certain manner, and by throwing it at a certain speed, pitchers can cause the baseball to break to either side, or downward, as it approaches the batter. Among the resulting wide variety of pitches that may be thrown, the four basic types are the fastball, the changeup (or off-speed pitch), and two breaking balls—the curveball and the slider. Pitchers have different repertoires of pitches they are skillful at throwing. Conventionally, before each pitch, the catcher signals the pitcher what type of pitch to throw, as well as its general vertical and/or horizontal location. If there is disagreement on the selection, the pitcher may shake off the sign and the catcher will call for a different pitch. With a runner on base and taking a lead, the pitcher may attempt a pickoff, a quick throw to a fielder covering the base to keep the runner's lead in check or, optimally, effect a tag out. Pickoff attempts, however, are subject to rules that severely restrict the pitcher's movements before and during the pickoff attempt. Violation of any one of these rules could result in the umpire calling a balk against the pitcher, with the result being runners on base, if any, advance one base with impunity. If an attempted stolen base is anticipated, the catcher may call for a pitchout, a ball thrown deliberately off the plate, allowing the catcher to catch it while standing and throw quickly to a base. Facing a batter with a strong tendency to hit to one side of the field, the fielding team may employ a shift, with most or all of the fielders moving to the left or right of their usual positions. With a runner on third base, the infielders may play in, moving closer to home plate to improve the odds of throwing out the runner on a ground ball, though a sharply hit grounder is more likely to carry through a drawn-in infield. Batting and baserunning tactics Several basic offensive tactics come into play with a runner on first base, including the fundamental choice of whether to attempt a steal of second base. The hit and run is sometimes employed with a skillful contact hitter: the runner takes off with the pitch drawing the shortstop or second baseman over to second base, creating a gap in the infield for the batter to poke the ball through. The sacrifice bunt calls for the batter to focus on making contact with the ball so that it rolls a short distance into the infield, allowing the runner to advance into scoring position even at the expense of the batter being thrown out at first—a batter who succeeds is credited with a sacrifice. (A batter, particularly one who is a fast runner, may also attempt to bunt for a hit.) A sacrifice bunt employed with a runner on third base, aimed at bringing that runner home, is known as a squeeze play. With a runner on third and fewer than two outs, a batter may instead concentrate on hitting a fly ball that, even if it is caught, will be deep enough to allow the runner to tag up and score—a successful batter in this case gets credit for a sacrifice fly. The manager will sometimes signal a batter who is ahead in the count (i.e., has more balls than strikes) to take, or not swing at, the next pitch. Distinctive elements Baseball has certain attributes that set it apart from the other popular team sports in the countries where it has a following, including American and Canadian football, basketball, ice hockey, and soccer. All of these sports use a clock; in all of them, play is less individual and more collective; and in none of them is the variation between playing fields nearly as substantial or important. The comparison between cricket and baseball demonstrates that many of baseball's distinctive elements are shared in various ways with its cousin sports. No clock to kill In clock-limited sports, games often end with a team that holds the lead killing the clock rather than competing aggressively against the opposing team. In contrast, baseball has no clock; a team cannot win without getting the last batter out and rallies are not constrained by time. At almost any turn in any baseball game, the most advantageous strategy is some form of aggressive strategy. In contrast, again, the clock comes into play even in the case of multi-day Test and first-class cricket: the possibility of a draw often encourages a team that is batting last and well behind to bat defensively, giving up any faint chance at a win to avoid a loss. Baseball offers no such reward for conservative batting.While nine innings has been the standard since the beginning of professional baseball, the duration of the average major league game has increased steadily through the years. At the turn of the 20th century, games typically took an hour and a half to play. In the 1920s, they averaged just less than two hours, which eventually ballooned to 2:38 in 1960. By 1997, the average American League game lasted 2:57 (National League games were about 10 minutes shorter—pitchers at the plate making for quicker outs than designated hitters). In 2004, Major League Baseball declared that its goal was an average game of merely 2:45. By 2014, though, the average MLB game took over three hours to complete. The lengthening of games is attributed to longer breaks between half-innings for television commercials, increased offense, more pitching changes, and a slower pace of play with pitchers taking more time between each delivery, and batters stepping out of the box more frequently. Other leagues have experienced similar issues. In 2008, Nippon Professional Baseball took steps aimed at shortening games by 12 minutes from the preceding decade's average of 3:18. Individual focus Although baseball is a team sport, individual players are often placed under scrutiny and pressure. In 1915, a baseball instructional manual pointed out that every single pitch, of which there are often more than two hundred in a game, involves an individual, one-on-one contest: "the pitcher and the batter in a battle of wits". Contrasting the game with both football and basketball, scholar Michael Mandelbaum argues that "baseball is the one closest in evolutionary descent to the older individual sports". Pitcher, batter, and fielder all act essentially independent of each other. While coaching staffs can signal pitcher or batter to pursue certain tactics, the execution of the play itself is a series of solitary acts. If the batter hits a line drive, the outfielder is solely responsible for deciding to try to catch it or play it on the bounce and for succeeding or failing. The statistical precision of baseball is both facilitated by this isolation and reinforces it. As described by Mandelbaum,It is impossible to isolate and objectively assess the contribution each [football] team member makes to the outcome of the play ... [E]very basketball player is interacting with all of his teammates all the time. In baseball, by contrast, every player is more or less on his own ... Baseball is therefore a realm of complete transparency and total responsibility. A baseball player lives in a glass house, and in a stark moral universe ... Everything that every player does is accounted for and everything accounted for is either good or bad, right or wrong.Cricket is more similar to baseball than many other team sports in this regard: while the individual focus in cricket is mitigated by the importance of the batting partnership and the practicalities of tandem running, it is enhanced by the fact that a batsman may occupy the wicket for an hour or much more. There is no statistical equivalent in cricket for the fielding error and thus less emphasis on personal responsibility in this area of play. Uniqueness of each baseball park Unlike those of most sports, baseball playing fields can vary significantly in size and shape. While the dimensions of the infield are specifically regulated, the only constraint on outfield size and shape for professional teams following the rules of Major League and Minor League Baseball is that fields built or remodeled since June 1, 1958, must have a minimum distance of 325 feet (99 m) from home plate to the fences in left and right field and 400 feet (122 m) to center. Major league teams often skirt even this rule. For example, at Minute Maid Park, which became the home of the Houston Astros in 2000, the Crawford Boxes in left field are only 315 feet (96 m) from home plate. There are no rules at all that address the height of fences or other structures at the edge of the outfield. The most famously idiosyncratic outfield boundary is the left-field wall at Boston's Fenway Park, in use since 1912: the Green Monster is 310 feet (94 m) from home plate down the line and 37 feet (11 m) tall.Similarly, there are no regulations at all concerning the dimensions of foul territory. Thus a foul fly ball may be entirely out of play in a park with little space between the foul lines and the stands, but a foulout in a park with more expansive foul ground. A fence in foul territory that is close to the outfield line will tend to direct balls that strike it back toward the fielders, while one that is farther away may actually prompt more collisions, as outfielders run full speed to field balls deep in the corner. These variations can make the difference between a double and a triple or inside-the-park home run. The surface of the field is also unregulated. While the adjacent image shows a traditional field surfacing arrangement (and the one used by virtually all MLB teams with naturally surfaced fields), teams are free to decide what areas will be grassed or bare. Some fields—including several in MLB—use an artificial surface, such as AstroTurf. Surface variations can have a significant effect on how ground balls behave and are fielded as well as on baserunning. Similarly, the presence of a roof (seven major league teams play in stadiums with permanent or retractable roofs) can greatly affect how fly balls are played. While football and soccer players deal with similar variations of field surface and stadium covering, the size and shape of their fields are much more standardized. The area out-of-bounds on a football or soccer field does not affect play the way foul territory in baseball does, so variations in that regard are largely insignificant.These physical variations create a distinctive set of playing conditions at each ballpark. Other local factors, such as altitude and climate, can also significantly affect play. A given stadium may acquire a reputation as a pitcher's park or a hitter's park, if one or the other discipline notably benefits from its unique mix of elements. The most exceptional park in this regard is Coors Field, home of the Colorado Rockies. Its high altitude—5,282 feet (1,610 m) above sea level—is responsible for giving it the strongest hitter's park effect in the major leagues. Wrigley Field, home of the Chicago Cubs, is known for its fickle disposition: a hitter's park when the strong winds off Lake Michigan are blowing out, it becomes more of a pitcher's park when they are blowing in. The absence of a standardized field affects not only how particular games play out, but the nature of team rosters and players' statistical records. For example, hitting a fly ball 330 feet (100 m) into right field might result in an easy catch on the warning track at one park, and a home run at another. A team that plays in a park with a relatively short right field, such as the New York Yankees, will tend to stock its roster with left-handed pull hitters, who can best exploit it. On the individual level, a player who spends most of his career with a team that plays in a hitter's park will gain an advantage in batting statistics over time—even more so if his talents are especially suited to the park. Statistics Organized baseball lends itself to statistics to a greater degree than many other sports. Each play is discrete and has a relatively small number of possible outcomes. In the late 19th century, a former cricket player, English-born Henry Chadwick of Brooklyn, New York, was responsible for the "development of the box score, tabular standings, the annual baseball guide, the batting average, and most of the common statistics and tables used to describe baseball." The statistical record is so central to the game's "historical essence" that Chadwick came to be known as Father Baseball. In the 1920s, American newspapers began devoting more and more attention to baseball statistics, initiating what journalist and historian Alan Schwarz describes as a "tectonic shift in sports, as intrigue that once focused mostly on teams began to go to individual players and their statistics lines."The Official Baseball Rules administered by Major League Baseball require the official scorer to categorize each baseball play unambiguously. The rules provide detailed criteria to promote consistency. The score report is the official basis for both the box score of the game and the relevant statistical records. General managers, managers, and baseball scouts use statistics to evaluate players and make strategic decisions.Certain traditional statistics are familiar to most baseball fans. The basic batting statistics include:At bats: plate appearances, excluding walks and hit by pitches—where the batter's ability is not fully tested—and sacrifices and sacrifice flies—where the batter intentionally makes an out in order to advance one or more baserunnersHits: times reached base because of a batted, fair ball without fielding error or fielder's choiceRuns: times circling the bases and reaching home safelyRuns batted in (RBIs): number of runners who scored due to a batter's action (including the batter, in the case of a home run), except when batter grounded into double play or reached on an errorHome runs: hits on which the batter successfully touched all four bases, without the contribution of a fielding errorBatting average: hits divided by at bats—the traditional measure of batting abilityThe basic baserunning statistics include:Stolen bases: times advancing to the next base entirely due to the runner's own efforts, generally while the pitcher is preparing to deliver or delivering the ballCaught stealing: times tagged out while attempting to steal a baseThe basic pitching statistics include:Wins: credited to pitcher on winning team who last pitched before the team took a lead that it never relinquished (a starting pitcher must pitch at least five innings to qualify for a win)Losses: charged to pitcher on losing team who was pitching when the opposing team took a lead that it never relinquishedSaves: games where the pitcher enters a game led by the pitcher's team, finishes the game without surrendering the lead, is not the winning pitcher, and either (a) the lead was three runs or less when the pitcher entered the game; (b) the potential tying run was on base, at bat, or on deck; or (c) the pitcher pitched three or more inningsInnings pitched: outs recorded while pitching divided by three (partial innings are conventionally recorded as, e.g., "5.2" or "7.1", the last digit actually representing thirds, not tenths, of an inning)Strikeouts: times pitching three strikes to a batterWinning percentage: wins divided by decisions (wins plus losses)Earned run average (ERA): runs allowed, excluding those resulting from fielding errors, per nine innings pitchedThe basic fielding statistics include:Putouts: times the fielder catches a fly ball, tags or forces out a runner, or otherwise directly effects an outAssists: times a putout by another fielder was recorded following the fielder touching the ballErrors: times the fielder fails to make a play that should have been made with common effort, and the batting team benefits as a resultTotal chances: putouts plus assists plus errorsFielding average: successful chances (putouts plus assists) divided by total chancesAmong the many other statistics that are kept are those collectively known as situational statistics. For example, statistics can indicate which specific pitchers a certain batter performs best against. If a given situation statistically favors a certain batter, the manager of the fielding team may be more likely to change pitchers or have the pitcher intentionally walk the batter in order to face one who is less likely to succeed. Sabermetrics Sabermetrics refers to the field of baseball statistical study and the development of new statistics and analytical tools. The term is also used to refer directly to new statistics themselves. The term was coined around 1980 by one of the field's leading proponents, Bill James, and derives from the Society for American Baseball Research (SABR).The growing popularity of sabermetrics since the early 1980s has brought more attention to two batting statistics that sabermetricians argue are much better gauges of a batter's skill than batting average:On-base percentage measures a batter's ability to get on base. It is calculated by taking the sum of the batter's successes in getting on base (hits plus walks plus hit by pitches) and dividing that by the batter's total plate appearances (at bats plus walks plus hit by pitches plus sacrifice flies), except for sacrifice bunts.Slugging percentage measures a batter's ability to hit for power. It is calculated by taking the batter's total bases (one per each single, two per double, three per triple, and four per home run) and dividing that by the batter's at bats.Some of the new statistics devised by sabermetricians have gained wide use:On-base plus slugging (OPS) measures a batter's overall ability. It is calculated by adding the batter's on-base percentage and slugging percentage.Walks plus hits per inning pitched (WHIP) measures a pitcher's ability at preventing hitters from reaching base. It is calculated exactly as its name suggests. Popularity and cultural impact Writing in 1919, philosopher Morris Raphael Cohen described baseball as America's national religion. In the words of sports columnist Jayson Stark, baseball has long been "a unique paragon of American culture"—a status he sees as devastated by the steroid abuse scandal. Baseball has an important place in other national cultures as well: Scholar Peter Bjarkman describes "how deeply the sport is ingrained in the history and culture of a nation such as Cuba, [and] how thoroughly it was radically reshaped and nativized in Japan." Since the early 1980s, the Dominican Republic, in particular the city of San Pedro de Macorís, has been the major leagues' primary source of foreign talent. Hall-of-Famer Roberto Clemente remains one of the greatest national heroes in Puerto Rico's history. While baseball has long been the island's primary athletic pastime, its once well-attended professional winter league has declined in popularity since 1990, when young Puerto Rican players began to be included in the major leagues' annual first-year player draft. In the Western Hemisphere, baseball is also one of the leading sports in Canada, Colombia, Mexico, the Netherlands Antilles, Nicaragua, Panama, and Venezuela. In Asia, it is among the most popular sports in Japan, South Korea and Taiwan.The major league game in the United States was originally targeted toward a middle-class, white-collar audience: relative to other spectator pastimes, the National League's set ticket price of 50 cents in 1876 was high, while the location of playing fields outside the inner city and the workweek daytime scheduling of games were also obstacles to a blue-collar audience. A century later, the situation was very different. With the rise in popularity of other team sports with much higher average ticket prices—football, basketball, and hockey—professional baseball had become among the most blue-collar-oriented of leading American spectator sports.In the late 1900s and early 2000s, baseball's position compared to football in the United States moved in contradictory directions. In 2008, Major League Baseball set a revenue record of $6.5 billion, matching the NFL's revenue for the first time in decades. A new MLB revenue record of $6.6 billion was set in 2009. On the other hand, the percentage of American sports fans polled who named baseball as their favorite sport was 16%, compared to pro football at 31%. In 1985, the respective figures were pro football 24%, baseball 23%. Because there are so many more major league baseball games played, there is no comparison in overall attendance. In 2008, total attendance at major league games was the second-highest in history: 78.6 million, 0.7% off the record set the previous year. The following year, amid the U.S. recession, attendance fell by 6.6% to 73.4 million. Attendance at games held under the Minor League Baseball umbrella also set a record in 2007, with 42.8 million; this figure does not include attendance at games of the several independent minor leagues.In Japan, where baseball is inarguably the leading spectator team sport, combined revenue for the twelve teams in Nippon Professional Baseball (NPB), the body that oversees both the Central and Pacific leagues, was estimated at $1 billion in 2007. Total NPB attendance for the year was approximately 20 million. While in the preceding two decades, MLB attendance grew by 50 percent and revenue nearly tripled, the comparable NPB figures were stagnant. There are concerns that MLB's growing interest in acquiring star Japanese players will hurt the game in their home country. In Cuba, where baseball is by every reckoning the national sport, the national team overshadows the city and provincial teams that play in the top-level domestic leagues. Revenue figures are not released for the country's amateur system. Similarly, according to one official pronouncement, the sport's governing authority "has never taken into account attendance ... because its greatest interest has always been the development of athletes".As of 2007, Little League Baseball oversees more than 7,000 children's baseball leagues with more than 2.2 million participants–2.1 million in the United States and 123,000 in other countries. Babe Ruth League teams have over 1 million participants. According to the president of the International Baseball Federation, between 300,000 and 500,000 women and girls play baseball around the world, including Little League and the introductory game of Tee Ball.A varsity baseball team is an established part of physical education departments at most high schools and colleges in the United States. In 2008, nearly half a million high schoolers and over 35,000 collegians played on their schools' baseball teams. The number of Americans participating in baseball has declined since the late 1980s, falling well behind the number of soccer participants. By early in the 20th century, intercollegiate baseball was Japan's leading sport. Today, high school baseball in particular is immensely popular there. The final rounds of the two annual tournaments—the National High School Baseball Invitational Tournament in the spring, and the even more important National High School Baseball Championship in the summer—are broadcast around the country. The tournaments are known, respectively, as Spring Koshien and Summer Koshien after the 55,000-capacity stadium where they are played. In Cuba, baseball is a mandatory part of the state system of physical education, which begins at age six. Talented children as young as seven are sent to special district schools for more intensive training—the first step on a ladder whose acme is the national baseball team. Baseball in popular culture Baseball has had a broad impact on popular culture, both in the United States and elsewhere. Dozens of English-language idioms have been derived from baseball; in particular, the game is the source of a number of widely used sexual euphemisms. The first networked radio broadcasts in North America were of the 1922 World Series: famed sportswriter Grantland Rice announced play-by-play from New York City's Polo Grounds on WJZ–Newark, New Jersey, which was connected by wire to WGY–Schenectady, New York, and WBZ–Springfield, Massachusetts. The baseball cap has become a ubiquitous fashion item not only in the United States and Japan, but also in countries where the sport itself is not particularly popular, such as the United Kingdom.Baseball has inspired many works of art and entertainment. One of the first major examples, Ernest Thayer's poem "Casey at the Bat", appeared in 1888. A wry description of the failure of a star player in what would now be called a "clutch situation", the poem became the source of vaudeville and other staged performances, audio recordings, film adaptations, and an opera, as well as a host of sequels and parodies in various media. There have been many baseball movies, including the Academy Award–winning The Pride of the Yankees (1942) and the Oscar nominees The Natural (1984) and Field of Dreams (1989). The American Film Institute's selection of the ten best sports movies includes The Pride of the Yankees at number 3 and Bull Durham (1988) at number 5. Baseball has provided thematic material for hits on both stage—the Adler–Ross musical Damn Yankees—and record—George J. Gaskin's "Slide, Kelly, Slide", Simon and Garfunkel's "Mrs. Robinson", and John Fogerty's "Centerfield". The baseball-inspired comedic sketch "Who's on First", popularized by Abbott and Costello in 1938, quickly became famous. Six decades later, Time named it the best comedy routine of the 20th century. Baseball is also featured in various video games including MLB: The Show, Wii Sports, Kinect Sports: Season 2 and Mario Baseball.Literary works connected to the game include the short fiction of Ring Lardner and novels such as Bernard Malamud's The Natural (the source for the movie), Robert Coover's The Universal Baseball Association, Inc., J. Henry Waugh, Prop., and W. P. Kinsella's Shoeless Joe (the source for Field of Dreams). Baseball's literary canon also includes the beat reportage of Damon Runyon; the columns of Grantland Rice, Red Smith, Dick Young, and Peter Gammons; and the essays of Roger Angell. Among the celebrated nonfiction books in the field are Lawrence S. Ritter's The Glory of Their Times, Roger Kahn's The Boys of Summer, and Michael Lewis's Moneyball. The 1970 publication of major league pitcher Jim Bouton's tell-all chronicle Ball Four is considered a turning point in the reporting of professional sports.Baseball has also inspired the creation of new cultural forms. Baseball cards were introduced in the late 19th century as trade cards. A typical example would feature an image of a baseball player on one side and advertising for a business on the other. In the early 1900s they were produced widely as promotional items by tobacco and confectionery companies. The 1930s saw the popularization of the modern style of baseball card, with a player photograph accompanied on the rear by statistics and biographical data. Baseball cards—many of which are now prized collectibles—are the source of the much broader trading card industry, involving similar products for different sports and non-sports-related fields.Modern fantasy sports began in 1980 with the invention of Rotisserie League Baseball by New York writer Daniel Okrent and several friends. Participants in a Rotisserie league draft notional teams from the list of active Major League Baseball players and play out an entire imaginary season with game outcomes based on the players' latest real-world statistics. Rotisserie-style play quickly became a phenomenon. Now known more generically as fantasy baseball, it has inspired similar games based on an array of different sports. The field boomed with increasing Internet access and new fantasy sports–related websites. By 2008, 29.9 million people in the United States and Canada were playing fantasy sports, spending $800 million on the hobby. The burgeoning popularity of fantasy baseball is also credited with the increasing attention paid to sabermetrics—first among fans, only later among baseball professionals. See also Baseball awardsBaseball clothing and equipmentList of organized baseball leaguesList of Major League Baseball single-game recordsOutline of baseballRelated sportsBrännboll (Scandinavian bat-and-ball game)British baseballLapta (game) (Russian bat-and-ball game)Oină (Romanian bat-and-ball game)Pesäpallo ("Finnish baseball")SoftballStickballStoop ballWiffleballCricket References  Sources  Further reading Bradbury, J.C. The Baseball Economist: The Real Game Exposed (Dutton, 2007). ISBN 0-525-94993-3Dickson, Paul. The Dickson Baseball Dictionary, 3d ed. (W. W. Norton, 2009). ISBN 0-393-06681-9Elias, Robert (2010) The Empire Strikes Out: How Baseball Sold U.S. Foreign Policy and Promoted the American Way Abroad. New York: The New Press. ISBN 978-1-59558-195-2Elliott, Bob. The Northern Game: Baseball the Canadian Way (Sport Classic, 2005). ISBN 1-894963-40-7Euchner, Charles. The Last Nine Innings: Inside the Real Game Fans Never See (Sourcebooks, 2007). ISBN 1-4022-0579-1Fitts, Robert K. Remembering Japanese Baseball: An Oral History of the Game (Southern Illinois University Press, 2005). ISBN 0-8093-2629-9Gutkind, Lee. The Best Seat in Baseball, But You Have to Stand: The Game as Umpires See It (Southern Illinois University Press, 1999). ISBN 978-0-8093-2195-7Gillette, Gary, and Pete Palmer (eds.). The ESPN Baseball Encyclopedia, 5th ed. (Sterling, 2008). ISBN 1-4027-6051-5James, Bill. The New Bill James Historical Baseball Abstract, rev. ed. (Simon and Schuster, 2003). ISBN 0-7432-2722-0James, Bill. The Bill James Handbook 2009 (ACTA, 2008). ISBN 0-87946-367-8Mahony, Phillip, Baseball Explained (McFarland Books, 2014) ISBN 978-0-7864-7964-1.Pepe, Phil. (2005). Catfish, Yaz, and Hammerin' Hank: The Unforgettable Era That Transformed Baseball. Triumph Books; Chicago. ISBN 978-1-57243-839-2Peterson, Robert. Only the Ball was White: A History of Legendary Black Players and All-Black Professional Teams (Oxford University Press, 1992 [1970]). ISBN 0-19-507637-0Posnanski, Joe (2007) The Soul of Baseball New York: Harper Collins. ISBN 978-0-06-085403-4Reaves, Joseph A. Taking in a Game: A History of Baseball in Asia (Bison, 2004). ISBN 0-8032-3943-2Ritter, Lawrence S. The Glory of Their Times: The Story of the Early Days of Baseball Told by the Men Who Played It, enlarged ed. (Harper, 1992). ISBN 0-688-11273-0Tango, Tom, Mitchel G. Lichtman, and Andrew E. Dolphin, The Book: Playing the Percentages in Baseball (Potomac, 2007). ISBN 1-59797-129-4Sexton, John (2013) Baseball as a Road to God: Seeing Beyond the Game New York: Gotham Books. ISBN 978-1-59240-754-5.Ward, Geoffrey C., and Ken Burns. Baseball: An Illustrated History (Alfred A. Knopf, 1996). ISBN 0-679-40459-7OnlineBoswell, Thomas (January 18, 1987). "Why Is Baseball So Much Better Than Football?". Washington Post. Baseball Almanac. Archived from the original on April 23, 2009. Retrieved 2009-05-06. Carlin, George. "Baseball and Football". Baseball Almanac. Retrieved 2009-05-06. Gmelch, George (September 2000). "Baseball Magic". McGraw Hill–Dushkin. Archived from the original on April 22, 2007. Retrieved 2009-09-25. Lamster, Mark (April 10, 2005). "Baseball Before We Knew It: What's the French for 'Juiced'? (book review)". New York Times. Retrieved 2014-06-27.  External links Leagues and organizationsMajor League BaseballInternational Baseball FederationMinor League BaseballBritish Baseball FederationStatistics and game recordsBaseball AlmanacBaseball-Reference.comRetrosheetNews and other resources"Baseball". Encyclopædia Britannica Online.Baseball at DMOZBaseballLibrary.comBaseball ProspectusSociety for American Baseball ResearchMister Baseball European baseball newsBaseball Heritage Museum at League Park in Cleveland, Ohio
Memory is the faculty of the mind by which information is encoded, stored, and retrieved (Atkinson & Shiffrin, 1968). Memory is vital to experiences and related to limbic systems, it is the retention of information over time for the purpose of influencing future action. If we could not remember past events, we could not learn or develop language, relationships, nor personal identity (Eysenck, 2012).Often memory is understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory (Baddely, 2007). The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to with various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store data through various categorical models or systems (Baddely, 2007).Explicit and implicit functions of memory are also known as declarative and non-declarative systems (Squire, 2009). These systems involve the purposeful intention of memory retrieval and storage, or lack thereof. Declarative, or explicit, memory is the conscious storage and recollection of data (Graf & Schacter, 1985). Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning (Eysenck, 2012), while episodic memory refers to information that is encoded along a spatial and temporal plane (Schacter & Addis, 2007; Szpunar, 2010). Declarative memory is usually the primary process thought of when referencing memory (Eysenck, 2012).Non-declarative, or implicit, memory is the unconscious storage and recollection of information (Foerde & Poldrack, 2009). An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon (Eysenck, 2012; Foerde & Poldrack, 2009; Tulving & Schacter, 1990). Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated (Tulving & Schacter, 1990), whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning (Eysenck, 2012; Foerde & Poldrack, 2009).Memory is not a perfect processor, and is affected by many factors. The manner information is encoded, stored, and retrieved can all be corrupted. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage (Eysenck, 2012). Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus (Squire, 2009). Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory (Eysenck, 2012). Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of memory.From an information processing perspective there are three main stages in the formation and retrieval of memory:Encoding or registration: receiving, processing and combining of received informationStorage: creation of a record of the encoded information in short-term or long-term memoryRetrieval, recall or recollection: calling back the stored information in response to some cue for use in a process or activityThe loss of memory is described as forgetfulness or amnesia. Sensory memory Sensory memory holds sensory information less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is the example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to "see" more than they can actually report. The first experiments exploring this form of sensory memory were precisely conducted by George Sperling (1963) using the "partial report paradigm". Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments,Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the "whole report" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal.Three types of sensory memories exist. Iconic memory is a fast decaying store of visual information; a type of sensory memory that briefly stores an image which has been perceived for a small duration. Echoic memory is a fast decaying store of auditory information, another type of sensory memory that briefly stores sounds that have been perceived for short durations. Haptic memory is a type of sensory memory that represents a database for touch stimuli. Short-term memory Short-term memory is also known as working memory. Short-term memory allows recall for a period of several seconds to a minute without rehearsal. Its capacity is also very limited: George A. Miller (1956), when working at Bell Laboratories, conducted experiments showing that the store of short-term memory was 7±2 items (the title of his famous paper, "The magical number 7±2"). Modern estimates of the capacity of short-term memory are lower, typically of the order of 4–5 items; however, memory capacity can be increased through a process called chunking. For example, in recalling a ten-digit telephone number, a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456) and lastly a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This may be reflected in some countries in the tendency to display telephone numbers as several chunks of two to four numbers.Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent a visual code. Conrad (1964) found that test subjects had more difficulty recalling collections of letters that were acoustically similar (e.g. E, P, D). Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text; thus, while memory of written language may rely on acoustic components, generalisations to all forms of memory cannot be made. Long-term memory The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number we may remember it for only a few seconds before forgetting, suggesting it was stored in our short-term memory. On the other hand, we can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, "which attempts to capture information such as 'what', 'when' and 'where'". With episodic memory, individuals are able to recall specific events such as birthday parties and weddings.Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short attention span, as first gleaned from patient Henry Molaison  after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning.Research has suggested that long-term memory storage in humans may be maintained by DNA methylation, or prions. Models Models of memory provide abstract representations of how memory is believed to work. Below are several models proposed over the years by various psychologists. Controversy is involved as to whether several memory structures exist. Atkinson–Shiffrin The multi-store model (also known as Atkinson–Shiffrin memory model) was first described in 1968 by Atkinson and Shiffrin.The multi-store model has been criticised for being too simplistic. For instance, long-term memory is believed to be actually made up of multiple subcomponents, such as episodic and procedural memory. It also proposes that rehearsal is the only mechanism by which information eventually reaches long-term storage, but evidence shows us capable of remembering things without rehearsal.The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged, displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Interestingly, visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory. Working memory In 1974 Baddeley and Hitch proposed a "working memory model" that replaced the general concept of short-term memory with an active maintenance of information in the short-term storage. In this model, working memory consists of three basic stores: the central executive, the phonological loop and the visuo-spatial sketchpad. In 2000 this model was expanded with the multimodal episodic buffer (Baddeley's model of working memory).The central executive essentially acts as an attention sensory store. It channels information to the three component processes: the phonological loop, the visuo-spatial sketchpad, and the episodic buffer.The phonological loop stores auditory information by silently rehearsing sounds or words in a continuous loop: the articulatory process (for example the repetition of a telephone number over and over again). A short list of data is easier to remember.The visuospatial sketchpad stores visual and spatial information. It is engaged when performing spatial tasks (such as judging distances) or visual ones (such as counting the windows on a house or imagining images).The episodic buffer is dedicated to linking information across domains to form integrated units of visual, spatial, and verbal information and chronological ordering (e.g., the memory of a story or a movie scene). The episodic buffer is also assumed to have links to long-term memory and semantical meaning.The working memory model explains many practical observations, such as why it is easier to do two different tasks (one verbal and one visual) than two similar tasks (e.g., two visual), and the aforementioned word-length effect. However, the concept of a central executive as noted here has been criticised as inadequate and vague. Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics. Types Researchers distinguish between recognition and recall memory. Recognition memory tasks require individuals to indicate whether they have encountered a stimulus (such as a picture or a word) before. Recall memory tasks require participants to retrieve previously learned information. For example, individuals might be asked to produce a series of actions they have seen before or to say a list of words they have heard before. By information type Topographic memory involves the ability to orient oneself in space, to recognize and follow an itinerary, or to recognize familiar places. Getting lost when traveling alone is an example of the failure of topographic memory.Flashbulb memories are clear episodic memories of unique and highly emotional events. People remembering where they were or what they were doing when they first heard the news of President Kennedy's assassination, the Sydney Siege or of 9/11 are examples of flashbulb memories.Anderson (1976) divides long-term memory into declarative (explicit) and procedural (implicit) memories. Declarative Declarative memory requires conscious recall, in that some conscious process must call back the information. It is sometimes called explicit memory, since it consists of information that is explicitly stored and retrieved.Declarative memory can be further sub-divided into semantic memory, concerning principles and facts taken independent of context; and episodic memory, concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as "Paris is the capital of France". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the "firsts" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Autobiographical memory - memory for particular events within one's own life - is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image. Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon. Procedural In contrast, procedural memory (or implicit memory) is not based on the conscious recall of information, but on implicit learning. It can best be summarized as remembering how to do something. Procedural memory is primarily employed in learning motor skills and should be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition - no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia.A characteristic of procedural memory is that the things remembered are automatically translated into actions, and thus sometimes difficult to describe. Some examples of procedural memory include the ability to ride a bike or tie shoelaces. By temporal direction Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory, or in the future, prospective memory. Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory. Study techniques  To assess infants Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children's memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants' recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants' recognition memory and the deferred and elicited imitation techniques have been used to assess infants' recall memory.Techniques used to assess infants' recognition memory include the following:Visual paired comparison procedure (relies on habituation): infants are first presented with pairs of visual stimuli, such as two black-and-white photos of human faces, for a fixed amount of time; then, after being familiarized with the two photos, they are presented with the "familiar" photo and a new photo. The time spent looking at each photo is recorded. Looking longer at the new photo indicates that they remember the "familiar" one. Studies using this procedure have found that 5- to 6-month-olds can retain information for as long as fourteen days.Operant conditioning technique: infants are placed in a crib and a ribbon that is connected to a mobile overhead is tied to one of their feet. Infants notice that when they kick their foot the mobile moves – the rate of kicking increases dramatically within minutes. Studies using this technique have revealed that infants' memory substantially improves over the first 18-months. Whereas 2- to 3-month-olds can retain an operant response (such as activating the mobile by kicking their foot) for a week, 6-month-olds can retain it for two weeks, and 18-month-olds can retain a similar operant response for as long as 13 weeks.Techniques used to assess infants' recall memory include the following:Deferred imitation technique: an experimenter shows infants a unique sequence of actions (such as using a stick to push a button on a box) and then, after a delay, asks the infants to imitate the actions. Studies using deferred imitation have shown that 14-month-olds' memories for the sequence of actions can last for as long as four months.Elicited imitation technique: is very similar to the deferred imitation technique; the difference is that infants are allowed to imitate the actions before the delay. Studies using the elicited imitation technique have shown that 20-month-olds can recall the action sequences twelve months later. To assess older children and adults Researchers use a variety of tasks to assess older children and adults' memory. Some examples are:Paired associate learning – when one learns to associate one specific word with another. For example, when given a word such as "safe" one must learn to say another specific word, such as "green". This is stimulus and response.Free recall – during this task a subject would be asked to study a list of words and then later they will be asked to recall or write down as many words that they can remember, similar to free response questions. Earlier items are affected by retroactive interference (RI), which means the longer the list, the greater the interference, and the less likelihood that they are recalled. On the other hand, items that have been presented lastly suffer little RI, but suffer a great deal from proactive interference (PI), which means the longer the delay in recall, the more likely that the items will be lost.Cued recall – one is given significant hints about the information. This is similar to fill in the blank assessments used in classrooms.Recognition – subjects are asked to remember a list of words or pictures, after which point they are asked to identify the previously presented words or pictures from among a list of alternatives that were not presented in the original list. This is similar to multiple choice assessments.Detection paradigm – individuals are shown a number of objects and color samples during a certain period of time. They are then tested on their visual ability to remember as much as they can by looking at testers and pointing out whether the testers are similar to the sample, or if any change is present.Savings method – compares the speed of originally learning to the speed of relearning it. The amount of time saved measures memory. Failures Transience – memories degrade with the passing of time. This occurs in the storage stage of memory, after the information has been stored and before it is retrieved. This can happen in sensory, short-term, and long-term storage. It follows a general pattern where the information is rapidly forgotten during the first couple of days or years, followed by small losses in later days or years.Absentmindedness – Memory failure due to the lack of attention. Attention plays a key role in storing information into long-term memory; without proper attention, the information might not be stored, making it impossible to be retrieved later. Physiology Brain areas involved in the neuroanatomy of memory such as the hippocampus, the amygdala, the striatum, or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning, while the amygdala is thought to be involved in emotional memory. Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning, as solely dependent on specific brain regions. Learning and memory are usually attributed to changes in neuronal synapses, thought to be mediated by long-term potentiation and long-term depression. But this has been questioned on computational as well as neurophysiological grounds by the cognitive scientist Charles R. Gallistel and others.In general, the more emotionally charged an event or experience is, the better it is remembered; this phenomenon is known as the memory enhancement effect. Patients with amygdala damage, however, do not show a memory enhancement effect.Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage. This memory loss includes retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage. Cognitive neuroscience Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007).One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: "components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits." Encoding. Encoding of working memory involves the spiking of individual neurons induced by sensory input, which persists even after the sensory input disappears (Jensen and Lisman 2005; Fransen et al. 2002). Encoding of episodic memory involves persistent changes in molecular structures that alter synaptic transmission between neurons. Examples of such structural changes include long-term potentiation (LTP) or spike-timing-dependent plasticity (STDP). The persistent spiking in working memory can enhance the synaptic and cellular changes in the encoding of episodic memory (Jensen and Lisman 2005).Working memory. Recent functional imaging studies detected working memory signals in both medial temporal lobe (MTL), a brain area strongly associated with long-term memory, and prefrontal cortex (Ranganath et al. 2005), suggesting a strong relationship between working memory and long-term memory. However, the substantially more working memory signals seen in the prefrontal lobe suggest that this area play a more important role in working memory than MTL (Suzuki 2007).Consolidation and reconsolidation. Short-term memory (STM) is temporary and subject to disruption, while long-term memory (LTM), once consolidated, is persistent and stable. Consolidation of STM into LTM at the molecular level presumably involves two processes: synaptic consolidation and system consolidation. The former involves a protein synthesis process in the medial temporal lobe (MTL), whereas the latter transforms the MTL-dependent memory into an MTL-independent memory over months to years (Ledoux 2007). In recent years, such traditional consolidation dogma has been re-evaluated as a result of the studies on reconsolidation. These studies showed that prevention after retrieval affects subsequent retrieval of the memory (Sara 2000). New studies have shown that post-retrieval treatment with protein synthesis inhibitors and many other compounds can lead to an amnestic state (Nadel et al. 2000b; Alberini 2005; Dudai 2006). These findings on reconsolidation fit with the behavioral evidence that retrieved memory is not a carbon copy of the initial experiences, and memories are updated during retrieval. Genetics Study of the genetics of human memory is in its infancy. A notable initial success was the association of APOE with memory dysfunction in Alzheimer's Disease. The search for genes associated with normally varying memory continues. One of the first candidates for normal variation in memory is the gene KIBRA, which appears to be associated with the rate at which material is forgotten over a delay period. In infancy Up until the mid-1980s it was assumed that infants could not encode, retain, and retrieve information. A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it.It should be noted that although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order - that is, recalling step 1 and then step 2. In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months.In fact, the term 'infantile amnesia' refers to the phenomenon of accelerated forgetting during infancy. Importantly, infantile amnesia is not unique to humans, and preclinical research (using rodent models) provides insight into the precise neurobiology of this phenomenon. A review of the literature from behavioral neuroscientist Dr Jee Hyun Kim suggests that accelerated forgetting during early life is at least partly due to rapid growth of the brain during this period. Aging One of the key concerns of older adults is the experience of memory loss, especially as it is one of the hallmark symptoms of Alzheimer's disease. However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals' performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information; source memory tasks that require them to remember the specific circumstances or context in which they learned information; and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example. Effects of physical exercise Physical exercise, particularly continuous aerobic exercises such as running, cycling and swimming, has many cognitive benefits and effects on the brain. Influences on the brain include increases in neurotransmitter levels, improved oxygen and nutrient delivery, and increased neurogenesis in the hippocampus. The effects of exercise on memory have important implications for improving children's academic performance, maintaining mental abilities in old age, and the prevention and potential cure of neurological diseases. Disorders Much of the current knowledge of memory has come from studying memory disorders, particularly amnesia. Loss of memory is known as amnesia. Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer's disease and Parkinson's disease can also affect memory and cognition. Hyperthymesia, or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. Korsakoff's syndrome, also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex.While not a disorder, a common temporary failure of word retrieval from memory is the tip-of-the-tongue phenomenon. Sufferers of Anomic aphasia (also called Nominal aphasia or Anomia), however, do experience the tip-of-the-tongue phenomenon on an ongoing basis due to damage to the frontal and parietal lobes of the brain. Influencing factors Interference can hamper memorization and retrieval. There is retroactive interference, when learning new information makes it harder to recall old information and proactive interference, where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer. Stress Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or 'Socially Evaluated Cold Pressor Test') for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process.However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game "Concentration" or "Memory". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar.This research on the effects of stress on memory may have practical implications for education, for eyewitness testimony and for psychotherapy: students may perform better when tested in their regular classroom rather than an exam room, eyewitnesses may recall details better at the scene of an event than in a courtroom, and persons suffering from post-traumatic stress may improve when helped to situate their memories of a traumatic event in an appropriate context.Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress damage neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate. This high level of extracellular glutamate allows calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child.The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they have often difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life. Sleep Making memories occurs through a three-step process, which can be enhanced by sleep. The three steps are as follows:Acquisition which is the process of storage and retrieval of new information in memoryConsolidationRecallSleep does not affect acquisition or recall while one is awake. Therefore, sleep has the greatest effect on memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain's abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). This process implicates that memories are reactivated during sleep, but that the process doesn't enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. When you are sleeping, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When you do not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. Therefore, it is important to get the proper amount of sleep so that memory can function at the highest level. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, suggesting that new memories may be solidified through such rehearsal. Construction for general manipulation Although people often think that memory operates like recording equipment, it is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. In fact, research has revealed that our memories are constructed: "current hypotheses suggest that constructive processes allow individuals to simulate and imagine future episodes, happenings, and scenarios. Since the future is not an exact repetition of the past, simulation of future episodes requires a complex system that can draw on the past in a manner that flexibly extracts and recombines elements of previous experiences - a constructive rather than a reproductive system."  People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, "How fast were the cars going when they smashed into each other?" gave higher estimates than those who were asked, "How fast were the cars going when they hit each other?" Furthermore, when asked a week later whether they have seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they have seen broken glass than those who had been asked the question with hit. There was no broken glass depicted in the film. Thus, the wording of the questions distorted viewers' memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.Interestingly, research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events.Research reported in 2013 revealed that it is possible to artificially stimulate prior memories and artificially implant false memories in mice. Using optogenetics, a team of RIKEN-MIT scientists caused the mice to incorrectly associate a benign environment with a prior unpleasant experience from different surroundings. Some scientists believe that the study may have implications in studying false memory formation in humans, and in treating PTSD and schizophrenia. Improving A UCLA research study published in the June 2006 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating, physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a "brain healthy" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long term follow up was conducted, it is therefore unclear if this intervention has lasting effects on memory.There are a loosely associated group of mnemonic principles and techniques that can be used to vastly improve memory known as the art of memory.The International Longevity Center released in 2001 a report which includes in pages 14–16 recommendations for keeping the mind in good functionality until advanced age. Some of the recommendations are to stay intellectually active through learning, training or reading, to keep physically active so to promote blood circulation to the brain, to socialize, to reduce stress, to keep sleep time regular, to avoid depression or emotional instability and to observe good nutrition.Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets. The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming: an intensive memorization in a short period of time. Also relevant is the Zeigarnik effect which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information. Levels of processing Craik and Lockhart (1972) proposed that it is the method and depth of processing that affects how an experience is stored in memory, rather than rehearsal.Organization: Mandler (1967) gave participants a pack of word cards and asked them to sort them into any number of piles using any system of categorisation they liked. When they were later asked to recall as many of the words as they could, those who used more categories remembered more words. This study suggested that the organization of memory is one of its central aspects (Mandler, 2011).Distinctiveness: Eysenck and Eysenck (1980) asked participants to say words in a distinctive way, e.g. spell the words out loud. Such participants recalled the words better than those who simply read them off a list.Effort: Tyler et al. (1979) had participants solve a series of anagrams, some easy (FAHTER) and some difficult (HREFAT). The participants recalled the difficult anagrams better, presumably because they put more effort into them.Elaboration: Palmere et al. (1983) gave participants descriptive paragraphs of a fictitious African nation. There were some short paragraphs and some with extra sentences elaborating the main idea. Recall was higher for the ideas in the elaborated paragraphs. See also Adaptive memoryFalse memoryIntermediate-term memoryMethod of lociMnemonic major systemPhotographic memoryPolitics of memory Notes  References  Further reading Fernyhough, Charles (2013). Pieces of Light: How the New Science of Memory Illuminates Stories We Tell About Our Pasts. ISBN 978-0062237897. Eck, Allison (June 3, 2014). "For More Effective Studying, Take Notes With Pen and Paper". Nova Next. PBS. Leyden, Andrea (January 24, 2014). "20 Study Hacks to Improve Your Memory". Exam Time.  External links "Memory". Stanford Encyclopedia of Philosophy. Memory at PhilPapersMemory at the Indiana Philosophy Ontology ProjectMemory on In Our Time at the BBC. (listen now)Memory-related resources from the National Institutes of HealthOn the Seven Sins of Memory with Professor Daniel Schacter 'Bridging the Gaps: A Portal for Curious Minds'
Computer hardware is the collection of physical components that constitute a computer system. Computer hardware is the physical parts or components of a computer, such as monitor, keyboard, computer data storage, graphic card, sound card, motherboard, and so on, all of which are tangible objects. By contrast, software is instructions that can be stored and run by hardware.Hardware is directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system. Von Neumann architecture The template for all modern computers is the Von Neumann architecture, detailed in a 1945 paper by Hungarian mathematician John von Neumann. This describes a design architecture for an electronic digital computer with subdivisions of a processing unit consisting of an arithmetic logic unit and processor registers, a control unit containing an instruction register and program counter, a memory to store both data and instructions, external mass storage, and input and output mechanisms. The meaning of the term has evolved to mean a stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the Von Neumann bottleneck and often limits the performance of the system. Sales For the third consecutive year, U.S. business-to-business channel sales (sales through distributors and commercial resellers) increased, ending 2013 up nearly 6 percent at $61.7 billion. The impressive growth was the fastest sales increase since the end of the recession. Sales growth accelerated in the second half of the year peaking in fourth quarter with a 6.9 percent increase over the fourth quarter of 2012. Different systems There are a number of different types of computer system in use today. Personal computer The personal computer, also known as the PC, is one of the most common types of computer due to its versatility and relatively low price. Laptops are generally very similar, although they may use lower-power or reduced size components, thus lower performance. Case The computer case is a plastic or metal enclosure that houses most of the components. Those found on desktop computers are usually small enough to fit under a desk; however, in recent years more compact designs have become more commonplace, such as the all-in-one style designs from Apple, namely the iMac. A case can be either big or small, but the form factor of motherboard for which it is designed matters more. Laptops are computers that usually come in a clamshell form factor; however, in more recent years, deviations from this form factor, such as laptops that have a detachable screen that become tablet computers in their own right, have started to emerge. Power supply A power supply unit (PSU) converts alternating current (AC) electric power to low-voltage DC power for the internal components of the computer. Laptops are capable of running from a built-in battery, normally for a period of hours. Motherboard The motherboard is the main component of a computer. It is a board with integrated circuitry that connects the other parts of the computer including the CPU, the RAM, the disk drives (CD, DVD, hard disk, or any others) as well as any peripherals connected via the ports or the expansion slots.Components directly attached to or to part of the motherboard include:The CPU (Central Processing Unit), which performs most of the calculations which enable a computer to function, and is sometimes referred to as the brain of the computer. It is usually cooled by a heatsink and fan, or water-cooling system. Most newer CPUs include an on-die Graphics Processing Unit (GPU). The clock speed of CPUs governs how fast it executes instructions, and is measured in GHz; typical values lie between 1 GHz and 5 GHz. Many modern computers have the option to overclock the CPU which enhances performance at the expense of greater thermal output and thus a need for improved cooling.The chipset, which includes the north bridge, mediates communication between the CPU and the other components of the system, including main memory.Random-Access Memory (RAM), which stores the code and data that are being actively accessed by the CPU. For example, when a web browser is opened on the computer it takes up memory; this is stored in the RAM until the web browser is closed. RAM usually comes on DIMMs in the sizes 2GB, 4GB, and 8GB, but can be much larger.Read-Only Memory (ROM), which stores the BIOS that runs when the computer is powered on or otherwise begins execution, a process known as Bootstrapping, or "booting" or "booting up". The BIOS (Basic Input Output System) includes boot firmware and power management firmware. Newer motherboards use Unified Extensible Firmware Interface (UEFI) instead of BIOS.Buses that connect the CPU to various internal components and to expand cards for graphics and sound.The CMOS battery, which powers the memory for date and time in the BIOS chip. This battery is generally a watch battery.The video card (also known as the graphics card), which processes computer graphics. More powerful graphics cards are better suited to handle strenuous tasks, such as playing intensive video games. Expansion cards An expansion card in computing is a printed circuit board that can be inserted into an expansion slot of a computer motherboard or backplane to add functionality to a computer system via the expansion bus. Expansions cards can be used to obtain or expand on features not offered by the motherboard. Storage devices A storage device is any computing hardware and digital media that is used for storing, porting and extracting data files and objects. It can hold and store information both temporarily and permanently, and can be internal or external to a computer, server or any similar computing device. Data storage is a core function and fundamental component of computers. Fixed media Data is stored by a computer using a variety of media. Hard disk drives are found in virtually all older computers, due to their high capacity and low cost, but solid-state drives are faster and more power efficient, although currently more expensive than hard drives in terms of dollar per gigabyte, so are often found in personal computers built post-2007. Some systems may use a disk array controller for greater performance or reliability. Removable media To transfer data between computers, a USB flash drive or optical disc may be used. Their usefulness depends on being readable by other systems; the majority of machines have an optical disk drive, and virtually all have at least one USB port. Input and output peripherals Input and output devices are typically housed externally to the main computer chassis. The following are either standard or very common to many computer systems. Input Input devices allow the user to enter information into the system, or control its operation. Most personal computers have a mouse and keyboard, but laptop systems typically use a touchpad instead of a mouse. Other input devices include webcams, microphones, joysticks, and image scanners. Output device Output devices display information in a human readable form. Such devices could include printers, speakers, monitors or a Braille embosser. Mainframe computer A mainframe computer is a much larger computer that typically fills a room and may cost many hundreds or thousands of times as much as a personal computer. They are designed to perform large numbers of calculations for governments and large enterprises. Departmental computing In the 1960s and 1970s, more and more departments started to use cheaper and dedicated systems for specific purposes like process control and laboratory automation. Supercomputer A supercomputer is superficially similar to a mainframe, but is instead intended for extremely demanding computational tasks. As of June 2016, the fastest supercomputer in the world is the Sunway TaihuLight, in Jiangsu, China.The term supercomputer does not refer to a specific technology. Rather it indicates the fastest computations available at any given time. In mid 2011, the fastest supercomputers boasted speeds exceeding one petaflop, or 1 quadrillion (10^15 or 1,000 trillion) floating point operations per second. Super computers are fast but extremely costly so they are generally used by large organizations to execute computationally demanding tasks involving large data sets. Super computers typically run military and scientific applications. Although they cost millions of dollars, they are also being used for commercial applications where huge amounts of data must be analyzed. For example, large banks employ supercomputers to calculate the risks and returns of various investment strategies, and healthcare organizations use them to analyze giant databases of patient data to determine optimal treatments for various diseases and problems incurring to the country. Hardware upgrade When using computer hardware, an upgrade means adding new hardware to a computer that improves its performance, adds capacity or new features. For example, a user could perform a hardware upgrade to replace the hard drive with a SSD to get a boost in performance or increase the amount of files that may be stored. Also, the user could increase the RAM so the computer may run more smoothly. The user could add a USB 3.0 expansion card in order to fully use USB 3.0 devices, or could upgrade the GPU for extra rendering power. Performing such hardware upgrades may be necessary for older computers to meet a programs' system requirements. Recycling Computer parts cannot just be thrown away due to the fact that some computer parts are hazardous. Some of the parts contain chemicals such as: lead, mercury, nickel, and cadmium. According to the EPA these e-wastes have a harmful effect on the environment unless it is disposed properly. Making hardware requires energy, and recycling parts will reduce air pollution, water pollution, as well as greenhouse gas emissions. Disposing unauthorized computer equipment is in fact illegal. Legislation makes it mandatory to recycle computers through the government approved facilities. See also Computer architectureElectronic hardwareHistory of computing hardwareList of computer hardware manufacturersOpen-source computing hardware References  External links  Media related to Computer hardware at Wikimedia Commons Learning materials related to Computer hardware at Wikiversity
Carrom (also known as Karrom) is a "strike and pocket" table game of Eastern origin similar to billiards and table shuffleboard. It is found throughout the East under different names though most non-eastern people know it by the East Asian name of Carroms (or Karrom). It is very popular in Nepal, India, Pakistan, Bangladesh, Sri Lanka and surrounding areas and in the Middle East as well. In South Asia, many clubs and cafés hold regular tournaments. Carrom is very commonly played by families, including the children, and at social functions. Different standards and rules exist in different areas. Origins The game of carrom is believed to have originated from the Indian subcontinent. Although no concrete evidence is available, it is believed that carrom was invented by the Indian Maharajas. One Carrom Board with its surface made of glass is still available in one of the palaces in Patiala, India. It became very popular among the masses after World War I. State-level competitions were being held in different States of India during early part of the nineteenth century. Serious carrom tournaments may have begun in Sri Lanka in 1935 but by 1958, both India and Sri Lanka had formed official federations of carrom clubs, sponsoring tournaments and awarding prizes.The International Carrom Federation (ICF) was formed in the year 1988 in Chennai, India. The formal rules for the Indian version of the game were published in 1988. In the same year the ICF officially codified the rules. The game has been very popular throughout South Asia, mainly in India, Pakistan, Bangladesh, Sri Lanka, and Nepal. It has gained some popularity in Europe and the United States where it has been introduced by the Indian diaspora. The United States Carrom Association reports on competitions in the U.S. and Canada and has a player ranking list as of the last tournament.The board and pieces can be bought in Europe or the U.S. and are usually imported from India. The most expensive boards are made to a high standard with high quality wood and decorations though cheaper boards are available. Some of the largest exporters of carrom boards are in India, e.g. Precise, Surco, Syndicate Sports and Paul Traders. Objective of play The objective of play is to use a striker disk with a flick of the finger to make contact with and move lighter object disks called carrom men, which are thus propelled into one of four corner pockets.The aim of the game is to pot (or pocket) one's nine carrom men and the Queen before your opponent. Equipment The game is usually played on a board made of plywood. The dimensions of the standardised game is a 29 inches (74 cm) square playing surface on a board of lacquered plywood. The edges of the playing surface are bounded by bumpers of wood, and the underside of each pocket is covered by a net which is 10 cm2 or larger. Carrom men Carrom is played using small disks of wood or plastic known as carrom men, sometimes abbreviated c/m. The pieces are also known as seed, coin or Pawnpuck. Carrom men are designed to slide when struck and are made with a smooth surface that allows contact with the board when the pieces are laid flat. They are struck by a Striker of standard specification which is larger and heavier. Carrom follows similar "strike and pocket" games, like pool, with its use of rebounds, angles and obstruction of opponent's pieces.A carrom set contains 19 pieces (striker not included) in three distinct colours. Two colours to represent the players' pieces and one colour for the Queen. The usual colours are white (or unstained) and black for the players and red for the queen.ICF-approved pieces must have a diameter of no more than 3.18 cm and no less than 3.02 cm. The pieces must be between 7 and 9 mm thick. The pieces have a plain, rounded edge. The mass of the pieces must be between 5.0 and 5.5g. Strikers Striker pieces are used to push the carrom men and the queen across the board to the pockets. The carrom striker normally weighs 15 grams. The Queen The red disk is called the queen. The queen is the most powerful carrom piece. During board setup, it is placed at the centre of the circle. In accordance with the ICF rules, pocketing the queen adds 3 points to the player's total score. The dimensions of the queen must be the same as those of other carrom men.The player must pocket the queen and subsequently pocket a carrom man of the player's own colour. This is termed covering the queen. If, by mistake, a player puts the carrom man of the opposite team in the pocket after "pocketing" the queen, then the queen has to be placed in the center of the Carrom board again.If the player fails to pocket a subsequent carrom man, the queen is replaced at the centre of the circle.If the player pockets his or her opponent's last carrom man before pocketing the queen, then the other player wins that board.If a player puts the queen and a carrom man of the player's own color in the pocket with one use of the striker, the queen is automatically covered, no matter which went first. Powder Fine-grained powder is used on the board to enable the pieces to slide easily. Boric acid powder is the most commonly used for this purpose despite having recently been reclassified by the EU as 'Toxic for reproduction'.In the UK, many players use a version of anti-set-off spray powder from the printing industry which has specific electrostatic properties with particles of 50 micrometres in diameter. The powder is made from pure, food-grade vegetable starch. Standardised rules and regulations The ICF promulgates International Rules of Carrom (also termed "The Laws of Carrom"). ICF acts as the governing body of carrom. The organisation also ranks players, sanctions tournaments and presents awards. ICF has many national affiliates such as the All-India Carrom Federation, Australian Carrom Federation, and United States Carrom Association. The toss Order of play is determined by the process of "calling the carrom men" or "the toss". Before commencing each match, an umpire hides one black carrom in one hand and one white carrom man in the other hand. The players guess which colour carrom men is being held in each hand. The player who guesses correctly wins the toss.The winner of the toss strikes first, which is called the opening break. The winner of the toss has the option to change sides from white to black and give up the opening break. The winner of the toss may not pass this decision to the other player. If the winner of the toss chooses to change sides then the loser must strike first.The player taking the first shot (or break) plays white carrom men. The opponent plays black. If that player cannot take any point then that player losses the turn and opponent can choose to play any carrom man, Black or White in favour.Template:Of what? Shooting The aim of the game is to pot (or pocket) one's nine carrom men and the queen before your opponent does. A successful pot entitles the player to shoot again. This means that, like pool and snooker, it is possible for a player to pot all his/her pieces and cover the queen from the start of the game without the opponent being given the chance to shoot.Any player pocketing the queen is required to cover it by immediately pocketing one of their carrom men on the entitlement shot. If after potting the queen the player fails to cover it, then the queen is returned to the center of the table. It is illegal to pot the Queen after the last piece since the queen must always be covered.Thumbing is allowed by International Carrom Federation which allows the player to shoot with any finger including the thumb (known as "thumbing", "thumb shot", or "thumb hit").Crossing the diagonal lines on the board by coming in touch with it, or pocketing the striker is a foul. A player needs to ensure that his striking hand does not infringe/cross the diagonal lines aerially/physically. A player committing a foul must return one carrom man that was already pocketed.If a player pockets his striker, he has to pay a penalty. This penalty is usually 10 points. Variants  Family-Point Carrom Simple-Point Carrom (Family-Point Carrom) is a variant that is very popular with the young and old, or when playing with an odd number of players. Players are allowed to pocket carrom men of any colour. A majority of people play by the following simple rules:The objective of play is to use a striker disk with a flick of the finger to make contact with and move a carrom man (or coin) into one of four corner pockets.Typically a Black carrom man (coin) gives 5 points, white/khaki color (or non-black) gives 10 points and Red color (queen) gives 25 points to the player.Pocketing the queen must be followed by pocketing another carrom man (coin) on the same strike. To get Red color (queen) points, one needs to put a carrom man of any color in the pocket after the queen. If the player fails to cover the queen in this fashion, the queen is put back in the center of the board.The player or team will win if they have the most points.Sets of 1, 3 or 5 are common.With the points system, if one team/player gets queen points early in the game, the opponent still has a good chance to win by earning more points.This style of play is widely accepted in many areas of South Asia. Point Carrom Point Carrom is a variant that is popular with children or an odd number of players. Game play is as described above with a variation. Players are allowed to pocket carrom men of any colour.Carrom men of either colour are assigned 1 point each.The red queen is assigned 3 points.Pocketing the queen must be followed by pocketing another carrom man on the same or subsequent strike.The first player to reach 21 points is declared the winner.If no player reaches 21 points, the player with the highest points is declared the winner. If the scores are tied, a tie-breaker must be played. Players who are tied (in points) select a colour. They are allowed to pocket carrom men of an alternate colour only on rebound.This style of play is common in some areas of East Asia. Total-Point Carrom Total point carrom is a variant of point carrom, in which the black carrom men are worth 5 points and the white ones are worth 10 points.The red queen is assigned 50 points and must have a subsequent carrom man pocketed after it.To win, a player must receive all the carrom men on the board.After the first round the player or team with the lowest score puts all their carrom men in the center.The others must match this score in the center and the players play for the carrom men in the center.They repeat this until one team or player has all the carrom men.This style of play is widely accepted in many areas of India and Pakistan. Professional Carrom Each Team or player is assigned a color coin and can only pocket that color coin.Pocketing the queen must be followed by pocketing another coin on the same strike.The red 'queen,' can be pocketed at any time after sinking your first piece but must be sunk before your last one. After pocketing the queen, you must sink one of your carrom men, thereby 'covering' it, into any pocket in the next shot, or she is returned to the center spot.Once the queen is covered, whoever clears all their carrom men first wins the 'board'.Queen & cover can be pocketed in the same turn, irrespective of the order of falling of coin in the pockets.The winner of a board collects one point for each of the opponent's carrom men left at the finish and three points for the queen if covered by the winner (if covered by the loser, no-one gets those points). No more points are collected for the queen after your score reaches 21.As per new rules a game consists of 21 points .When placing the striker on the board to shoot, it must touch both 'base lines', either covering the end circle completely, or not touching it at all. The striker may not touch the diagonal arrow line.Shooting styles are very personal - whichever 'grip' works for you is fine as long as you 'flick' the striker and don't push it. Generally, it is best to orient your body in order to see the line of your aim while shooting comfortably; you may not move or leave your chair.For forward shots, you can use your index finger, middle finger, or even the 'scissors' shot. Before shooting, try touching the striker with your fingernail, to be sure that its really on line. This will improve your accuracy and prevent you from hurting your finger.Carrom men can be struck directly only if they are not touching the player’s baseline or situated behind the base line. If the carrom man is behind the baseline, the player must hit the carrom man by rebounding the carrom striker off any side of the carrom board or any other carrom piece on the boardSinking the striker costs you one piece and your turn. But, if you sink a piece in the same shot, then two come up and you do not shoot again.After sinking the striker, your opponent places the due piece(s) within the center circle. If you haven't sunk one yet, you owe one.If while shooting for the queen you also sink one of your carrom men in the same shot, the queen is automatically covered, no matter which went first.If a piece jumps off the board, it is placed on the center spot. If pieces land on end or are overlapping, they are left that way.If the center spot is partially covered when replacing the queen or a jumped piece, the piece should cover as much red as possible. If totally covered, the piece is placed opposite the next player behind the red spot.If you touch your last piece directly before the queen, you have to pay a penalty.If you sink your opponent's piece, you lose your turn. If you sink their last piece, you lose the board and three points.If you sink your last piece before the queen, you lose the board, three points and one point for each of your opponent's pieces left.If the striker does not leave both lines, go again. You get three tries to break before losing your turn.These rules are mostly played in UK and India. Duboo A popular variant of the game called Duboo is played mostly in Karachi, the largest city of Pakistan. In dubbo the size of the board is bigger than carrom, and instead of flicking the striker people usually slide it. Board variations Carrom boards are available in various board sizes and corner pocket sizes. There are smaller boards and boards with larger pockets. Boards with larger pockets are used by beginners for easier game play. On traditional carrom boards, the corner pockets are only slightly larger than the carrom men, but smaller than the striker. On boards with larger pockets, it is possible to pocket the striker, resulting in a "scratch shot" as in Pool. This results in a "due." On a "due", the player has to return one previously pocketed carrom man to the board. When the scores are tied at a point in the carrom game, a tie-breaker is played. The team which has pocketed the "queen" does not gain any advantage. The Standardised Association and Federation size is 29" x 29" Play Surface with borders between 2" each to 4" each. Other play areas are not used in Tournaments and Competitions. American carrom American carrom is a variant of carrom derived in America by missionaries to the East, around 1890. Concerned with young boys loitering around pool halls, a Sunday school teacher named Henry L. Haskell altered the game for Western tastes. Much of the game is the same, but the striker's weight is reduced and the carrom men are smaller. Generally, instead of disks, carrom men (including the striker) are rings, originally of wood but today commercially made of light plastic. In addition, as an alternative to using the fingers to flick the striker, some of the American carrom boards use miniature cue sticks. American carrom boards also have pockets built into the corners, rather than circular holes in the board, to make pocketing easier. While traditionally made boards vary widely, current commercially produced American carrom boards are 28 inches (71 cm) square, are printed with checkerboard and backgammon patterns, among others, and are sold with dice, skittles, etc. to allow other games to be played on the same board. These boards are also built to play crokinole with.A relatively rare series of makes among Western Carrom boards contains a variant referred to colloquially as a "Carrom maze" on the reverse, in which an entirely different game is played. The oblique side of the board is fashioned into a labyrinth via the addition of small plywood "walls" that restrict the carrom to defined paths; the objective becomes to traverse the maze with a single carrom and reach a region designated as the end of the maze successfully in the least amount of strokes (similarly to golf), or to be the first to finish the maze among competitors. Various regions within the maze, often found in "traps" or sharp corners and differently colored or designated via artwork, contain regions in which the player's carrom must not be caught when coming to rest, at risk of penalty of extra strokes or forced relocation of the player's carrom to an earlier position. Positive or bonus regions, usually small and hard to target, may offer "shortcuts" relocating to a region nearer the goal, or stroke count reduction. In solo play, course records may be kept for public tables. Japanese carrom Carrom was introduced to Japan in the early 20th century. Carrom became popular as tōkyūban (闘球盤?, Japanese for "pounding board", "fight ball board" or "throw ball board") and it fell in popularity in the Showa period. However, carrom is still popular in Hikone, Shiga under the name Hikone Karomu (Hikone carrom). The Hikone carrom board has larger pockets (not unlike those of pichenotte), the discs are arranged in a ring (also like in pichenotte), each player is given twelve discs instead of nine, and the queen (known as the "jack") is pocketed last (similar to Eight-ball or Black ball). In popular culture In 2010 a Hindi "Bollywood" film titled Striker was released. The movie focuses on carrom hustlers in Mumbai in the 1980s.The Hindi film Ankush showed the ability of carrom to help four unemployed youths escape the painful realities of life.A Tamil film called Vilayaada Vaa released in 2012 was also focussed on carrom board.Indian movies Munnabhai MBBS and its Telugu remake - Shankar Dada MBBS, Tamil remake - Vasool Raja MBBS, Kannada remake - Uppi Dada M.B.B.S. also features a movie scene with Munnabhai playing carrom to heal an elderly friend with his friends and an Orange Juice See also CrokinoleNovussPichenottePitchnutButton Soccer References  External links International Carrom FederationCarrom Ukraine — Ukraine Carrom FederationCarrom Korea — Korea Carrom FederationCarrom UK — UK Carrom Club
Chess is a two-player strategy board game played on a chessboard, a checkered gameboard with 64 squares arranged in an 8×8 grid. Chess is played by millions of people worldwide.Each player begins the game with 16 pieces: one king, one queen, two rooks, two knights, two bishops, and eight pawns. Each of the six piece types moves differently, with the most powerful being the queen and the least powerful the pawn. The objective is to checkmate the opponent's king by placing it under an inescapable threat of capture. To this end, a player's pieces are used to attack and capture the opponent's pieces, while supporting each other. In addition to checkmate, the game can be won by voluntary resignation of the opponent, which typically occurs when too much material is lost, or checkmate appears unavoidable. A game may also in several ways result in a draw.Chess is believed to have originated in India sometime before the 7th century, being derived from the Indian game chaturanga. Chaturanga is also the likely ancestor of the Eastern strategy games xiangqi, janggi, and shogi. The pieces assumed their current powers in Spain in the late 15th century; the rules were finally standardized in the 19th century. The first generally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886. Since 1948, the World Championship has been regulated by FIDE, the game's international governing body. FIDE also organizes the Women's World Championship, the World Junior Championship, the World Senior Championship, the Blitz and Rapid World Championships, and the Chess Olympiad, a popular competition among teams from different nations. There is also a Correspondence Chess World Championship and a World Computer Chess Championship. Online chess has opened amateur and professional competition to a wide and varied group of players. There are also many chess variants which utilize different rules, pieces, or boards. One of these, Chess960 (also called "Fischerandom") has achieved widespread popularity among chess players.FIDE awards titles to skilled players, the highest of which is grandmaster. Many national chess organizations also have a title system; however, these are not recognized by FIDE. The term "master" might refer to a formal title or describe more loosely any skilled player.Until recently, chess was a recognized sport of the International Olympic Committee; some national sporting bodies such as the Spanish Consejo Superior de Deportes also recognize chess as a sport. Chess was included in the 2006 and 2010 Asian Games.Since the second half of the 20th century, computers have been programmed to play chess with increasing success, to the point where the strongest home computers play at a higher level than the best human players. Since the 1990s, computer analysis has contributed significantly to chess theory, particularly in the endgame. The computer IBM Deep Blue was the first machine to overcome a reigning World Chess Champion in a match when it defeated Garry Kasparov in 1997. The rise of strong computer programs (called "engines") that can be run on hand-held devices has led to increasing concerns about cheating during tournaments. Rules The official rules of chess are maintained by FIDE (Fédération Internationale des Échecs), chess's international governing body. Along with information on official chess tournaments, the rules are described in the FIDE Handbook, Laws of Chess section. Setup Chess is played on a square board of eight rows (called ranks and denoted with numbers 1 to 8) and eight columns (called files and denoted with letters a to h). The colors of the 64 squares alternate and are referred to as light and dark squares. The chessboard is placed with a light square at the right-hand end of the rank nearest to each player.By convention, the game pieces are divided into white and black sets, and the players are referred to as White and Black respectively. Each player begins the game with 16 pieces of the specified color, which consist of one king, one queen, two rooks, two bishops, two knights, and eight pawns. The pieces are set out as shown in the diagram and photo, with each queen on a square of its own color, the white queen on a light square and the black queen on a dark. Movement The player with the white pieces always moves first. After the first move, players alternately move one piece per turn (except for castling, when two pieces are moved). Pieces are moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. A player may not make any move that would put or leave the player's own king under attack. A player cannot "pass"; at each turn one must make a legal move (this is the basis for the finesse called zugzwang).If the player to move has no legal move, the game is over; it is either a checkmate (a loss for the player with no legal moves) if the king is under attack, or a stalemate (a draw) if the king is not.Each chess piece has its own way of moving. In the diagrams, the dots mark the squares where the piece can move if there are no intervening piece(s) of either color.The king moves one square in any direction. The king also has a special move called castling that involves also moving a rook.The rook can move any number of squares along a rank or file, but cannot leap over other pieces. Along with the king, a rook is involved during the king's castling move.The bishop can move any number of squares diagonally, but cannot leap over other pieces.The queen combines the power of a rook and bishop and can move any number of squares along a rank, file, or diagonal, but cannot leap over other pieces.The knight moves to any of the closest squares that are not on the same rank, file, or diagonal, thus the move forms an "L"-shape: two squares vertically and one square horizontally, or two squares horizontally and one square vertically. The knight is the only piece that can leap over other pieces.The pawn can move forward to the unoccupied square immediately in front of it on the same file, or on its first move it can advance two squares along the same file, provided both squares are unoccupied (black dots in the diagram); or the pawn can capture an opponent's piece on a square diagonally in front of it on an adjacent file, by moving to that square (black "x"s). A pawn has two special moves: the en passant capture and promotion. Castling Once in every game, each king is allowed to make a special move, known as castling. Castling consists of moving the king two squares along the first rank toward a rook (which is on the player's first rank) and then placing the rook on the last square that the king has just crossed. Castling is permissible under the following conditions:Neither the king nor the rook have previously moved during the game.There cannot be any pieces between the king and the rook.The king cannot be in check, nor can the king pass through squares that are under attack by enemy pieces, or move to a square where it would result in a check. Note that castling is permissible if the rook is attacked, or if the rook crosses a square that is attacked. En passant When a pawn advances two squares from its starting position and there is an opponent's pawn on an adjacent file next to its destination square, then the opponent's pawn can capture it en passant (in passing), and move to the square the pawn passed over. This can only be done on the very next move, otherwise the right to do so is forfeit. For example, if the black pawn has just advanced two squares from g7 (initial starting position) to g5, then the white pawn on f5 may take it via en passant on g6 (but only on White's next move). Promotion When a pawn advances to the eighth rank, as a part of the move it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases another piece is chosen; this is called underpromotion. In the diagram on the right, the pawn on c7 can be advanced to the eighth rank and be promoted to an allowed piece. There is no restriction placed on the piece that is chosen on promotion, so it is possible to have more pieces of the same type than at the start of the game (for example, two queens). Check When a king is under immediate attack by one or two of the opponent's pieces, it is said to be in check. A response to a check is a legal move if it results in a position where the king is no longer under direct attack (that is, not in check). This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check. The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to remove it from attack. It is illegal for a player to make a move that would put or leave the player's own king in check.In casual games it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of the game, and is not usually done in tournaments. End of the game  Win Games can be won in the following ways:Checkmate: The player whose turn it is to move is in check and has no legal move to escape check.Resignation: Either player may resign, conceding the game to the opponent. It is usually considered poor etiquette to play on in a truly hopeless position, and for this reason high-level games rarely end in checkmate.Win on time: In games with a time control, a player wins if the opponent runs out of time, even if the opponent has a much superior position, as long as the player still has a theoretical possibility to checkmate the opponent.Forfeit: A player who cheats, or violates the rules of the game, or violates the rules specified for the particular tournament can be forfeited. In high-level tournaments, players have been forfeited for such things as arriving late for the game (even by a matter of seconds), receiving a call or text on a cell phone, refusing to undergo a drug test, refusing to undergo a body search for electronic devices, and unsporting behavior (such as refusing to shake the opponent's hand). Draw There are several ways games can end in a draw:Draw by agreement: Draws are most commonly reached by mutual agreement between the players. The correct procedure is to verbally offer the draw, make a move, then start the opponent's clock. Traditionally, players have been allowed to agree to a draw at any point in the game, occasionally even without playing a move; in recent years efforts have been made to discourage short draws, for example by forbidding draw offers before move thirty.Stalemate: The player whose turn it is to move has no legal move and is not in check.Threefold repetition of position: This most commonly occurs when neither side is able to avoid repeating moves without incurring a disadvantage. In this situation, either player can claim a draw; this requires the players to keep a valid written record of the game so that the claim can be verified by the arbiter if challenged. The three occurrences of the position need not occur on consecutive moves for a claim to be valid. FIDE rules make no mention of perpetual check; this is merely a specific type of draw by threefold repetition.Fifty-move rule: If during the previous 50 moves no pawn has been moved and no capture has been made, either player can claim a draw. There are in fact several known endgames where it is theoretically possible to force a mate but require more than 50 moves before a pawn move or capture is made; examples include some endgames with two knights against a pawn and some pawnless endgames such as queen against two bishops. These endings are rare, and few players study them in detail, so the fifty-move rule is considered adequate for over-the-board play. Some correspondence chess organizations allow exceptions to the fifty-move rule.Fivefold repetition of position: Similar to the threefold-repetition rule, but in this case no player needs to claim the draw for the game to be drawn. This rule took effect on 1 July 2014 and establishes that there is a theoretical upper bound on the length of lawful chess games.Seventy-five-move rule: Similar to the fifty-move rule; however, if the final move in the sequence resulted in checkmate, this takes precedence. As for the fivefold-repetition rule, this applies independently of claims by the players. This rule also took effect on 1 July 2014 and also establishes, independently, an upper bound on game length.Insufficient material: If neither player has a theoretical possibility to checkmate the opponent; for example, if a player has only the king and a knight left, and the opponent has only the king left, checkmate is impossible and the game is drawn by this rule. On the other hand, if both players have a king and a knight left, there is a highly unlikely yet theoretical possibility of checkmate, so this rule does not apply.Draw on time: In games with a time control, the game is drawn if a player is out of time and the opponent has no theoretical possibility to checkmate the player. Time control Chess games may also be played with a time control, mostly by club and professional players. If a player's time runs out before the game is completed, the game is automatically lost (provided the opponent has enough pieces left to deliver checkmate). The duration of a game ranges from long games played up to seven hours to shorter rapid chess games, usually lasting 30 minutes or one hour per game. Even shorter is blitz chess, with a time control of 3 to 15 minutes for each player, and bullet chess (under 3 minutes). In tournament play, time is controlled using a game clock that has two displays, one for each player's remaining time. Notation for recording moves Chess games and positions are recorded using a system of notation, most commonly algebraic chess notation. Abbreviated (or short) algebraic notation generally records moves in the format "abbreviation of the piece moved – file where it moved – rank where it moved". The pieces are identified by their initials. In English, these are K (King), Q (Queen), R (Rook), B (Bishop), and N (Knight; N is used to avoid confusion with King). For example, Qg5 means "queen moves to the g-file and the 5th rank" (that is, to the square g5). Chess literature published in other languages may use different initials to indicate the pieces, or Figurine Algebraic Notation may be used to avoid language difficulties. To resolve ambiguities, one more letter or number is added to indicate the file or rank from which the piece moved, e.g. Ngf3 means "knight from the g-file moves to the square f3", and R1e2 means "rook on the first rank moves to e2". The letter P for a pawn is not used, so that e4 means "pawn moves to the square e4".If the piece makes a capture, "x" is inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used in place of a piece initial, and ranks may be omitted if unambiguous. For example, exd5 (pawn on the e-file captures the piece on d5) or exd (pawn on the e-file captures a piece somewhere on the d-file). Particularly in Germany, some publications have used ":" rather than "x" to indicate a capture, but this is now rare. Some publications omit the capture symbol altogether, so that exd5 would be rendered simply as "ed".If a pawn moves to its last rank, achieving promotion, the piece chosen is indicated after the move, for example e1Q or e1Q. Castling is indicated by the special notations 0-0 for kingside castling and 0-0-0 for queenside castling. An en passant capture is sometimes marked with the notation "e.p." A move that places the opponent's king in check usually has the notation "+" added. (The notation "++" for a double check is considered obsolete.) Checkmate can be indicated by "#". At the end of the game, "1–0" means "White won", "0–1" means "Black won", and "½–½" indicates a draw.Chess moves can be annotated with punctuation marks and other symbols. For example, "!" indicates a good move, "!!" an excellent move, "?" a mistake, "??" a blunder, "!?" an interesting move that may not be best, or "?!" a dubious move not easily refuted.For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded: 1. e4 e5 2. Qh5?! Nc6 3. Bc4 Nf6?? 4. Qxf7# 1–0The text-based Portable Game Notation (PGN), which is understood by chess software, is based on short form English language algebraic notation.Until about 1980, the majority of English language chess publications used a form of descriptive notation. In descriptive notation, files are named according to the piece which occupies the back rank at the start of the game, and each square has two different names depending on whether it is from White's or Black's point of view. For example, the square known as "e3" in algebraic notation is "K3" (King's 3rd) from White's point of view, and "K6" (King's 6th) from Black's point of view. When recording captures, the captured piece is named rather than the square on which it is captured (except to resolve ambiguities). The "Scholar's mate" is rendered thus in descriptive notation:1. P-K4 P-K42. Q-R5?! N-QB33. B-B4 N-B3??4. QxBP# 1–0A few players still prefer descriptive notation, but it is no longer recognized by FIDE.Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline. Squares are identified by numeric co-ordinates, for example a1 is "11" and h8 is "88". Moves are described by the "from" and "to" squares, and captures are not indicated. For example, the opening move 1.e4 is rendered as 1.5254. Castling is described by the king's move only, for example 5171 for White castling king's side, 5838 for Black castling queen's side. Strategy and tactics Chess strategy consists of setting and achieving long-term positioning advantages during the game – for example, where to place different pieces – while tactics concentrate on immediate maneuver. These two parts of the chess-playing process cannot be completely separated, because strategic goals are mostly achieved through tactics, while the tactical opportunities are based on the previous strategy of play. A game of chess is normally divided into three phases: opening, typically the first 10 moves, when players move their pieces to useful positions for the coming battle; then middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive. Fundamentals of tactics In chess, tactics in general concentrate on short-term actions – so short-term that they can be calculated in advance by a human player or by a computer. The possible depth of calculation depends on the player's ability. In quiet positions with many possibilities on both sides, a deep calculation is more difficult and may not be practical, while in "tactical" positions with a limited number of forced variations, strong players can calculate long sequences of moves.Simple one-move or two-move tactical actions – threats, exchanges of material, and double attacks – can be combined into more complicated combinations, sequences of tactical maneuvers that are often forced from the point of view of one or both players. Theoreticians describe many elementary tactical methods and typical maneuvers; for example, pins, forks, skewers, batteries, discovered attacks (especially discovered checks), zwischenzugs, deflections, decoys, sacrifices, underminings, overloadings, and interferences.A forced variation that involves a sacrifice and usually results in a tangible gain is called a combination. Brilliant combinations – such as those in the Immortal Game – are considered beautiful and are admired by chess lovers. A common type of chess exercise, aimed at developing players' skills, is showing players a position where a decisive combination is available and challenging them to find it. Fundamentals of strategy Chess strategy is concerned with evaluation of chess positions and with setting up goals and long-term plans for the future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).The most basic step in evaluating a position is to count the total value of pieces of both sides. The point values used for this purpose are based on experience; usually pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook. These basic values are then modified by other factors like position of the piece (for example, advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (for example, a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).Another important factor in the evaluation of chess positions is the pawn structure (sometimes known as the pawn skeleton), or the configuration of pawns on the chessboard. Since pawns are the least mobile of the chess pieces, the pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in the pawn structure, such as isolated, doubled, or backward pawns and holes, once created, are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack). Phases  Opening A chess opening is the group of initial moves of a game (the "opening moves"). Recognized sequences of opening moves are referred to as openings and have been given names such as the Ruy Lopez or Sicilian Defence. They are catalogued in reference works such as the Encyclopaedia of Chess Openings. There are dozens of different openings, varying widely in character from quiet positional play (for example, the Réti Opening) to very aggressive (the Latvian Gambit). In some opening lines, the exact sequence considered best for both sides has been worked out to more than 30 moves. Professional players spend years studying openings and continue doing so throughout their careers, as opening theory continues to evolve.The fundamental strategic aims of most openings are similar:Development: This is the technique of placing the pieces (particularly bishops and knights) on useful squares where they will have an optimal impact on the game.Control of the center: Control of the central squares allows pieces to be moved to any part of the board relatively easily, and can also have a cramping effect on the opponent.King safety: It is critical to keep the king safe from dangerous possibilities. A correctly timed castling can often enhance this.Pawn structure: Players strive to avoid the creation of pawn weaknesses such as isolated, doubled, or backward pawns, and pawn islands – and to force such weaknesses in the opponent's position.Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative. Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position. Middlegame The middlegame is the part of the game which starts after the opening. There is no clear line between the opening and the middlegame, but typically the middlegame will start when most pieces have been developed. (Similarly, there is no clear transition from the middlegame to the endgame; see start of the endgame.) Because the opening theory has ended, players have to form plans based on the features of the position, and at the same time take into account the tactical possibilities of the position. The middlegame is the phase in which most combinations occur. Combinations are a series of tactical moves executed to achieve some gain. Middlegame combinations are often connected with an attack against the opponent's king. Some typical patterns have their own names; for example, the Boden's Mate or the Lasker–Bauer combination.Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage. Endgame The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:Pawns become more important. Endgames often revolve around endeavors to promote a pawn by advancing it to the furthest rank.The king, which required safeguarding from checkmate in the middlegame, emerges as a strong piece in the endgame. It is often brought to the center of the board where it can protect its own pawns, attack enemy pawns, and hinder moves of the opponent's king.Zugzwang, disadvantageous since it forces a player to move, is often a factor in endgames but rarely in other stages of the game. In the example diagram, either side having the move is in zugzwang: Black to move must play 1...Kb7 allowing White to promote the pawn after 2.Kd7; White to move must permit a draw, either by 1.Kc6 stalemate or by losing the pawn after any other legal move.Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames. History  Predecessors Chess is believed to have originated in Eastern India, c. 280–550, in the Gupta Empire, where its early form in the 6th century was known as chaturaṅga (Sanskrit: चतुरङ्ग), literally four divisions [of the military] – infantry, cavalry, elephants, and chariotry, represented by the pieces that would evolve into the modern pawn, knight, bishop, and rook, respectively. Thence it spread eastward and westward along the Silk Road. The earliest evidence of chess is found in the nearby Sassanid Persia around 600, where the game came to be known by the name chatrang. Chatrang was taken up by the Muslim world after the Islamic conquest of Persia (633–44), where it was then named shatranj, with the pieces largely retaining their Persian names. In Spanish "shatranj" was rendered as ajedrez ("al-shatranj"), in Portuguese as xadrez, and in Greek as ζατρίκιον (zatrikion, which comes directly from the Persian chatrang), but in the rest of Europe it was replaced by versions of the Persian shāh ("king"), which was familiar as an exclamation and became the English words "check" and "chess".The oldest archaeological artifacts, ivory chess pieces, were excavated in ancient Afrasiab, today's Samarkand, in Uzbekistan, central Asia, and date to about 760, with some of them possibly older. The oldest known chess manual was in Arabic and dates to 840–850, written by al-Adli ar-Rumi (800–870), a renowned Arab chess player, titled Kitab ash-shatranj (Book of the chess). This is a lost manuscript, but referenced in later works. The eastern migration of chess, into China and Southeast Asia, has even less documentation than its migration west. The first reference to chess, called Xiang Qi, in China comes in the xuán guaì lù (玄怪录, record of the mysterious and strange) dating to about 800. Alternatively, some contend that chess arose from Chinese chess or one of its predecessors, although this has been contested.The game reached Western Europe and Russia by at least three routes, the earliest being in the 9th century. By the year 1000, it had spread throughout Europe. Introduced into the Iberian Peninsula by the Moors in the 10th century, it was described in a famous 13th-century manuscript covering shatranj, backgammon, and dice named the Libro de los juegos. 1200–1700: Origins of the modern game Around 1200, the rules of shatranj started to be modified in southern Europe, and around 1475, several major changes made the game essentially as it is known today. These modern rules for the basic moves had been adopted in Italy and Spain. Pawns gained the option of advancing two squares on their first move, while bishops and queens acquired their modern abilities. The queen replaced the earlier vizier chess piece towards the end of the 10th century and by the 15th century had become the most powerful piece; consequently modern chess was referred to as "Queen's Chess" or "Mad Queen Chess". Castling, derived from the 'kings leap' usually in combination with a pawn or rook move to bring the king to safety, was introduced. These new rules quickly spread throughout western Europe. The rules concerning stalemate were finalized in the early 19th century. Also in the 19th century, the convention that White moves first was established (formerly either White or Black could move first). Finally the rules around castling were standardized – variations in the castling rules had persisted in Italy until the late 19th century. The resulting standard game is sometimes referred to as Western chess or international chess, particularly in Asia where other games of the chess family such as xiangqi are prevalent. Since the 19th century, the only rule changes have been technical in nature, for example establishing the correct procedure for claiming a draw by repetition.Writings about the theory of how to play chess began to appear in the 15th century. The Repetición de Amores y Arte de Ajedrez (Repetition of Love and the Art of Playing Chess) by Spanish churchman Luis Ramirez de Lucena was published in Salamanca in 1497. Lucena and later masters like Portuguese Pedro Damiano, Italians Giovanni Leonardo Di Bona, Giulio Cesare Polerio and Gioachino Greco, and Spanish bishop Ruy López de Segura developed elements of openings and started to analyze simple endgames. 1700–1873: The Romantic Era in Chess The romantic era was characterized by opening gambits (sacrificing pawns or even pieces), daring attacks, and brazen sacrifices. Many elaborate and beautiful but unsound move sequences called 'combinations' were played by the masters of the time. The game was played more for art than theory. A profound belief that chess merit resided in the players' genius rather than inherent in the position on the board pervaded chess practice.In the 18th century, the center of European chess life moved from the Southern European countries to France. The two most important French masters were François-André Danican Philidor, a musician by profession, who discovered the importance of pawns for chess strategy, and later Louis-Charles Mahé de La Bourdonnais, who won a famous series of matches with the Irish master Alexander McDonnell in 1834. Centers of chess activity in this period were coffee houses in big European cities like Café de la Régence in Paris and Simpson's Divan in London.As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824. Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time. Sparkling games like Anderssen's Immortal game and Evergreen game or Morphy's Opera game were regarded as the highest possible summit of the chess art.Deeper insight into the nature of chess came with two younger players. American Paul Morphy, an extraordinary chess prodigy, won against all important competitors (except Howard Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks. 1873–1945: Birth of a sport Prague-born Wilhelm Steinitz beginning in 1873 described how to avoid weaknesses in one's own position and how to create and exploit such weaknesses in the opponent's position. The scientific approach and positional understanding of Steinitz revolutionized the game. Steinitz was the first to break a position down into its components. Before Steinitz, players brought their queen out early, did not completely develop their other pieces, and mounted a quick attack on the opposing king, which either succeeded or failed. The level of defense was poor and players did not form any deep plan. In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of all World Champions.After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim. The tradition of awarding such titles was continued by the World Chess Federation (FIDE), founded in 1924 in Paris. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik. It took a prodigy from Cuba, José Raúl Capablanca (World Champion 1921–1927), who loved simple positions and endgames, to end the German-speaking dominance in chess; he was undefeated in tournament play for eight years, until 1924. His successor was Russian-French Alexander Alekhine, a strong attacking player who died as the World champion in 1946. He briefly lost the title to Dutch player Max Euwe in 1935 and regained it two years later.Between the world wars, chess was revolutionized by the new theoretical school of so-called hypermodernists like Aron Nimzowitsch and Richard Réti. They advocated controlling the center of the board with distant pieces rather than with pawns, which invited opponents to occupy the center with pawns, which become objects of attack. 1945–present: Post-war era After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then (except for one interruption), ran a tournament of elite players. The winner of the 1948 tournament, Russian Mikhail Botvinnik, started an era of Soviet dominance in the chess world. Until the end of the Soviet Union, there was only one non-Soviet champion, American Bobby Fischer (champion 1972–1975). Botvinnik revolutionized opening theory. Previously Black strove for equality, to neutralize White's first-move advantage. As Black, Botvinnik strove for the initiative from the beginning. In the previous informal system of World Championships, the current champion decided which challenger he would play for the title and the challenger was forced to seek sponsors for the match. FIDE set up a new system of qualifying tournaments and matches. The world's strongest players were seeded into Interzonal tournaments, where they were joined by players who had qualified from Zonal tournaments. The leading finishers in these Interzonals would go on the "Candidates" stage, which was initially a tournament, and later a series of knockout matches. The winner of the Candidates would then play the reigning champion for the title. A champion defeated in a match had a right to play a rematch a year later. This system operated on a three-year cycle. Botvinnik participated in championship matches over a period of fifteen years. He won the world championship tournament in 1948 and retained the title in tied matches in 1951 and 1954. In 1957, he lost to Vasily Smyslov, but regained the title in a rematch in 1958. In 1960, he lost the title to the 23-year-old Latvian prodigy Mikhail Tal, an accomplished tactician and attacking player. Botvinnik again regained the title in a rematch in 1961.Following the 1961 event, FIDE abolished the automatic right of a deposed champion to a rematch, and the next champion, Armenian Tigran Petrosian, a player renowned for his defensive and positional skills, held the title for two cycles, 1963–1969. His successor, Boris Spassky from Russia (champion 1969–1972), won games in both positional and sharp tactical style. The next championship, the so-called Match of the Century, saw the first non-Soviet challenger since World War II, American Bobby Fischer, who defeated his Candidates opponents by unheard-of margins and clearly won the world championship match. In 1975, however, Fischer refused to defend his title against Soviet Anatoly Karpov when FIDE did not meet his demands, and Karpov obtained the title by default. Fischer modernized many aspects of chess, especially by extensively preparing openings.Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes. Karpov's reign finally ended in 1985 at the hands of Garry Kasparov, another Soviet player from Baku, Azerbaijan. Kasparov and Karpov contested five world title matches between 1984 and 1990; Karpov never won his title back. In 1993, Garry Kasparov and Nigel Short broke with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Champions and World Championships: the PCA or Classical champion extending the Steinitzian tradition in which the current champion plays a challenger in a series of many games, and the other following FIDE's new format of many players competing in a tournament to determine the champion. Kasparov lost his Classical title in 2000 to Vladimir Kramnik of Russia. The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion. In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008, 2010 and 2012. In 2013, Magnus Carlsen beat Anand in the 2013 World Chess Championship. He defended his title the following year, again against Anand, and is the reigning world champion.Chess remains a highly popular pastime among the general populace. A 2012 survey found that "chess players now make up one of the largest communities in the world: 605 million adults play chess regularly". Chess is played at least once a year by 12% of British people, 15% of Americans, 23% of Germans, 43% of Russians, and 70% of Indian people. Place in culture  Pre-modern In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game". Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.Many of the elaborate chess sets used by the aristocracy have been lost, but others partially survive, such as the Lewis chessmen.Chess was often used as a basis of sermons on morality. An example is Liber de moribus hominum et officiis nobilium sive super ludo scacchorum ('Book of the customs of men and the duties of nobles or the Book of Chess'), written by an Italian Dominican monk Jacobus de Cessolis c. 1300. This book was one of the most popular of the Middle Ages. The work was translated into many other languages (the first printed edition was published at Utrecht in 1473) and was the basis for William Caxton's The Game and Playe of the Chesse (1474), one of the first books printed in English. Different chess pieces were used as metaphors for different classes of people, and human duties were derived from the rules of the game or from visual properties of the chess pieces:The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.Known in the circles of clerics, students, and merchants, chess entered into the popular culture of Middle Ages. An example is the 209th song of Carmina Burana from the 13th century, which starts with the names of chess pieces, Roch, pedites, regina... Modern During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action [...]II. Circumspection, which surveys the whole Chess-board, or scene of action: – the relation of the several Pieces, and their situations [...]III. Caution, not to make our moves too hastily [...]With these or similar hopes, chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.Chess is often depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess is featured in films like Ingmar Bergman's The Seventh Seal and Satyajit Ray's The Chess Players.Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Tri-Dimensional Chess". "Wizard's Chess" is featured in J. K. Rowling's Harry Potter plays. The hero of Searching for Bobby Fischer struggles against adopting the aggressive and misanthropic views of a world chess champion. Chess is used as the core theme in the musical Chess by Tim Rice, Björn Ulvaeus, and Benny Andersson. The thriller film Knight Moves is about a chess grandmaster who is accused of being a serial killer. Pawn Sacrifice, starring Tobey Maguire as Bobby Fischer and Liev Schreiber as Boris Spassky, depicts the drama surrounding the 1972 World Chess Championship in Iceland during the Cold War. Composition Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer. There are many types of chess problems; the two most important are:Directmates: White to move first and checkmate Black within a specified number of moves, against any defense. These are often referred to as "mate in n" – for example "mate in three" (a three-mover); two- and three-move problems are the most common. These usually involve positions that would be highly unlikely to occur in an actual game, and are intended to illustrate a particular theme, usually requiring a surprising or counter-intuitive key move.Studies: orthodox problems where the stipulation is that White to play must win or draw. Almost all studies are endgame positions.Chess composition is a distinct branch of chess sport, and tournaments exist for both the composition and solving of chess problems. Example This is one of the most famous chess studies; it was published by Richard Réti 4 December 1921. It seems impossible to catch the advanced black pawn, while the black king can easily stop the white pawn. The solution is a diagonal advance, which brings the king to both pawns simultaneously:1. Kg7! h4 2. Kf6! Kb6Or 2...h3 3.Ke7 and the white king can support its pawn.3. Ke5!!Now the white king comes just in time to support his pawn, or catch the black one.3... h3 4. Kd6 ½–½ Competitive play  Organization of competitions Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Chess's international governing body is FIDE (Fédération Internationale des Échecs). Most countries have a national chess organization as well (such as the US Chess Federation and English Chess Federation) which in turn is a member of FIDE. FIDE is a member of the International Olympic Committee, but the game of chess has never been part of the Olympic Games; chess does have its own Olympiad, held every two years as a team event.The current World Chess Champion is Magnus Carlsen of Norway. The reigning Women's World Champion is Hou Yifan from China. The world's highest rated female player, Judit Polgár, has never participated in the Women's World Chess Championship, instead preferring to compete with the leading men and maintaining a ranking among the top male players.Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, and the National Chess Championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.Regular team chess events include the Chess Olympiad and the European Team Chess Championship. The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events.Besides these prestigious competitions, there are thousands of other chess tournaments, matches, and festivals held around the world every year catering to players of all levels. Chess is promoted as a "mind sport" by the Mind Sports Organisation, alongside other mental-skill games such as Contract Bridge, Go, and Scrabble. Titles and rankings The best players can be awarded specific lifetime titles by the world chess organization FIDE:Grandmaster (shortened as GM; sometimes International Grandmaster or IGM is used) is awarded to world-class chess masters. Apart from World Champion, Grandmaster is the highest title a chess player can attain. Before FIDE will confer the title on a player, the player must have an Elo chess rating (see below) of at least 2500 at one time and three favorable results (called norms) in tournaments involving other grandmasters, including some from countries other than the applicant's. There are other milestones a player can achieve to attain the title, such as winning the World Junior Championship.International Master (shortened as IM). The conditions are similar to GM, but less demanding. The minimum rating for the IM title is 2400.FIDE Master (shortened as FM). The usual way for a player to qualify for the FIDE Master title is by achieving a FIDE rating of 2300 or more.Candidate Master (shortened as CM). Similar to FM, but with a FIDE rating of at least 2200.All the titles are open to men and women. Separate women-only titles, such as Woman Grandmaster (WGM), are available. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the GM title, and most of the top ten women in 2006 hold the unrestricted GM title.As of August 2011, there are 1363 active grandmasters and 3153 international masters in the world. Top three countries with the largest numbers of grandmasters are Russia, Ukraine, and Germany, with 208, 78, and 76. The country with most grandmasters per capita is Iceland, with 11 GMs and 13 IMs among the population of 310,000.International titles are awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles, usually to the advanced players still under the level needed for international titles; an example is the Chess expert title used in the United States.In order to rank players, FIDE, ICCF, and national chess organizations use the Elo rating system developed by Arpad Elo. Elo is a statistical system based on the assumption that the chess performance of each player in his or her games is a random variable. Arpad Elo thought of a player's true skill as the average of that player's performance random variable, and showed how to estimate the average from results of player's games. The US Chess Federation implemented Elo's suggestions in 1960, and the system quickly gained recognition as being both fairer and more accurate than older systems; it was adopted by FIDE in 1970. The highest FIDE rating of all time, 2881, was achieved by Magnus Carlsen on the March 2014 FIDE rating list. Publications Chess has a very extensive literature. In 1913, the chess historian H. J. R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000. B.H. Wood estimated the number, as of 1949, to be about 20,000. David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed." There are two significant public chess libraries: the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals; and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books. Grandmaster Lothar Schmid owned the world's largest private collection of chess books and memorabilia. David DeLucia's chess library contains 7,000 to 8,000 chess books, a similar number of autographs (letters, score sheets, manuscripts), and about 1,000 items of "ephemera". Dirk Jan ten Geuzendam opines that DeLucia's collection "is arguably the finest chess collection in the world". Mathematics and computers The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess have been known of for hundreds of years. Combinatorics of chess and chess puzzles The number of legal positions in chess is estimated to be about 1043, and is provably less than 1047, with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number. Typically an average position has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or as many as 218.Chess has inspired many combinatorial puzzles, such as the knight's tour and the eight queens puzzle. Computer chess One of the most important mathematical challenges of chess is the development of algorithms that can play chess. The idea of creating a chess-playing machine dates to the 18th century; around 1769, the chess-playing automaton called The Turk became famous before being exposed as a hoax. Serious trials based on automata, such as El Ajedrecista, were too complex and limited to be useful.Since the advent of the digital computer in the 1950s, chess enthusiasts, computer engineers and computer scientists have built, with increasing degrees of seriousness and success, chess-playing machines and computer programs. The groundbreaking paper on computer chess, "Programming a Computer for Playing Chess", was published in 1950 by Shannon. He wrote:The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of modern computers.The Association for Computing Machinery (ACM) held the first major chess tournament for computers, the North American Computer Chess Championship, in September 1970. CHESS 3.0, a chess program from Northwestern University, won the championship. Nowadays, chess programs compete in the World Computer Chess Championship, held annually since 1974. At first considered only a curiosity, the best chess playing programs have become extremely strong. In 1997, a computer won a chess match using classical time controls against a reigning World Champion for the first time: IBM's Deep Blue beat Garry Kasparov 3½–2½ (it scored two wins, one loss, and three draws). In 2009, a mobile phone won a category 6 tournament with a performance rating 2898: chess engine Hiarcs 13 running on the mobile phone HTC Touch HD won the Copa Mercosur tournament with nine wins and one draw. The best chess programs are now able to consistently beat the strongest human players, to the extent that human-computer matches no longer attract interest from chess players or media.With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents all over the world. The presence of computers and modern communication tools have raised concerns regarding cheating during games, most notably the "bathroom controversy" during the 2006 World Championship. Relation to game theory In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered as one of the predecessors of game theory. Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either white can force a win, or black can force a win, or both sides can force at least a draw). However, according to Claude Shannon, there are 1043 legal positions in chess, so it will take an impossibly long time to compute a perfect strategy with any feasible technology.The 11-category, game theoretical taxonomy of chess includes: two player, no-chance, combinatorial, Markov state (present state is all a player needs to move; although past state led up to that point, knowledge of the sequence of past moves is not required to make the next move, except to take into account of en passant and castling, which do depend on the past moves), zero sum, symmetric, perfect information, non-cooperative, discrete, extensive form (tree decisions, not payoff matrices), sequential. Computational complexity Generalized chess (played on n × n board, without the fifty-move rule) is EXPTIME-complete. Combinatorial game theory Some applications of combinatorial game theory to chess endgames were found by Elkies (1996). Psychology There is an extensive scientific literature on chess psychology. Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise. In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position. According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about half a dozen positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to a lot of research recently. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess. Recent research indicates that factors other than practice are also important. For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill. Chess and intelligence Although the link between performance in chess and general intelligence is often assumed, researchers have largely failed to confirm its existence. For example, a 2006 study found no differences in fluid intelligence, as measured by Raven's Progressive Matrices, between strong adult chess players and regular people. There is some evidence towards a correlation between performance in chess and intelligence among beginning players. However, performance in chess also relies substantially on one's experience playing the game, and the role of experience may overwhelm the role of intelligence. Chess experts are estimated to have in excess of 10,000 and possibly as many as 300,000 position patterns stored in their memory; long training is necessary to acquire that amount of data.A 2007 study of young chess players in the United Kingdom found that strong players tended to have above-average IQ scores, but, within that group, the correlation between chess skill and IQ was moderately negative, meaning that smarter children tended to achieve a lower level of chess skill. This result was explained by a negative correlation between intelligence and practice in the elite subsample, and by practice having a higher influence on chess skill. Variants There are more than two thousand published chess variants, most of them of relatively recent origin, including:Direct predecessors of chess such as chaturanga and shatranj.Traditional national or regional games that share common ancestors with Western chess such as xiangqi, shogi, janggi, makruk, and sittuyin.Modern variations employing different rules (e.g. Losing chess), different forces (e.g. Dunsany's chess), non-standard pieces (e.g. Grand Chess), or different board geometries (e.g. hexagonal chess). One of the most popular is Chess960, where the starting position is selected randomly, rendering the use of prepared opening lines impracticable.Infinite chess, which has drawn the attention of mathematicians.In the context of chess variants, regular (i.e. FIDE) chess is sometimes referred to as Western chess, international chess, orthodox chess, or orthochess. See also Reference aidsOutline of chess (subject-wide table of contents)Glossary of chessIndex of chess articlesListsList of chess booksList of chess gamesList of chess playersList of chess world championship matchesList of strong chess tournaments Notes  References Bibliography Further reading Dunnington, Angus (2003). Chess Psychology: Approaching the Psychological Battle Both on and Off the Board. Everyman Chess. ISBN 978-1-85744-326-4. Fine, Reuben (1983). The World's Great Chess Games. Courier Dover Publications. ISBN 0-486-24512-8. OCLC 9394460. Hale, Benjamin (2008). Philosophy Looks at Chess. Open Court Publishing Company. ISBN 978-0-8126-9633-2. Kotov, Alexander (1971). Think Like a Grandmaster. B.T.Batsford Ltd. ISBN 0-7134-3160-1. Lasker, Emanuel (1960). Lasker's Manual of Chess. Dover. ISBN 0-486-20640-8. Mason, James (1947). The Art of Chess. Dover Publications. ISBN 0-486-20463-4. OCLC 45271009.  (see the included supplement, "How Do You Play Chess")Pachman, Ludek (1971). Modern Chess Strategy. Dover. ISBN 0-486-20290-9. Reti, Richard (1960). Modern Ideas in Chess. Dover. ISBN 0-486-20638-6. Rizzitano, James (2004). Understanding Your Chess. Gambit Publications. ISBN 1-904600-07-7. OCLC 55205602.  External links Chess at DMOZInternational organizationsFIDE – World Chess FederationICCF – International Correspondence Chess FederationNewsChessbase newsThe Week in Chess
Income is the consumption and savings opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. However, for households and individuals, "income is the sum of all the wages, salaries, profits, interests payments, rents, and other forms of earnings received... in a given period of time."In the field of public economics, the term may refer to the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income. Increase in income Income per capita has been increasing steadily in almost every country. Many factors contribute to people having a higher income such as education, globalisation and favorable political circumstances such as economic freedom and peace. Increase in income also tends to lead to people choosing to work less hours. Developed countries (defined as countries with a "developed economy") have higher incomes as opposed to developing countries tending to have lower incomes. Economic definitions In economics, "factor income" is the return accruing for a person, or a nation, derived from the "factors of production": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.From labor services, as well as ownership of land and capital.In consumer theory 'income' is another name for the "budget constraint," an amount                     Y              {\displaystyle Y}   to be spent on different goods x and y in quantities                     x              {\displaystyle x}   and                     y              {\displaystyle y}   at prices                               P                      x                                {\displaystyle P_{x}}   and                               P                      y                                {\displaystyle P_{y}}  . The basic equation for this is                    Y                          P                      x                          ⋅        x        +                  P                      y                          ⋅        y              {\displaystyle YP_{x}\cdot x+P_{y}\cdot y}  This equation implies two things. First buying one more unit of good x implies buying                                                         P                              x                                                    P                              y                                                          {\displaystyle {\frac {P_{x}}{P_{y}}}}   less units of good y. So,                                                         P                              x                                                    P                              y                                                          {\displaystyle {\frac {P_{x}}{P_{y}}}}   is the relative price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed                     Y              {\displaystyle Y}  , then its relative price falls. The usual hypothesis is that the quantity demanded of x would increase at the lower price, the law of demand. The generalization to more than two goods consists of modelling y as a composite good.The theoretical generalization to more than one period is a multi-period wealth and income constraint. For example, the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis. Full and Haig-Simons income "Full income" refers to the accumulation of both the monetary and the non-monetary consumption-ability of any given entity, such as a person or a household. According to what the economist Nicholas Barr describes as the "classical definition of income" (the 1938 Haig-Simons definition): "income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights..." Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, i.e. failing to accurately reflect affluence (and thus the consumption opportunities) of any given agent. It omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, "in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full-income prevent a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income. Income inequality Income inequality refers to the extent to which income is distributed in an uneven manner. It can be measured by various methods, including the Lorenz curve and the Gini coefficient. Economists generally agree that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice.National income, measured by statistics such as the Net National Income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see measures of national income and output. Income in philosophy and ethics Throughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' (1 Timothy 6:10 (ASV)).Some scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both the individual and the national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his Theory of Moral Sentiments, and has more recently been developed by Harvard economist Benjamin Friedman in his book The Moral Consequences of Economic Growth. Accountancy The International Accounting Standards Board (IASB) uses the following definition: "Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants." [F.70] (IFRS Framework).According to John Hicks' definitions, income "is the maximum amount which can be spent during a period if there is to be an expectation of maintaining intact, the capital value of prospective receipts (in money terms)”. See also Basic incomeComprehensive incomeIncome taxUnpaid workers References D. Usher (1987). "real income," The New Palgrave: A Dictionary of Economics, v. 4, pp. 104–05
Graphic design is the process of visual communication and problem-solving using one or more of typography, photography and illustration. The field is considered a subset of visual communication and communication design, but sometimes the term "graphic design" is used synonymously. Graphic designers create and combine symbols, images and text to form visual representations of ideas and messages. They use typography, visual arts and page layout techniques to create visual compositions. Common uses of graphic design include corporate design (logos and branding), editorial design (magazines, newspapers and books), wayfinding or environmental design, advertising, web design, communication design, product packaging and signage. History The term graphic design was coined by William Addison Dwiggins in 1922. However, graphic design-like activities span human existence: from the caves of Lascaux, to Rome's Trajan's Column to the illuminated manuscripts of the Middle Ages, to the neon lights of Ginza, Tokyo. In "Babylon, artisans pressed cuneiform inscriptions into clay bricks or tablets which were used for construction. The bricks gave information such as the name of the reigning monarch, the builder, or some other dignitary". This was the first known road sign announcing the name of the governor of a state or mayor of the city. The Egyptians developed communication by hieroglyphics that used picture symbols dating as far back as 136 B.C. found on the Rosetta Stone. "The Rosetta stone, found by one of Napoleon's engineers was an advertisement for the Egyptian ruler, Ptolemy as the "true Son of the Sun, the Father of the Moon, and the Keeper of the Happiness of Men""  The Egyptians also invented papyrus, paper made from reeds found along the Nile, on which they transcribed advertisements more common among their people at the time. During the "Dark Ages", from 500 AD to 1450 AD, monks created elaborate, illustrated manuscripts.In both its lengthy history and in the relatively recent explosion of visual communication in the 20th and 21st centuries, the distinction between advertising, art, graphic design and fine art has disappeared. They share many elements, theories, principles, practices, languages and sometimes the same benefactor or client. In advertising, the ultimate objective is the sale of goods and services. In graphic design, "the essence is to give order to information, form to ideas, expression, and feeling to artifacts that document human experience."Graphic design in the United States began with Benjamin Franklin who used his newspaper The Pennsylvania Gazette, to master the art of publicity to promote his own books and to influence the masses. "Benjamin Franklin's ingenuity gained in strength as did his cunning and in 1737 he had replaced his counterpart in Pennsylvania, Andrew Bradford as postmaster and printer after a competition he instituted and won. He showed his prowess by running an ad in his General Magazine and the Historical Chronicle of British Plantations in America (the precursor to the Saturday Evening Post) that stressed the benefits offered by a stove he invented, named called the Pennsylvania Fireplace. His invention is still sold today and is known as the Franklin stove. "American advertising initially imitated British newspapers and magazines. Advertisements were printed in scrambled type and uneven lines that made it difficult to read. Franklin better organized this by adding 14-point type for the first line of the advertisement; although later shortened and centered it, making "headlines". Franklin added illustrations, something that London printers had not attempted. Franklin was the first to utilize logos, which were early symbols that announced such services as opticians by displaying golden spectacles. Franklin taught advertisers that the use of detail was important in marketing their products. Some advertisements ran for 10-20 lines, including color, names, varieties, and sizes of the goods that were offered. The advent of printing During the Tang Dynasty (618–907) wood blocks were cut to print on textiles and later to reproduce Buddhist texts. A Buddhist scripture printed in 868 is the earliest known printed book. Beginning in the 11th century, longer scrolls and books were produced using movable type printing, making books widely available during the Song dynasty (960–1279).During the 17th-18th century movable type was used for handbills or trade cards which were printed from wood or copper engravings. These documents announced a business and its location. English painter William Hogarth used his skill in engraving was one of the first to design for business trade.In Mainz Germany, in 1448, Johann Gutenberg introduced movable type using a new metal alloy for use in a printing press and opened a new era of commerce. Previously, most advertising was word of mouth. In France and England, for example, criers announced products for sale just as ancient Romans had done.The printing press made books more widely available. Aldus Manutius developed the book structure that became the foundation of western publication design. This era of graphic design is called Humanist or Old Style. Additionally, William Caxton, England's first printer produced religious books, but had trouble selling them. He discovered the use of leftover pages and used them to announce the books and post them on church doors. This practice was termed "squis" or "pin up" posters, in approximately 1612, becoming the first form of print advertising in Europe. The term Siquis came from the Roman era when public notices were posted stating "if anybody...", which is Latin for "si quis". These printed announcements were followed by later public registers of wants called want ads and in some areas such as the first periodical in Paris advertising was termed "advices". The "Advices" were what we know today as want ad media or advice columns.In 1638 Harvard University received a printing press from England. More than 52 years passed before London bookseller Benjamin Harris received another printing press in Boston. Harris published a newspaper in serial form, 'Publick Occurrences Both Foreign and Domestick'. It was four pages long and suppressed by the government after its first edition.John Campbell is credited for the first newspaper, the 'Boston News-Letter', which appeared in 1704. The paper was known during the revolution as "Weeklies". The name came from the 13 hours required for the ink to dry on each side of the paper. 'The solution was to first, print the ads and then to print the news on the other side the day before publication. The paper was four pages long having ads on at least 20%-30% of the total paper, (pages one and four) the hot news was located on the inside.' The initial use of the Boston News-Letter carried Campbell's own solicitations for advertising from his readers. Campbell's first paid advertisement was in his third edition, May 7 or 8th, 1704. Two of the first ads were for stolen anvils. The third was for real estate in Oyster Bay, owned by William Bradford, a pioneer printer in New York, and the first to sell something of value. Bradford published his first newspaper in 1725, New York's first, The New York Gazette. Bradford's son preceded him in Philadelphia publishing the American Weekly Mercury, 1719. The Mercury and William Brooker's Massachusetts Gazette, first published a day earlier. Design industry In late 19th-century Europe, especially in the United Kingdom, the first official publication of a printed design was released, marking the separation of graphic design from fine art.In 1849, Henry Cole became one of the major forces in design education in Great Britain, informing the government of the importance of design in his Journal of Design and Manufactures. He organized the Great Exhibition as a celebration of modern industrial technology and Victorian design.From 1891 to 1896, William Morris' Kelmscott Press published some of the most significant of the graphic design products of the Arts and Crafts movement, and made a lucrative business of creating and selling stylish books. Morris created a market for works of graphic design in their own right and a profession for this new type of art. The Kelmscott Press is characterized by an obsession with historical styles. This historicism was the first significant reaction to the state of nineteenth-century graphic design. Morris' work, along with the rest of the Private Press movement, directly influenced Art Nouveau. Twentieth century design The term "graphic design" first appeared in print in the 1922 essay "New Kind of Printing Calls for New Design" by William Addison Dwiggins, an American book designer in the early 20th century. Raffe's Graphic Design, published in 1927, was the first book to use "Graphic Design" in its title.The signage in the London Underground is a classic design example of the modern era and used a typeface designed by Edward Johnston in 1916.In the 1920s, Soviet constructivism applied 'intellectual production' in different spheres of production. The movement saw individualistic art as useless in revolutionary Russia and thus moved towards creating objects for utilitarian purposes. They designed buildings, theater sets, posters, fabrics, clothing, furniture, logos, menus, etc.Jan Tschichold codified the principles of modern typography in his 1928 book, New Typography. He later repudiated the philosophy he espoused in this book as fascistic, but it remained influential. Tschichold, Bauhaus typographers such as Herbert Bayer and László Moholy-Nagy and El Lissitzky greatly influenced graphic design. They pioneered production techniques and stylistic devices used throughout the twentieth century. The following years saw graphic design in the modern style gain widespread acceptance and application. The post-World War II American economy revealed a greater need for graphic design, mainly in advertising and packaging. The spread of the German Bauhaus school of design to Chicago in 1937 brought a "mass-produced" minimalism to America; sparking "modern" architecture and design. Notable names in mid-century modern design include Adrian Frutiger, designer of the typefaces Univers and Frutiger; Paul Rand, who took the principles of the Bauhaus and applied them to popular advertising and logo design, helping to create a uniquely American approach to European minimalism while becoming one of the principal pioneers of the subset of graphic design known as corporate identity; and Josef Müller-Brockmann, who designed posters in a severe yet accessible manner typical of the 1950s and 1970s era.The professional graphic design industry grew in parallel with consumerism. This raised concerns and criticisms, notably from within the graphic design community with the First Things First manifesto. First launched by Ken Garland in 1964, it was re-published as the First Things First 2000 manifesto in 1999 in the magazine Emigre 51 stating "We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication - a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design." Both editions attracted signatures from practitioners and thinkers such as Rudy VanderLans, Erik Spiekermann, Ellen Lupton and Rick Poynor. The 2000 manifesto was also published in Adbusters, known for its strong critiques of visual culture. Applications Graphic design is applied to everything visual, from road signs to technical schematics, from interoffice memorandums to reference manuals.Design can aid in selling a product or idea. It is applied to products and elements of company identity such as logos, colors, packaging and text as part of branding (see also advertising). Branding has increasingly become important in the range of services offered by graphic designers. Graphic designers often form part of a branding team.Textbooks are designed to present subjects such as geography, science and math. These publications have layouts that illustrate theories and diagrams. Graphic design also applied to layout, formatting, illustrations and charts.Graphic design is applied in the entertainment industry in decoration, scenery and visual story telling. Other examples of design for entertainment purposes include novels, comic books, DVD covers, opening credits and closing credits in filmmaking, and programs and props on stage. This could also include artwork used for T-shirts and other items screenprinted for sale.From scientific journals to news reporting, the presentation of opinion and facts is often improved with graphics and thoughtful compositions of visual information - known as information design. Newspapers, magazines, blogs, television and film documentaries may use graphic design. With the advent of the web, information designers with experience in interactive tools are increasingly used to illustrate the background to news stories. Skills A graphic design project may involve the stylization and presentation of existing text and either preexisting imagery or images developed by the graphic designer. Elements can be incorporated in both traditional and digital form, which involves the use of visual arts, typography, and page layout techniques. Graphic designers organize pages and optionally add graphic elements. Graphic designers can commission photographers or illustrators to create original pieces. Designers use digital tools, often referred to as interactive design, or multimedia design. Designers need communication skills to convince an audience and sell their designs.The "process school" is concerned with communication; it highlights the channels and media through which messages are transmitted and by which senders and receivers encode and decode these message. The semiotic school treats a message as a construction of signs which through interaction with receivers, produces meaning; communication as an agent. Typography Typography includes type design, modifying type glyphs and arranging type. Type glyphs (characters) are created and modified using illustration techniques. Type arrangement is the selection of typefaces, point size, tracking (the space between all characters used), kerning (the space between two specific characters) and leading (line spacing).Typography is performed by typesetters, compositors, typographers, graphic artists, art directors and clerical workers. Until the digital age, typography was a specialized occupation. Page layout Page layout deals with the arrangement of elements (content) on a page, such as image placement, text layout and style. Page design has always been a consideration in printed material and more recently extended to displays such as web pages. Elements typically consist of type (text), images (pictures), and (with print media) occasionally place-holder graphics for elements that are not printed with ink such as die/laser cutting, foil stamping or blind embossing. Printmaking Printmaking is the process of making artworks by printing on paper and other materials or surfaces. The process is capable of producing multiples of the same work, each called a print. Each print is an original, technically known as an impression. Prints are created from a single original surface, technically a matrix. Common types of matrices include: plates of metal, usually copper or zinc for engraving or etching; stone, used for lithography; blocks of wood for woodcuts, linoleum for linocuts and fabric plates for screen-printing. Works printed from a single plate create an edition, in modern times usually each signed and numbered to form a limited edition. Prints may be published in book form, as artist's books. A single print could be the product of one or multiple techniques.Aside from technology, graphic design requires judgment and creativity. Critical, observational, quantitative and analytic thinking are required for design layouts and rendering. If the executor is merely following a solution (e.g. sketch, script or instructions) provided by another designer (such as an art director), then the executor is not usually considered the designer. Tools The method of presentation (e.g. arrangement, style, medium) is important to the design. The development and presentation tools can change how an audience perceives a project. The image or layout is produced using traditional media and guides, or digital image editing tools on computers. Tools in computer graphics often take on traditional names such as "scissors" or "pen". Some graphic design tools such as a grid are used in both traditional and digital form.In the mid-1980s desktop publishing and graphic art software applications introduced computer image manipulation and creation capabilities that had previously been manually executed. Computers enabled designers to instantly see the effects of layout or typographic changes, and to simulate the effects of traditional media. Traditional tools such as pencils can be useful even when computers are used for finalization; a designer or art director may sketch numerous concepts as part of the creative process. Styluses can be used with tablet computers to capture hand drawings digitally. Computers Some designers dispute that computers enhance the creative process. Computers allow designers to explore multiple ideas quickly and in more detail than can be achieved by hand-rendering or paste-up. However, some designers find the limitless choices from digital design lead to paralysis or to endless iterations with no clear outcome.Hand-rendered layouts can be used to get approval for an idea execution before a designer invests time to produce finished visuals. Then the idea is finished on the computer in a hybrid process.Nearly all of the popular and "industry standard" software programs used for graphic design since the early 1990s are products of Adobe Systems Incorporated. Adobe Photoshop (a raster-based program for photo editing) and Adobe Illustrator (a vector-based program for drawing) are often used in the final stage. Designers often use pre-designed raster images and vector graphics in their work from online design databases. Raster images may be edited in Adobe Photoshop, logos and illustrations in Adobe Illustrator, and the final product assembled in one of the major page layout programs, including Adobe InDesign. Graphic designers are expected to be proficient in these programs. Related Design Fields  Interface design Since the advent of personal computers, many graphic designers have become involved in interface design, in an environment commonly referred to as a Graphical User Interface (GUI). This has included web design and software design, when end user interactivity is a design consideration of the layout or interface. Combining visual communication skills with an understanding of user interaction and online branding, graphic designers often work with software developers and web developers to create the look and feel of a web site or software application. An important aspect of interface design is icon design. User experience design User experience design considers how a user interacts with and responds to an interface, service or product. Experiential graphic design Experiential graphic design is the application of communication skills to the built environment. This area of graphic design requires practitioners to understand physical installations that have to be manufactured and withstand the same environmental conditions as buildings. As such, it is a cross-disciplinary collaborative process involving designers, fabricators, city planners, architects, manufacturers and construction teams.Experiential graphic designers try to solve problems that people encounter while interacting with buildings and space. Examples of practice areas for environmental graphic designers are wayfinding, placemaking, branded environments, exhibitions and museum displays, public installations and digital environments. Occupations Graphic design career paths cover all parts of the creative spectrum and often overlap. Workers perform specialized tasks, such as design services, publishing, advertising and public relations. As of 2016 median pay was $53,316 per year. The main job titles within the industry are often country specific. They can include graphic designer, art director, creative director, animator and entry level production artist. Depending on the industry served, the responsibilities may have different titles such as "DTP Associate" or "Graphic Artist". The responsibilities may involve specialized skills such as illustration, photography, animation or interactive design.Employment in design of online projects was expected to increase by 35%, while traditional designs, such as publications, faced slower rates of 16% or less.Graphic designers can work within companies devoted specifically to the industry, such as design consultancies or branding agencies, others may work within publishing, marketing or other communications companies. Especially since the introduction of personal computers, many graphic designers work as in-house designers in non-design oriented organizations. Graphic designers may also work freelance, working on their own terms, prices, ideas, etc.A graphic designer typically reports to the art director, creative director or senior media creative. As a designer becomes more senior, they spend less time designing and more time leading and directing other designers on broader creative activities, such as brand development and corporate identity development. They are often expected to interact more directly with clients, for example taking and interpreting briefs. Crowdsourcing in graphic design Jeff Howe of Wired Magazine first used the term "crowdsourcing" in his 2006 article, "The Rise of Crowdsourcing." It spans such creative domains as graphic design, architecture, apparel design, writing, illustration etc. Tasks may be assigned to individuals or a group and may be categorized as convergent or divergent. An example of a divergent task is generating alternative designs for a poster. An example of a convergent task is selecting one poster design. See also  Related areas  Related topics  References  Bibliography Fiell, Charlotte and Fiell, Peter (editors). Contemporary Graphic Design. Taschen Publishers, 2008. ISBN 978-3-8228-5269-9Wiedemann, Julius and Taborda, Felipe (editors). Latin-American Graphic Design. Taschen Publishers, 2008. ISBN 978-3-8228-4035-1 External links  Media related to Graphic design at Wikimedia CommonsThe Universal Arts of Graphic Design – Documentary produced by Off BookGraphic Designers, entry in the Occupational Outlook Handbook of the Bureau of Labor Statistics of the United States Department of Labor
A motorcycle (also called a motorbike, bike, or cycle) is a two- or three-wheeled motor vehicle. Motorcycle design varies greatly to suit a range of different purposes: long distance travel, commuting, cruising, sport including racing, and off-road riding. Motorcycling is riding a motorcycle and related social activity such as joining a motorcycle club and attending motorcycle rallies.In 1894, Hildebrand & Wolfmüller became the first series production motorcycle, and the first to be called a motorcycle. In 2014, the three top motorcycle producers globally by volume were Honda, Yamaha (both from Japan), and Hero MotoCorp (India).In developing countries, motorcycles are overwhelmingly utilitarian due to lower prices and greater fuel economy. Of all the motorcycles in the world, 58% are in the Asia-Pacific and Southern and Eastern Asia regions, excluding car-centric Japan.According to the United States Department of Transportation the number of fatalities per vehicle mile traveled was 37 times higher for motorcycles than for cars. Types The term motorcycle has different legal definitions depending on jurisdiction (see #Legal definitions and restrictions).There are three major types of motorcycle: street, off-road, and dual purpose. Within these types, there are many sub-types of motorcycles for different purposes. There is often a racing counterpart to each type, such as road racing and street bikes, or motocross and dirt bikes.Street bikes include cruisers, sportbikes, scooters and mopeds, and many other types. Off-road motorcycles include many types designed for dirt-oriented racing classes such as motocross and are not street legal in most areas. Dual purpose machines like the dual-sport style are made to go off-road but include features to make them legal and comfortable on the street as well.Each configuration offers either specialised advantage or broad capability, and each design creates a different riding posture. History  Experimentation and invention The first internal combustion, petroleum fueled motorcycle was the Daimler Reitwagen. It was designed and built by the German inventors Gottlieb Daimler and Wilhelm Maybach in Bad Cannstatt, Germany in 1885. This vehicle was unlike either the safety bicycles or the boneshaker bicycles of the era in that it had zero degrees of steering axis angle and no fork offset, and thus did not use the principles of bicycle and motorcycle dynamics developed nearly 70 years earlier. Instead, it relied on two outrigger wheels to remain upright while turning.The inventors called their invention the Reitwagen ("riding car"). It was designed as an expedient testbed for their new engine, rather than a true prototype vehicle.The first commercial design for a self-propelled cycle was a three-wheel design called the Butler Petrol Cycle, conceived of Edward Butler in England in 1884. He exhibited his plans for the vehicle at the Stanley Cycle Show in London in 1884. The vehicle was built by the Merryweather Fire Engine company in Greenwich, in 1888.The Butler Petrol Cycle was a three-wheeled vehicle, with the rear wheel directly driven by a 5/8hp (466W) 600 cc (40 in3; 2¼×5-inch {57×127-mm}) flat twin four stroke engine (with magneto ignition replaced by coil and battery) equipped with rotary valves and a float-fed carburettor (five years before Maybach) and Ackermann steering, all of which were state of the art at the time. Starting was by compressed air. The engine was liquid-cooled, with a radiator over the rear driving wheel. Speed was controlled by means of a throttle valve lever. No braking system was fitted; the vehicle was stopped by raising and lowering the rear driving wheel using a foot-operated lever; the weight of the machine was then borne by two small castor wheels. The driver was seated between the front wheels. It wasn't, however, a success, as Butler failed to find sufficient financial backing.Many authorities have excluded steam powered, electric motorcycles or diesel-powered two-wheelers from the definition of a 'motorcycle', and credit the Daimler Reitwagen as the world's first motorcycle. Given the rapid rise in use of electric motorcycles worldwide, defining only internal-combustion powered two-wheelers as 'motorcycles' is increasingly problematic.If a two-wheeled vehicle with steam propulsion is considered a motorcycle, then the first motorcycles built seem to be the French Michaux-Perreaux steam velocipede which patent application was filled in December 1868, constructed around the same time as the American Roper steam velocipede, built by Sylvester H. Roper Roxbury, Massachusetts. who demonstrated his machine at fairs and circuses in the eastern U.S. in 1867, Roper built about 10 steam cars and cycles from the 1860s until his death in 1896. Summary of early inventions  First motorcycle companies In 1894, Hildebrand & Wolfmüller became the first series production motorcycle, and the first to be called a motorcycle (German: Motorrad). Excelsior Motor Company, originally a bicycle manufacturing company based in Coventry, England, began production of their first motorcycle model in 1896. The first production motorcycle in the US was the Orient-Aster, built by Charles Metz in 1898 at his factory in Waltham, Massachusetts.In the early period of motorcycle history, many producers of bicycles adapted their designs to accommodate the new internal combustion engine. As the engines became more powerful and designs outgrew the bicycle origins, the number of motorcycle producers increased. Many of the nineteenth century inventors who worked on early motorcycles often moved on to other inventions. Daimler and Roper, for example, both went on to develop automobiles.At the turn of the century the first major mass-production firms were set up. In 1898, Triumph Motorcycles in England began producing motorbikes, and by 1903 it was producing over 500 bikes. Other British firms were Royal Enfield, Norton and Birmingham Small Arms Company who began motorbike production in 1899, 1902 and 1910, respectively. Indian began production in 1901 and Harley-Davidson was established two years later. By the outbreak of the First World War, the largest motorcycle manufacturer in the world was Indian, producing over 20,000 bikes per year. First World War During the First World War, motorbike production was greatly ramped up for the war effort to supply effective communications with front line troops. Messengers on horses were replaced with despatch riders on motorcycles carrying messages, performing reconnaissance and acting as a military police. American company Harley-Davidson was devoting over 50% of its factory output toward military contract by the end of the war. The British company Triumph Motorcycles sold more than 30,000 of its Triumph Type H model to allied forces during the war. With the rear wheel driven by a belt, the Model H was fitted with a 499 cc (30.5 cu in) air-cooled four-stroke single-cylinder engine. It was also the first Triumph without pedals.The Model H in particular, is regarded by many as having been the first "modern motorcycle". Introduced in 1915 it had a 550 cc side-valve four-stroke engine with a three-speed gearbox and belt transmission. It was so popular with its users that it was nicknamed the "Trusty Triumph." Postwar By 1920, Harley-Davidson was the largest manufacturer, with their motorcycles being sold by dealers in 67 countries. By the late 1920s or early 1930s, DKW in Germany took over as the largest manufacturer.In the 1950s, streamlining began to play an increasing part in the development of racing motorcycles and the "dustbin fairing" held out the possibility of radical changes to motorcycle design. NSU and Moto Guzzi were in the vanguard of this development, both producing very radical designs well ahead of their time. NSU produced the most advanced design, but after the deaths of four NSU riders in the 1954–1956 seasons, they abandoned further development and quit Grand Prix motorcycle racing.Moto Guzzi produced competitive race machines, and by 1957 nearly all the Grand Prix races were being won by streamlined machines. The following year, 1958, full enclosure fairings were banned from racing by the FIM in the light of the safety concerns.From the 1960s through the 1990s, small two-stroke motorcycles were popular worldwide, partly as a result of East German Walter Kaaden's engine work in the 1950s. Today In the 21st century, the motorcycle industry is mainly dominated by the Chinese motorcycle industry and by Japanese motorcycle companies. In addition to the large capacity motorcycles, there is a large market in smaller capacity (less than 300 cc) motorcycles, mostly concentrated in Asian and African countries and produced in China and India. A Japanese example is the 1958 Honda Super Cub, which went on to become the biggest selling vehicle of all time, with its 60 millionth unit produced in April 2008. Today, this area is dominated by mostly Indian companies with Hero MotoCorp emerging as the world's largest manufacturer of two wheelers. Its Splendor model has sold more than 8.5 million to date. Other major producers are Bajaj and TVS Motors. Technical aspects  Construction Motorcycle construction is the engineering, manufacturing, and assembly of components and systems for a motorcycle which results in the performance, cost, and aesthetics desired by the designer. With some exceptions, construction of modern mass-produced motorcycles has standardised on a steel or aluminium frame, telescopic forks holding the front wheel, and disc brakes. Some other body parts, designed for either aesthetic or performance reasons may be added. A petrol powered engine typically consisting of between one and four cylinders (and less commonly, up to eight cylinders) coupled to a manual five- or six-speed sequential transmission drives the swingarm-mounted rear wheel by a chain, driveshaft or belt. Fuel economy Motorcycle fuel economy varies greatly with engine displacement and riding style. A streamlined, fully faired Matzu Matsuzawa Honda XL125 achieved 470 mpg‑US (0.50 L/100 km; 560 mpg‑imp) in the Craig Vetter Fuel Economy Challenge "on real highways –  in real conditions." Due to low engine displacements (100 cc–200 cc), and high power-to-mass ratios, motorcycles offer good fuel economy. Under conditions of fuel scarcity like 1950s Britain and modern developing nations, motorcycles claim large shares of the vehicle market. Electric motorcycles Very high fuel economy equivalents are often derived by electric motorcycles. Electric motorcycles are nearly silent, zero-emission electric motor-driven vehicles. Operating range and top speed are limited by battery technology. Fuel cells and petroleum-electric hybrids are also under development to extend the range and improve performance of the electric drive system. Reliability A 2013 survey of 4,424 readers of the US Consumer Reports magazine collected reliability data on 4,680 motorcycles purchased new from 2009 to 2012. The most common problem areas were accessories, brakes, electrical (including starters, charging, ignition), and fuel systems, and the types of motorcycles with the greatest problems were touring, off road/dual sport, sport-touring, and cruisers. There were not enough sport bikes in the survey for a statistically significant conclusion, though the data hinted at reliability as good as cruisers. These results may be partially explained by accessories including such equipment as fairings, luggage, and auxiliary lighting, which are frequently added to touring, adventure touring/dual sport and sport touring bikes. Trouble with fuel systems is often the result of improper winter storage, and brake problems may also be due to poor maintenance. Of the five brands with enough data to draw conclusions, Honda, Kawasaki and Yamaha were statistically tied, with 11 to 14% of those bikes in the survey experiencing major repairs. Harley-Davidsons had a rate of 24%, while BMWs did worst, with 30% of those needing major repairs. There were not enough Triumph and Suzuki motorcycles surveyed for a statistically sound conclusion, though it appeared Suzukis were as reliable as the other three Japanese brands while Triumphs were comparable to Harley-Davidson and BMW. Three fourths of the repairs in the survey cost less than US$200 and two thirds of the motorcycles were repaired in less than two days. In spite of their relatively worse reliability in this survey, Harley-Davidson and BMW owners showed the greatest owner satisfaction, and three fourths of them said they would buy the same bike again, followed by 72% of Honda owners and 60 to 63% of Kawasaki and Yamaha owners. Dynamics Different types of motorcycles have different dynamics and these play a role in how a motorcycle performs in given conditions. For example, one with a longer wheelbase provides the feeling of more stability by responding less to disturbances. Motorcycle tyres have a large influence over handling.Motorcycles must be leaned in order to make turns. This lean is induced by the method known as countersteering, in which the rider momentarily steers the handlebars in the direction opposite of the desired turn. This practice is counterintuitive and therefore often confusing to novices –  and even many experienced motorcyclists.With such short wheelbase, motorcycles can generate enough torque at the rear wheel, and enough stopping force at the front wheel, to lift the opposite wheel off the road. These actions, if performed on purpose, are known as wheelies and stoppies (or endos) respectively. Accessories Various features and accessories may be attached to a motorcycle either as OEM (factory-fitted) or aftermarket. Such accessories are selected by the owner to enhance the motorcycle's appearance, safety, performance, or comfort, and may include anything from mobile electronics to sidecars and trailers. Safety Motorcycles have a higher rate of fatal accidents than automobiles or trucks and buses. United States Department of Transportation data for 2005 from the Fatality Analysis Reporting System show that for passenger cars, 18.62 fatal crashes occur per 100,000 registered vehicles. For motorcycles this figure is higher at 75.19 per 100,000 registered vehicles –  four times higher than for cars. The same data shows that 1.56 fatalities occur per 100 million vehicle miles travelled for passenger cars, whereas for motorcycles the figure is 43.47 which is 28 times higher than for cars (37 times more deaths per mile travelled in 2007). Furthermore, for motorcycles the accident rates have increased significantly since the end of the 1990s, while the rates have dropped for passenger cars.The most common configuration of motorcycle accidents in the United States is when a motorist pulls out or turns in front of a motorcyclist, violating their right-of-way. This is sometimes called a SMIDSY, an acronym formed from the motorists' common response of "Sorry mate, I didn't see you". Motorcyclists can anticipate and avoid some of these crashes with proper training, increasing their visibility to other traffic, keeping the speed limits, and not consuming alcohol or other drugs before riding.The United Kingdom has several organisations dedicated to improving motorcycle safety by providing advanced rider training beyond what is necessary to pass the basic motorcycle licence test. These include the Institute of Advanced Motorists (IAM) and the Royal Society for the Prevention of Accidents (RoSPA). Along with increased personal safety, riders with these advanced qualifications may benefit from reduced insurance costs In South Africa, the Think Bike campaign is dedicated to increasing both motorcycle safety and the awareness of motorcycles on the country's roads. The campaign, while strongest in the Gauteng province, has representation in Western Cape, KwaZulu Natal and the Free State. It has dozens of trained marshals available for various events such as cycle races and is deeply involved in numerous other projects such as the annual Motorcycle Toy Run.Motorcycle safety education is offered throughout the United States by organisations ranging from state agencies to non-profit organisations to corporations. Most states use the courses designed by the Motorcycle Safety Foundation (MSF), while Oregon and Idaho developed their own. All of the training programs include a Basic Rider Course, an Intermediate Rider Course and an Advanced Rider Course.In Ireland, since 2010, in the UK and some Australian jurisdictions, such as Victoria, New South Wales, the Australian Capital Territory, Tasmania and the Northern Territory, it is compulsory to complete a basic rider training course before being issued a Learners Licence, after which they can ride on public roads.In Canada, motorcycle rider training is compulsory in Quebec and Manitoba only, but all provinces and territories have graduated licence programs which place restrictions on new drivers until they have gained experience. Eligibility for a full motorcycle licence or endorsement for completing a Motorcycle Safety course varies by province. The Canada Safety Council, a non-profit safety organisation, offers the Gearing Up program across Canada and is endorsed by the Motorcycle and Moped Industry Council. Training course graduates may qualify for reduced insurance premiums. Motorcycle rider postures The motorcyclist's riding position depends on rider body-geometry (anthropometry) combined with the geometry of the motorcycle itself. These factors create a set of three basic postures.Sport –  the rider leans forward into the wind and the weight of the upper torso is supported by the rider's core at low speed and air pressure at high speed (e.g., above 50 mph (80 km/h)). The footpegs are below the rider or to the rear. The reduced frontal area cuts wind resistance and allows higher speeds. At low-speed this position throws the weight of the rider onto the arms, which can tire the rider's wrists.Standard –  the rider sits upright or leans forward slightly. The feet are below the rider. These are motorcycles that are not specialised to one task, so they do not excel in any particular area. The standard posture is used with touring and commuting as well as dirt and dual-sport bikes, and may offer advantages for beginners.Cruiser –  the rider sits at a lower seat height with the upper torso upright or leaning slightly rearward. Legs are extended forwards, sometimes out of reach of the regular controls on cruiser pegs. The low seat height can be a consideration for new or short riders. Handlebars tend to be high and wide. The emphasis is on comfort, while compromising cornering ability because of low ground clearance and the greater likelihood of scraping foot pegs, floor boards, or other parts if turns are taken at the speeds other motorcycles can more readily accomplish.Factors of a motorcycle's ergonomic geometry that determine the seating posture include the height, angle and location of footpegs, seat and handlebars. Factors in a rider's physical geometry that contribute to seating posture include torso, arm, thigh and leg length, and overall rider height. Legal definitions and restrictions A motorcycle is broadly defined by law in most countries for the purposes of registration, taxation and rider licensing as a powered two-wheel motor vehicle. Most countries distinguish between mopeds of 49 cc and the more powerful, larger vehicles (scooters do not count as a separate category). Many jurisdictions include some forms of three-wheeled cars as motorcycles. Environmental impact Motorcycles and scooters' low fuel consumption has attracted interest in the United States from environmentalists and those whom increased fuel prices affect. Piaggio Group Americas supported this interest with the launch of a "Vespanomics" website and platform, claiming lower per-mile carbon emissions of 0.4 lb/mile (113 g/km) less than the average car, a 65% reduction, and better fuel economy.However, a motorcycle's exhaust emissions may contain 10–20 times more oxides of nitrogen (NOx), carbon monoxide, and unburned hydrocarbons than exhaust from a similar-year passenger car or SUV. This is because many motorcycles lack a catalytic converter, and the emission standard is much more permissive for motorcycles than for other vehicles. While catalytic converters have been installed in most gasoline-powered cars and trucks since 1975 in the United States, they can present fitment and heat difficulties in motorcycle applications.United States Environmental Protection Agency 2007 certification result reports for all vehicles versus on highway motorcycles (which also includes scooters), the average certified emissions level for 12,327 vehicles tested was 0.734. The average "Nox+Co End-Of-Useful-Life-Emissions" for 3,863 motorcycles tested was 0.8531. 54% of the tested 2007-model motorcycles were equipped with a catalytic converter. United States emissions limits The following table shows maximum acceptable legal emissions of the combination of hydrocarbons, oxides of nitrogen, and carbon monoxide for new motorcycles sold in the United States with 280 cc or greater piston displacement.The maximum acceptable legal emissions of hydrocarbon and carbon monoxide for new Class I and II motorcycles (50 cc–169 cc and 170 cc–279 cc respectively) sold in the United States are as follows: Europe European emission standards for motorcycles are similar to those for cars. New motorcycles must meet Euro III standards, while cars must meet Euro V standards. Motorcycle emission controls are being updated and it has been proposed to update to Euro IV in 2012 and Euro V in 2015. Guinness World Record The longest motorcycle in the world was created by Bharat Sinh Parmar in India. It measures at 26.29 m (86 ft 3 in) long. See also List of motorcycle manufacturersMotorcycle industry in China Notes  References G.N. Georgano (2002). Early and Vintage Years, 1885-1930: The Golden Era of Coachbuilding. Mason Crest Publishers.  External links Motorcycles at DMOZ
Badminton is a racquet sport played using racquets to hit a shuttlecock across a net. Although it may be played with larger teams, the most common forms of the game are "singles" (with one player per side) and "doubles" (with two players per side). Badminton is often played as a casual outdoor activity in a yard or on a beach; formal games are played on a rectangular indoor court. Points are scored by striking the shuttlecock with the racquet and landing it within the opposing side's half of the court.Each side may only strike the shuttlecock once before it passes over the net. Play ends once the shuttlecock has struck the floor or if a fault has been called by the umpire, service judge, or (in their absence) the opposing side.The shuttlecock is a feathered or (in informal matches) plastic projectile which flies differently from the balls used in many other sports. In particular, the feathers create much higher drag, causing the shuttlecock to decelerate more rapidly. Shuttlecocks also have a high top speed compared to the balls in other racquet sports.The game developed in British India from the earlier game of battledore and shuttlecock. European play came to be dominated by Denmark but the game has become very popular in Asia, with recent competition dominated by China. Since 1992, badminton has been a Summer Olympic sport with five events: men's singles, women's singles, men's doubles, women's doubles, and mixed doubles. At high levels of play, the sport demands excellent fitness: players require aerobic stamina, agility, strength, speed, and precision. It is also a technical sport, requiring good motor coordination and the development of sophisticated racquet movements. History Games employing shuttlecocks have been played for centuries across Eurasia but the modern game of badminton developed in the mid-19th century among the British as a variant of the earlier game of battledore and shuttlecock. ("Battledore" was an older term for "racquet".) Its exact origin remains obscure. The name derives from the Duke of Beaufort's Badminton House in Gloucestershire, but why or when remains unclear. As early as 1860, a London toy dealer named Isaac Spratt published a booklet titled Badminton Battledore—A New Game but unfortunately no copy has survived. An 1863 article in The Cornhill Magazine describes badminton as "battledore and shuttlecock played with sides, across a string suspended some five feet from the ground".The game may have originally developed among expatriate officers in British India, where it was very popular by the 1870s. Ball badminton, a form of the game played with a wool ball instead of a shuttlecock, was being played in Thanjavur as early as the 1850s and was at first played interchangeably with badminton by the British, the woollen ball being preferred in windy or wet weather.Early on, the game was also known as Poona or Poonah after the garrison town of Pune, where it was particularly popular and where the first rules for the game were drawn up in 1873. By 1875, returning officers had started a badminton club in Folkestone. Initially, the sport was played with sides ranging from 1–4 players but it was quickly established that games between two or four competitors worked the best. The shuttlecocks were coated with India rubber and, in outdoor play, sometimes weighted with lead. Although the depth of the net was of no consequence, it was preferred that it should reach the ground.The sport was played under the Pune rules until 1887, when the J.H.E. Hart of the Bath Badminton Club drew up revised regulations. In 1890, Hart and Bagnel Wild again revised the rules. The Badminton Association of England published these rules in 1893 and officially launched the sport at a house called "Dunbar" in Portsmouth on 13 September. The BAE started the first badminton competition, the All England Open Badminton Championships for gentlemen's doubles, ladies' doubles, and mixed doubles, in 1899. Singles competitions were added in 1900 and an England—Ireland championship match appeared in 1904.England, Scotland, Wales, Canada, Denmark, France, Ireland, the Netherlands, and New Zealand were the founding members of the International Badminton Federation in 1934, now known as the Badminton World Federation. India joined as an affiliate in 1936. The BWF now governs international badminton. Although initiated in England, competitive men's badminton has traditionally been dominated in Europe by Denmark. Worldwide, Asian nations have become dominant in international competition. China, Denmark, India, Indonesia, Malaysia, and South Korea are the nations which have consistently produced world-class players in the past few decades, with China being the greatest force in men's and women's competition recently. Rules The following information is a simplified summary of badminton rules based on the BWF Statutes publication, Laws of Badminton. Court The court is rectangular and divided into halves by a net. Courts are usually marked for both singles and doubles play, although badminton rules permit a court to be marked for singles only. The doubles court is wider than the singles court, but both are of same length. The exception, which often causes confusion to newer players, is that the doubles court has a shorter serve-length dimension.The full width of the court is 6.1 metres (20 ft), and in singles this width is reduced to 5.18 metres (17 ft). The full length of the court is 13.4 metres (44 ft). The service courts are marked by a centre line dividing the width of the court, by a short service line at a distance of 1.98 metres (6 ft 6 inch) from the net, and by the outer side and back boundaries. In doubles, the service court is also marked by a long service line, which is 0.76 metres (2 ft 6 inch) from the back boundary.The net is 1.55 metres (5 ft 1 inch) high at the edges and 1.524 metres (5 ft) high in the centre. The net posts are placed over the doubles sidelines, even when singles is played.The minimum height for the ceiling above the court is not mentioned in the Laws of Badminton. Nonetheless, a badminton court will not be suitable if the ceiling is likely to be hit on a high serve. Serving When the server serves, the shuttlecock must pass over the short service line on the opponents' court or it will count as a fault.At the start of the rally, the server and receiver stand in diagonally opposite service courts (see court dimensions). The server hits the shuttlecock so that it would land in the receiver's service court. This is similar to tennis, except that a badminton serve must be hit below waist height and with the racquet shaft pointing downwards, the shuttlecock is not allowed to bounce and in badminton, the players stand inside their service courts unlike tennis.When the serving side loses a rally, the serve immediately passes to their opponent(s) (this differs from the old system where sometimes the serve passes to the doubles partner for what is known as a "second serve").In singles, the server stands in their right service court when their score is even, and in her/his left service court when her/his score is odd.In doubles, if the serving side wins a rally, the same player continues to serve, but he/she changes service courts so that she/he serves to a different opponent each time. If the opponents win the rally and their new score is even, the player in the right service court serves; if odd, the player in the left service court serves. The players' service courts are determined by their positions at the start of the previous rally, not by where they were standing at the end of the rally. A consequence of this system is that, each time a side regains the service, the server will be the player who did not serve last time. Scoring Each game is played to 21 points, with players scoring a point whenever they win a rally regardless of whether they served (this differs from the old system where players could only win a point on their serve and each game was played to 15 points). A match is the best of three games.If the score reaches 20-all, then the game continues until one side gains a two-point lead (such as 24–22), except when there is a tie at 29-all, in which the game goes to a golden point. Whoever scores this point will win.At the start of a match, the shuttlecock is cast and the side towards which the shuttlecock is pointing serves first. Alternatively, a coin may be tossed, with the winners choosing whether to serve or receive first, or choosing which end of the court to occupy first, and their opponents making the leftover the remaining choice.In subsequent games, the winners of the previous game serve first. Matches are best out of three: a player or pair must win two games (of 21 points each) to win the match. For the first rally of any doubles game, the serving pair may decide who serves and the receiving pair may decide who receives. The players change ends at the start of the second game; if the match reaches a third game, they change ends both at the start of the game and when the leading player's or pair's score reaches 11 points.The server and receiver must remain within their service courts, without touching the boundary lines, until the server strikes the shuttlecock. The other two players may stand wherever they wish, so long as they do not block the vision of the server or receiver. Lets If a let is called, the rally is stopped and replayed with no change to the score. Lets may occur because of some unexpected disturbance such as a shuttlecock landing on court (having been hit there by players playing in adjacent court) or in small halls the shuttle may touch an overhead rail which can be classed as a let.If the receiver is not ready when the service is delivered, a let shall be called; yet, if the receiver attempts to return the shuttlecock, the receiver shall be judged to have been ready. Equipment Badminton rules restrict the design and size of racquets and shuttlecocks. Racquets Badminton racquets are lightweight, with top quality racquets weighing between 70 and 95 grams (2.5 and 3.4 ounces) not including grip or strings. They are composed of many different materials ranging from carbon fibre composite (graphite reinforced plastic) to solid steel, which may be augmented by a variety of materials. Carbon fibre has an excellent strength to weight ratio, is stiff, and gives excellent kinetic energy transfer. Before the adoption of carbon fibre composite, racquets were made of light metals such as aluminum. Earlier still, racquets were made of wood. Cheap racquets are still often made of metals such as steel, but wooden racquets are no longer manufactured for the ordinary market, because of their excessive mass and cost. Nowadays, nanomaterials such as fullerene and carbon nanotubes are added to racquets giving them greater durability.There is a wide variety of racquet designs, although the laws limit the racquet size and shape. Different racquets have playing characteristics that appeal to different players. The traditional oval head shape is still available, but an isometric head shape is increasingly common in new racquets. Strings Badminton strings are thin, high performing strings with thicknesses ranging from about 0.62 to 0.73 mm. Thicker strings are more durable, but many players prefer the feel of thinner strings. String tension is normally in the range of 80 to 160 N (18 to 36 lbf). Recreational players generally string at lower tensions than professionals, typically between 80 and 110 N (18 and 25 lbf). Professionals string between about 110 and 160 N (25 and 36 lbf). Some string manufacturers measure the thickness of their strings under tension so they are actually thicker than specified when slack. Ashaway Micropower is actually 0.7mm but Yonex BG-66 is about 0.72mm.It is often argued that high string tensions improve control, whereas low string tensions increase power. The arguments for this generally rely on crude mechanical reasoning, such as claiming that a lower tension string bed is more bouncy and therefore provides more power. This is in fact incorrect, for a higher string tension can cause the shuttle to slide off the racquet and hence make it harder to hit a shot accurately. An alternative view suggests that the optimum tension for power depends on the player: the faster and more accurately a player can swing their racquet, the higher the tension for maximum power. Neither view has been subjected to a rigorous mechanical analysis, nor is there clear evidence in favour of one or the other. The most effective way for a player to find a good string tension is to experiment. Grip The choice of grip allows a player to increase the thickness of their racquet handle and choose a comfortable surface to hold. A player may build up the handle with one or several grips before applying the final layer.Players may choose between a variety of grip materials. The most common choices are PU synthetic grips or towelling grips. Grip choice is a matter of personal preference. Players often find that sweat becomes a problem; in this case, a drying agent may be applied to the grip or hands, sweatbands may be used, the player may choose another grip material or change his/her grip more frequently.There are two main types of grip: replacement grips and overgrips. Replacement grips are thicker, and are often used to increase the size of the handle. Overgrips are thinner (less than 1 mm), and are often used as the final layer. Many players, however, prefer to use replacement grips as the final layer. Towelling grips are always replacement grips. Replacement grips have an adhesive backing, whereas overgrips have only a small patch of adhesive at the start of the tape and must be applied under tension; overgrips are more convenient for players who change grips frequently, because they may be removed more rapidly without damaging the underlying material. Shuttlecock A shuttlecock (often abbreviated to shuttle; also called a birdie) is a high-drag projectile, with an open conical shape: the cone is formed from sixteen overlapping feathers embedded into a rounded cork base. The cork is covered with thin leather or synthetic material. Synthetic shuttles are often used by recreational players to reduce their costs as feathered shuttles break easily. These nylon shuttles may be constructed with either natural cork or synthetic foam base, and a plastic skirt.Badminton rules also provide for testing a shuttlecock for the correct speed:3.1: To test a shuttlecock, hit a full underhand stroke which makes contact with the shuttlecock over the back boundary line. The shuttlecock shall be hit at an upward angle and in a direction parallel to the side lines.3.2: A shuttlecock of the correct speed will land not less than 530 mm and not more than 990 mm short of the other back boundary line. Shoes Badminton shoes are lightweight with soles of rubber or similar high-grip, non-marking materials.Compared to running shoes, badminton shoes have little lateral support. High levels of lateral support are useful for activities where lateral motion is undesirable and unexpected. Badminton, however, requires powerful lateral movements. A highly built-up lateral support will not be able to protect the foot in badminton; instead, it will encourage catastrophic collapse at the point where the shoe's support fails, and the player's ankles are not ready for the sudden loading, which can cause sprains. For this reason, players should choose badminton shoes rather than general trainers or running shoes, because proper badminton shoes will have a very thin sole, lower a person's centre of gravity, and therefore result in fewer injuries. Players should also ensure that they learn safe and proper footwork, with the knee and foot in alignment on all lunges. This is more than just a safety concern: proper footwork is also critical in order to move effectively around the court. Technique  Strokes Badminton offers a wide variety of basic strokes, and players require a high level of skill to perform all of them effectively. All strokes can be played either forehand or backhand. A player's forehand side is the same side as their playing hand: for a right-handed player, the forehand side is their right side and the backhand side is their left side. Forehand strokes are hit with the front of the hand leading (like hitting with the palm), whereas backhand strokes are hit with the back of the hand leading (like hitting with the knuckles). Players frequently play certain strokes on the forehand side with a backhand hitting action, and vice versa.In the forecourt and midcourt, most strokes can be played equally effectively on either the forehand or backhand side; but in the rear court, players will attempt to play as many strokes as possible on their forehands, often preferring to play a round-the-head forehand overhead (a forehand "on the backhand side") rather than attempt a backhand overhead. Playing a backhand overhead has two main disadvantages. First, the player must turn their back to their opponents, restricting their view of them and the court. Second, backhand overheads cannot be hit with as much power as forehands: the hitting action is limited by the shoulder joint, which permits a much greater range of movement for a forehand overhead than for a backhand. The backhand clear is considered by most players and coaches to be the most difficult basic stroke in the game, since precise technique is needed in order to muster enough power for the shuttlecock to travel the full length of the court. For the same reason, backhand smashes tend to be weak. Position of the shuttlecock and receiving player The choice of stroke depends on how near the shuttlecock is to the net, whether it is above net height, and where an opponent is currently positioned: players have much better attacking options if they can reach the shuttlecock well above net height, especially if it is also close to the net. In the forecourt, a high shuttlecock will be met with a net kill, hitting it steeply downwards and attempting to win the rally immediately. This is why it is best to drop the shuttlecock just over the net in this situation. In the midcourt, a high shuttlecock will usually be met with a powerful smash, also hitting downwards and hoping for an outright winner or a weak reply. Athletic jump smashes, where players jump upwards for a steeper smash angle, are a common and spectacular element of elite men's doubles play. In the rearcourt, players strive to hit the shuttlecock while it is still above them, rather than allowing it to drop lower. This overhead hitting allows them to play smashes, clears (hitting the shuttlecock high and to the back of the opponents' court), and drop shots (hitting the shuttlecock softly so that it falls sharply downwards into the opponents' forecourt). If the shuttlecock has dropped lower, then a smash is impossible and a full-length, high clear is difficult. Vertical position of the shuttlecock When the shuttlecock is well below net height, players have no choice but to hit upwards. Lifts, where the shuttlecock is hit upwards to the back of the opponents' court, can be played from all parts of the court. If a player does not lift, his only remaining option is to push the shuttlecock softly back to the net: in the forecourt this is called a netshot; in the midcourt or rearcourt, it is often called a push or block.When the shuttlecock is near to net height, players can hit drives, which travel flat and rapidly over the net into the opponents' rear midcourt and rearcourt. Pushes may also be hit flatter, placing the shuttlecock into the front midcourt. Drives and pushes may be played from the midcourt or forecourt, and are most often used in doubles: they are an attempt to regain the attack, rather than choosing to lift the shuttlecock and defend against smashes. After a successful drive or push, the opponents will often be forced to lift the shuttlecock. Spin Balls may be spun to alter their bounce (for example, topspin and backspin in tennis) or trajectory, and players may slice the ball (strike it with an angled racquet face) to produce such spin; but, since the shuttlecock is not allowed to bounce, this does not apply to badminton.Slicing the shuttlecock so that it spins, however, does have applications, and some are particular to badminton. (See Basic strokes for an explanation of technical terms.)Slicing the shuttlecock from the side may cause it to travel in a different direction from the direction suggested by the player's racquet or body movement. This is used to deceive opponents.Slicing the shuttlecock from the side may cause it to follow a slightly curved path (as seen from above), and the deceleration imparted by the spin causes sliced strokes to slow down more suddenly towards the end of their flight path. This can be used to create dropshots and smashes that dip more steeply after they pass the net.When playing a netshot, slicing underneath the shuttlecock may cause it to turn over itself (tumble) several times as it passes the net. This is called a spinning netshot or tumbling netshot. The opponent will be unwilling to address the shuttlecock until it has corrected its orientation.Due to the way that its feathers overlap, a shuttlecock also has a slight natural spin about its axis of rotational symmetry. The spin is in a counter-clockwise direction as seen from above when dropping a shuttlecock. This natural spin affects certain strokes: a tumbling netshot is more effective if the slicing action is from right to left, rather than from left to right. Biomechanics Badminton biomechanics have not been the subject of extensive scientific study, but some studies confirm the minor role of the wrist in power generation and indicate that the major contributions to power come from internal and external rotations of the upper and lower arm. Recent guides to the sport thus emphasize forearm rotation rather than wrist movements.The feathers impart substantial drag, causing the shuttlecock to decelerate greatly over distance. The shuttlecock is also extremely aerodynamically stable: regardless of initial orientation, it will turn to fly cork-first, and remain in the cork-first orientation.One consequence of the shuttlecock's drag is that it requires considerable power to hit it the full length of the court, which is not the case for most racquet sports. The drag also influences the flight path of a lifted (lobbed) shuttlecock: the parabola of its flight is heavily skewed so that it falls at a steeper angle than it rises. With very high serves, the shuttlecock may even fall vertically. Other factors When defending against a smash, players have three basic options: lift, block, or drive. In singles, a block to the net is the most common reply. In doubles, a lift is the safest option but it usually allows the opponents to continue smashing; blocks and drives are counter-attacking strokes, but may be intercepted by the smasher's partner. Many players use a backhand hitting action for returning smashes on both the forehand and backhand sides, because backhands are more effective than forehands at covering smashes directed to the body. Hard shots directed towards the body are difficult to defend.The service is restricted by the Laws and presents its own array of stroke choices. Unlike in tennis, the server's racquet must be pointing in a downward direction to deliver the serve so normally the shuttle must be hit upwards to pass over the net. The server can choose a low serve into the forecourt (like a push), or a lift to the back of the service court, or a flat drive serve. Lifted serves may be either high serves, where the shuttlecock is lifted so high that it falls almost vertically at the back of the court, or flick serves, where the shuttlecock is lifted to a lesser height but falls sooner. Deception Once players have mastered these basic strokes, they can hit the shuttlecock from and to any part of the court, powerfully and softly as required. Beyond the basics, however, badminton offers rich potential for advanced stroke skills that provide a competitive advantage. Because badminton players have to cover a short distance as quickly as possible, the purpose of many advanced strokes is to deceive the opponent, so that either he is tricked into believing that a different stroke is being played, or he is forced to delay his movement until he actually sees the shuttle's direction. "Deception" in badminton is often used in both of these senses. When a player is genuinely deceived, he will often lose the point immediately because he cannot change his direction quickly enough to reach the shuttlecock. Experienced players will be aware of the trick and cautious not to move too early, but the attempted deception is still useful because it forces the opponent to delay his movement slightly. Against weaker players whose intended strokes are obvious, an experienced player may move before the shuttlecock has been hit, anticipating the stroke to gain an advantage.Slicing and using a shortened hitting action are the two main technical devices that facilitate deception. Slicing involves hitting the shuttlecock with an angled racquet face, causing it to travel in a different direction than suggested by the body or arm movement. Slicing also causes the shuttlecock to travel more slowly than the arm movement suggests. For example, a good crosscourt sliced dropshot will use a hitting action that suggests a straight clear or smash, deceiving the opponent about both the power and direction of the shuttlecock. A more sophisticated slicing action involves brushing the strings around the shuttlecock during the hit, in order to make the shuttlecock spin. This can be used to improve the shuttle's trajectory, by making it dip more rapidly as it passes the net; for example, a sliced low serve can travel slightly faster than a normal low serve, yet land on the same spot. Spinning the shuttlecock is also used to create spinning netshots (also called tumbling netshots), in which the shuttlecock turns over itself several times (tumbles) before stabilizing; sometimes the shuttlecock remains inverted instead of tumbling. The main advantage of a spinning netshot is that the opponent will be unwilling to address the shuttlecock until it has stopped tumbling, since hitting the feathers will result in an unpredictable stroke. Spinning netshots are especially important for high level singles players.The lightness of modern racquets allows players to use a very short hitting action for many strokes, thereby maintaining the option to hit a powerful or a soft stroke until the last possible moment. For example, a singles player may hold his racquet ready for a netshot, but then flick the shuttlecock to the back instead with a shallow lift when she or he notices the opponent has moved before the actual shot was played. A shallow lift takes less time to reach the ground and as mentioned above a rally is over when the shuttlecock touches the ground. This makes the opponent's task of covering the whole court much more difficult than if the lift was hit higher and with a bigger, obvious swing. A short hitting action is not only useful for deception: it also allows the player to hit powerful strokes when he has no time for a big arm swing. A big arm swing is also usually not advised in badminton because bigger swings make it more difficult to recover for the next shot in fast exchanges. The use of grip tightening is crucial to these techniques, and is often described as finger power. Elite players develop finger power to the extent that they can hit some power strokes, such as net kills, with less than a 10 centimetres (4 inches) racquet swing.It is also possible to reverse this style of deception, by suggesting a powerful stroke before slowing down the hitting action to play a soft stroke. In general, this latter style of deception is more common in the rearcourt (for example, dropshots disguised as smashes), whereas the former style is more common in the forecourt and midcourt (for example, lifts disguised as netshots).Deception is not limited to slicing and short hitting actions. Players may also use double motion, where they make an initial racquet movement in one direction before withdrawing the racquet to hit in another direction. Players will often do this to send opponents in the wrong direction. The racquet movement is typically used to suggest a straight angle but then play the stroke cross court, or vice versa. Triple motion is also possible, but this is very rare in actual play. An alternative to double motion is to use a racquet head fake, where the initial motion is continued but the racquet is turned during the hit. This produces a smaller change in direction, but does not require as much time. Strategy To win in badminton, players need to employ a wide variety of strokes in the right situations. These range from powerful jumping smashes to delicate tumbling net returns. Often rallies finish with a smash, but setting up the smash requires subtler strokes. For example, a netshot can force the opponent to lift the shuttlecock, which gives an opportunity to smash. If the netshot is tight and tumbling, then the opponent's lift will not reach the back of the court, which makes the subsequent smash much harder to return.Deception is also important. Expert players prepare for many different strokes that look identical, and use slicing to deceive their opponents about the speed or direction of the stroke. If an opponent tries to anticipate the stroke, he may move in the wrong direction and may be unable to change his body momentum in time to reach the shuttlecock. Singles Since one person needs to cover the entire court, singles tactics are based on forcing the opponent to move as much as possible; this means that singles strokes are normally directed to the corners of the court. Players exploit the length of the court by combining lifts and clears with drop shots and net shots. Smashing tends to be less prominent in singles than in doubles because the smasher has no partner to follow up his effort and is thus vulnerable to a skillfully placed return. Moreover, frequent smashing can be exhausting in singles where the conservation of a player's energy is at a premium. However, players with strong smashes will sometimes use the shot to create openings, and players commonly smash weak returns to try to end rallies.In singles, players will often start the rally with a forehand high serve or with a flick serve. Low serves are also used frequently, either forehand or backhand. Drive serves are rare.At high levels of play, singles demands extraordinary fitness. Singles is a game of patient positional manoeuvring, unlike the all-out aggression of doubles. Doubles Both pairs will try to gain and maintain the attack, smashing downwards when the opportunity arises. Whenever possible, a pair will adopt an ideal attacking formation with one player hitting down from the rearcourt, and his partner in the midcourt intercepting all smash returns except the lift. If the rearcourt attacker plays a dropshot, his partner will move into the forecourt to threaten the net reply. If a pair cannot hit downwards, they will use flat strokes in an attempt to gain the attack. If a pair is forced to lift or clear the shuttlecock, then they must defend: they will adopt a side-by-side position in the rear midcourt, to cover the full width of their court against the opponents' smashes. In doubles, players generally smash to the middle ground between two players in order to take advantage of confusion and clashes.At high levels of play, the backhand serve has become popular to the extent that forehand serves have become fairly rare at a high level of play. The straight low serve is used most frequently, in an attempt to prevent the opponents gaining the attack immediately. Flick serves are used to prevent the opponent from anticipating the low serve and attacking it decisively.At high levels of play, doubles rallies are extremely fast. Men's doubles is the most aggressive form of badminton, with a high proportion of powerful jump smashes and very quick reflex exchanges. Because of this, spectator interest is sometimes greater for men's doubles than for singles. Mixed Doubles In mixed doubles, both pairs typically try to maintain an attacking formation with the woman at the front and the man at the back. This is because the male players are usually substantially stronger, and can therefore produce smashes that are more powerful. As a result, mixed doubles require greater tactical awareness and subtler positional play. Clever opponents will try to reverse the ideal position, by forcing the woman towards the back or the man towards the front. In order to protect against this danger, mixed players must be careful and systematic in their shot selection.At high levels of play, the formations will generally be more flexible: the top women players are capable of playing powerfully from the back-court, and will happily do so if required. When the opportunity arises, however, the pair will switch back to the standard mixed attacking position, with the woman in front and men in the back. Organization  Governing bodies The Badminton World Federation (BWF) is the internationally recognized governing body of the sport responsible for conduction of tournaments and approaching fair play. Five regional confederations are associated with the BWF:Asia: Badminton Asia Confederation (BAC)Africa: Badminton Confederation of Africa (BCA)Americas: Badminton Pan Am (North America and South America belong to the same confederation; BPA)Europe: Badminton Europe (BE)Oceania: Badminton Oceania (BO) Competitions The BWF organizes several international competitions, including the Thomas Cup, the premier men's international team event first held in 1948–1949, and the Uber Cup, the women's equivalent first held in 1956–1957. The competitions now take place once every two years. More than 50 national teams compete in qualifying tournaments within continental confederations for a place in the finals. The final tournament involves 12 teams, following an increase from eight teams in 2004.The Sudirman Cup, a gender-mixed international team event held once every two years, began in 1989. Teams are divided into seven levels based on the performance of each country. To win the tournament, a country must perform well across all five disciplines (men's doubles and singles, women's doubles and singles, and mixed doubles). Like association football (soccer), it features a promotion and relegation system in every level.Badminton was a demonstration event in the 1972 and 1988 Summer Olympics. It became an official Summer Olympic sport at the Barcelona Olympics in 1992 and its gold medals now generally rate as the sport's most coveted prizes for individual players.In the BWF World Championships, first held in 1977, currently only the highest ranked 64 players in the world, and a maximum of four from each country, can participate in any category. In both the Olympic and BWF World competitions restrictions on the number of participants from any one country have caused some controversy because they sometimes result in excluding elite world level players from the strongest badminton nations. The Thomas, Uber, and Sudirman Cups, the Olympics, and the BWF World (and World Junior Championships), are all categorized as level one tournaments.At the start of 2007, the BWF introduced a new tournament structure for the highest level tournaments aside from those in level one: the BWF Super Series. This level two tournament series, a tour for the world's elite players, stages twelve open tournaments around the world with 32 players (half the previous limit). The players collect points that determine whether they can play in Super Series Finals held at the year end. Among the tournaments in this series is the venerable All-England Championships, first held in 1900, which was once considered the unofficial world championships of the sport.Level three tournaments consist of Grand Prix Gold and Grand Prix event. Top players can collect the world ranking points and enable them to play in the BWF Super Series open tournaments. These include the regional competitions in Asia (Badminton Asia Championships) and Europe (European Badminton Championships), which produce the world's best players as well as the Pan America Badminton Championships.The level four tournaments, known as International Challenge, International Series, and Future Series, encourage participation by junior players. Comparison with tennis Badminton is frequently compared to tennis. The following is a list of manifest differences:Scoring: In badminton, a match is played best 2 of 3 games, with each game played up to 21 points. In tennis a match is played best of 3 or 5 sets, each set consisting of 6 games and each game ends when one player wins 4 points or wins two consecutive points at deuce points. If both team are tied at "game point", they must play until one team achieves a two-point advantage. However, at 29–all, whoever scores the golden point will win. In tennis, if the score is tied 6–6 in a set, a tiebreaker will be played, which ends once a player reaches 7 points or when one player has a two-point advantage.In tennis, the ball may bounce once before the point ends; in badminton, the rally ends once the shuttlecock touches the floor.In tennis, the serve is dominant to the extent that the server is expected to win most of his service games (at advanced level & onwards); a break of service, where the server loses the game, is of major importance in a match. In badminton a server has far less an advantage, and is unlikely to score an ace (unreturnable serve).In tennis, the server has two chances to hit a serve into the service box; in badminton, the server is allowed only one attempt.A tennis court is approximately twice the length and width of a badminton court.Tennis racquets are about four times as heavy as badminton racquets, 10 to 12 ounces (280 to 340 grams) versus 2 to 3 ounces (57 to 85 grams). Tennis balls are more than eleven times heavier than shuttlecocks, 57 grams (2.0 ounces) versus 5 grams (0.18 ounces).The fastest recorded tennis stroke is Samuel Groth's 163.4 miles per hour (263 kilometres per hour) serve, whereas the fastest badminton stroke during gameplay was Lee Chong Wei's 253 miles per hour (407 kilometres per hour) recorded smash at the 2015 Hong Kong Open.Statistics such as the smash speed, above, prompt badminton enthusiasts to make other comparisons that are more contentious. For example, it is often claimed that badminton is the fastest racquet sport. Although badminton holds the record for the fastest initial speed of a racquet sports projectile, the shuttlecock decelerates substantially faster than other projectiles such as tennis balls. In turn, this qualification must be qualified by consideration of the distance over which the shuttlecock travels: a smashed shuttlecock travels a shorter distance than a tennis ball during a serve.While fans of badminton and tennis often claim that their sport is the more physically demanding, such comparisons are difficult to make objectively because of the differing demands of the games. No formal study currently exists evaluating the physical condition of the players or demands during game play.Badminton and tennis techniques differ substantially. The lightness of the shuttlecock and of badminton racquets allow badminton players to make use of the wrist and fingers much more than tennis players; in tennis the wrist is normally held stable, and playing with a mobile wrist may lead to injury. For the same reasons, badminton players can generate power from a short racquet swing: for some strokes such as net kills, an elite player's swing may be less than 5 centimetres (2 inches). For strokes that require more power, a longer swing will typically be used, but the badminton racquet swing will rarely be as long as a typical tennis swing. See also Ball badmintonHanetsukiList of racquet sportsSpeed badminton Notes  References Adams, Bernard (1980), The Badminton Story, BBC Books, ISBN 0563164654 Boga, Steve (2008), Badminton, Paw Prints, ISBN 1439504784  Chisholm, Hugh, ed. (1911), "Badminton (game)", Encyclopædia Britannica, 3 (11th ed.), Cambridge University Press, p. 189 Connors, M.; Dupuis, D.L.; Morgan, B. (1991), The Olympics Factbook: A Spectator's Guide to the Winter and Summer Games, Visible Ink Press, ISBN 0-8103-9417-0 .Downey, Jake; Downey, Jason Charles (1982), Better Badminton for All, Pelham Books, ISBN 978-0-7207-1438-8 .Grice, Tony (2008), Badminton: Steps to Success, Human Kinetics, ISBN 978-0-7360-7229-8 Guillain, Jean-Yves (2004), Badminton: An Illustrated History, Publibook, ISBN 2-7483-0572-8  Jones, Henry (1878), "Badminton", in Baynes, T.S., Encyclopædia Britannica, 3 (9th ed.), New York: Charles Scribner's Sons, p. 228 Kim, Wangdo (2002), An Analysis of the Biomechanics of Arm Movement During a Badminton Smash (PDF), Nanyang Technological University, archived from the original (PDF) on 2 October 2008 http://wayback.archive.org/web/20151118173744/http://www3.ntu.edu.sg/home5/PG02259480/badminton_smash.pdf. External links Badminton at DMOZBadminton World FederationLaws of BadmintonSimplified RulesBadminton Asia ConfederationBadminton Pan AmBadminton OceaniaBadminton EuropeBadminton Confederation of Africa
Finance is a field that deals with the study of investments. It includes the dynamics of assets and liabilities over time under conditions of different degrees of uncertainty and risk. Finance can also be defined as the science of money management. Finance aims to price assets based on their risk level and their expected rate of return. Finance can be broken into three different sub-categories: public finance, corporate finance and personal finance. Areas of finance  Personal finance Questions in personal finance revolve around:Protection against unforeseen personal events, as well as events in the wider economiesTransference of family wealth across generations (bequests and inheritance)Effects of tax policies (tax subsidies or penalties) on management of personal financesEffects of credit on individual financial standingDevelopment of a savings plan or financing for large purchases (auto, education, home)Planning a secure financial future in an environment of economic instabilityPersonal finance may involve paying for education, financing durable goods such as real estate and cars, buying insurance, e.g. health and property insurance, investing and saving for retirement.Personal finance may also involve paying for a loan, or debt obligations. The six key areas of personal financial planning, as suggested by the Financial Planning Standards Board, are:Financial position: is concerned with understanding the personal resources available by examining net worth and household cash flow. Net worth is a person's balance sheet, calculated by adding up all assets under that person's control, minus all liabilities of the household, at one point in time. Household cash flow totals up all the expected sources of income within a year, minus all expected expenses within the same year. From this analysis, the financial planner can determine to what degree and in what time the personal goals can be accomplished.Adequate protection: the analysis of how to protect a household from unforeseen risks. These risks can be divided into the following: liability, property, death, disability, health and long term care. Some of these risks may be self-insurable, while most will require the purchase of an insurance contract. Determining how much insurance to get, at the most cost effective terms requires knowledge of the market for personal insurance. Business owners, professionals, athletes and entertainers require specialized insurance professionals to adequately protect themselves. Since insurance also enjoys some tax benefits, utilizing insurance investment products may be a critical piece of the overall investment planning.Tax planning: typically the income tax is the single largest expense in a household. Managing taxes is not a question of if you will pay taxes, but when and how much. Government gives many incentives in the form of tax deductions and credits, which can be used to reduce the lifetime tax burden. Most modern governments use a progressive tax. Typically, as one's income grows, a higher marginal rate of tax must be paid. Understanding how to take advantage of the myriad tax breaks when planning one's personal finances can make a significant impact in which it can later save you money in the long term.Investment and accumulation goals: planning how to accumulate enough money - for large purchases and life events - is what most people consider to be financial planning. Major reasons to accumulate assets include, purchasing a house or car, starting a business, paying for education expenses, and saving for retirement. Achieving these goals requires projecting what they will cost, and when you need to withdraw funds that will be necessary to be able to achieve these goals. A major risk to the household in achieving their accumulation goal is the rate of price increases over time, or inflation. Using net present value calculators, the financial planner will suggest a combination of asset earmarking and regular savings to be invested in a variety of investments. In order to overcome the rate of inflation, the investment portfolio has to get a higher rate of return, which typically will subject the portfolio to a number of risks. Managing these portfolio risks is most often accomplished using asset allocation, which seeks to diversify investment risk and opportunity. This asset allocation will prescribe a percentage allocation to be invested in stocks (either preferred stock or common stock), bonds (for example mutual bonds or government bonds, or corporate bonds), cash and alternative investments. The allocation should also take into consideration the personal risk profile of every investor, since risk attitudes vary from person to person.Retirement planning is the process of understanding how much it costs to live at retirement, and coming up with a plan to distribute assets to meet any income shortfall. Methods for retirement plan include taking advantage of government allowed structures to manage tax liability including: individual (IRA) structures, or employer sponsored retirement plans.Estate planning involves planning for the disposition of one's assets after death. Typically, there is a tax due to the state or federal government at one's death. Avoiding these taxes means that more of one's assets will be distributed to one's heirs. One can leave one's assets to family, friends or charitable groups. Corporate finance Corporate finance deals with the sources of funding and the capital structure of corporations, the actions that managers take to increase the value of the firm to the shareholders, and the tools and analysis used to allocate financial resources. Although it is in principle different from managerial finance which studies the financial management of all firms, rather than corporations alone, the main concepts in the study of corporate finance are applicable to the financial problems of all kinds of firms. Corporate finance generally involves balancing risk and profitability, while attempting to maximize an entity's assets, net incoming cash flow and the value of its stock, and generically entails three primary areas of capital resource allocation. In the first, "capital budgeting", management must choose which "projects" (if any) to undertake. The discipline of capital budgeting may employ standard business valuation techniques or even extend to real options valuation; see Financial modeling. The second, "sources of capital" relates to how these investments are to be funded: investment capital can be provided through different sources, such as by shareholders, in the form of equity (privately or via an initial public offering), creditors, often in the form of bonds, and the firm's operations (cash flow). Short-term funding or working capital is mostly provided by banks extending a line of credit. The balance between these elements forms the company's capital structure. The third, "the dividend policy", requires management to determine whether any unappropriated profit (excess cash) is to be retained for future investment / operational requirements, or instead to be distributed to shareholders, and if so, in what form. Short term financial management is often termed "working capital management", and relates to cash-, inventory- and debtors management.Corporate finance also includes within its scope business valuation, stock investing, or investment management. An investment is an acquisition of an asset in the hope that it will maintain or increase its value over time that will in hope give back a higher rate of return when it comes to disbursing dividends. In investment management –  in choosing a portfolio –  one has to use financial analysis to determine what, how much and when to invest. To do this, a company must:Identify relevant objectives and constraints: institution or individual goals, time horizon, risk aversion and tax considerations;Identify the appropriate strategy: active versus passive hedging strategyMeasure the portfolio performanceFinancial management overlaps with the financial function of the accounting profession. However, financial accounting is the reporting of historical financial information, while financial management is concerned with the allocation of capital resources to increase a firm's value to the shareholders and increase their rate of return on the investments.Financial risk management, an element of corporate finance, is the practice of creating and protecting economic value in a firm by using financial instruments to manage exposure to risk, particularly credit risk and market risk. (Other risk types include foreign exchange, shape, volatility, sector, liquidity, inflation risks, etc.) It focuses on when and how to hedge using financial instruments; in this sense it overlaps with financial engineering. Similar to general risk management, financial risk management requires identifying its sources, measuring it (see: Risk measure: Well known risk measures), and formulating plans to address these, and can be qualitative and quantitative. In the banking sector worldwide, the Basel Accords are generally adopted by internationally active banks for tracking, reporting and exposing operational, credit and market risks. Financial services An entity whose income exceeds its expenditure can lend or invest the excess income to help that excess income produce more income in the future. Though on the other hand, an entity whose income is less than its expenditure can raise capital by borrowing or selling equity claims, decreasing its expenses, or increasing its income. The lender can find a borrower—a financial intermediary such as a bank—or buy notes or bonds (corporate bonds, government bonds, or mutual bonds) in the bond market. The lender receives interest, the borrower pays a higher interest than the lender receives, and the financial intermediary earns the difference for arranging the loan.A bank aggregates the activities of many borrowers and lenders. A bank accepts deposits from lenders, on which it pays interest. The bank then lends these deposits to borrowers. Banks allow borrowers and lenders, of different sizes, to coordinate their activity.Finance is used by individuals (personal finance), by governments (public finance), by businesses (corporate finance) and by a wide variety of other organizations such as schools and non-profit organizations. In general, the goals of each of the above activities are achieved through the use of appropriate financial instruments and methodologies, with consideration to their institutional setting.Finance is one of the most important aspects of business management and includes analysis related to the use and acquisition of funds for the enterprise.In corporate finance, a company's capital structure is the total mix of financing methods it uses to raise funds. One method is debt financing, which includes bank loans and bond sales. Another method is equity financing - the sale of stock by a company to investors, the original shareholders (they own a portion of the business) of a share. Ownership of a share gives the shareholder certain contractual rights and powers, which typically include the right to receive declared dividends and to vote the proxy on important matters (e.g., board elections). The owners of both bonds (either government bonds or corporate bonds) and stock (whether its preferred stock or common stock), may be institutional investors - financial institutions such as investment banks and pension funds  or private individuals, called private investors or retail investors. Public finance Public finance describes finance as related to sovereign states and sub-national entities (states/provinces, counties, municipalities, etc.) and related public entities (e.g. school districts) or agencies. It usually encompasses a long-term, strategic perspective regarding investment decisions that affect public entities. These long-term, strategic periods usually encompass five or more years. Public finance is primarily concerned with:Identification of required expenditure of a public sector entitySource(s) of that entity's revenueThe budgeting processDebt issuance (municipal bonds) for public works projectsCentral banks, such as the Federal Reserve System banks in the United States and Bank of England in the United Kingdom, are strong players in public finance, acting as lenders of last resort as well as strong influences on monetary and credit conditions in the economy. Capital Capital, in the financial sense, is the money that gives the business the power to buy goods to be used in the production of other goods or the offering of a service. (The capital has two types of resources, Equity and Debt).The deployment of capital is decided by the budget. This may include the objective of business, targets set, and results in financial terms, e.g., the target set for sale, resulting cost, growth, required investment to achieve the planned sales, and financing source for the investment.A budget may be long term or short term. Long term budgets have a time horizon of 5–10 years giving a vision to the company; short term is an annual budget which is drawn to control and operate in that particular year.Budgets will include proposed fixed asset requirements and how these expenditures will be financed. Capital budgets are often adjusted annually (done every year) and should be part of a longer-term Capital Improvements Plan.A cash budget is also required. The working capital requirements of a business are monitored at all times to ensure that there are sufficient funds available to meet short-term expenses.The cash budget is basically a detailed plan that shows all expected sources and uses of cash when it comes to spending it appropriately. The cash budget has the following six main sections:Beginning Cash Balance - contains the last period's closing cash balance, in other words, the remaining cash from last years earnings.Cash collections - includes all expected cash receipts (all sources of cash for the period considered, mainly sales)Cash disbursements - lists all planned cash outflows for the period such as dividend, excluding interest payments on short-term loans, which appear in the financing section. All expenses that do not affect cash flow are excluded from this list (e.g. depreciation, amortization, etc.)Cash excess or deficiency - a function of the cash needs and cash available. Cash needs are determined by the total cash disbursements plus the minimum cash balance required by company policy. If total cash available is less than cash needs, a deficiency exists.Financing - discloses the planned borrowings and repayments of those planned borrowings, including interest. Financial theory  Financial economics Financial economics is the branch of economics studying the interrelation of financial variables, such as prices, interest rates and shares, as opposed to goods and services. Financial economics concentrates on influences of real economic variables on financial ones, in contrast to pure finance. It centres on managing risk in the context of the financial markets, and the resultant economic and financial models. It essentially explores how rational investors would apply risk and return to the problem of an investment policy. Here, the twin assumptions of rationality and market efficiency lead to modern portfolio theory (the CAPM), and to the Black–Scholes theory for option valuation; it further studies phenomena and models where these assumptions do not hold, or are extended. "Financial economics", at least formally, also considers investment under "certainty" (Fisher separation theorem, "theory of investment value", Modigliani–Miller theorem) and hence also contributes to corporate finance theory. Financial econometrics is the branch of financial economics that uses econometric techniques to parameterize the relationships suggested.Although closely related, the disciplines of economics and finance are distinctive. The “economy” is a social institution that organizes a society’s production, distribution, and consumption of goods and services, all of which must be financed.Economists make a number of abstract assumptions for purposes of their analyses and predictions. They generally regard financial markets that function for the financial system as an efficient mechanism (Efficient-market hypothesis). Instead, financial markets are subject to human error and emotion. New research discloses the mischaracterization of investment safety and measures of financial products and markets so complex that their effects, especially under conditions of uncertainty, are impossible to predict. The study of finance is subsumed under economics as financial economics, but the scope, speed, power relations and practices of the financial system can uplift or cripple whole economies and the well-being of households, businesses and governing bodies within them—sometimes in a single day. Financial mathematics Financial mathematics is a field of applied mathematics, concerned with financial markets. The subject has a close relationship with the discipline of financial economics, which is concerned with much of the underlying theory that is involved in financial mathematics. Generally, mathematical finance will derive, and extend, the mathematical or numerical models suggested by financial economics. In terms of practice, mathematical finance also overlaps heavily with the field of computational finance (also known as financial engineering). Arguably, these are largely synonymous, although the latter focuses on application, while the former focuses on modelling and derivation (see: Quantitative analyst). The field is largely focused on the modelling of derivatives, although other important subfields include insurance mathematics and quantitative portfolio problems. See Outline of finance: Mathematical tools; Outline of finance: Derivatives pricing. Experimental finance Experimental finance aims to establish different market settings and environments to observe experimentally and provide a lens through which science can analyze agents' behavior and the resulting characteristics of trading flows, information diffusion and aggregation, price setting mechanisms, and returns processes. Researchers in experimental finance can study to what extent existing financial economics theory makes valid predictions and therefore prove them, and attempt to discover new principles on which such theory can be extended and be applied to future financial decisions. Research may proceed by conducting trading simulations or by establishing and studying the behavior, and the way that these people act or react, of people in artificial competitive market-like settings. Behavioral finance Behavioral finance studies how the psychology of investors or managers affects financial decisions and markets when making a decision that can impact either negatively or positively on one of their areas. Behavioral finance has grown over the last few decades to become central and very important to finance.Behavioral finance includes such topics as:Empirical studies that demonstrate significant deviations from classical theories.Models of how psychology affects and impacts trading and pricesForecasting based on these methods.Studies of experimental asset markets and use of models to forecast experiments.A strand of behavioral finance has been dubbed Quantitative Behavioral Finance, which uses mathematical and statistical methodology to understand behavioral biases in conjunction with valuation. Some of this endeavor has been led by Gunduz Caginalp (Professor of Mathematics and Editor of Journal of Behavioral Finance during 2001-2004) and collaborators including Vernon Smith (2002 Nobel Laureate in Economics), David Porter, Don Balenovich, Vladimira Ilieva, Ahmet Duran). Studies by Jeff Madura, Ray Sturm and others have demonstrated significant behavioral effects in stocks and exchange traded funds. Among other topics, quantitative behavioral finance studies behavioral effects together with the non-classical assumption of the finiteness of assets. Professional qualifications There are several related professional qualifications, that can lead to the field:Generalist Finance qualifications:Degrees: Master of Science in Finance (MSF), Master of Finance (M.Fin), Master of Financial Economics, Master of Applied Finance, Master of Liberal Arts in Finance (ALM.Fin)Certifications: Chartered Financial Analyst (CFA), Certified Treasury Professional (CTP), Certified Valuation Analyst (CVA), Certified Patent Valuation Analyst (CPVA), Chartered Business Valuator (CBV), Certified International Investment Analyst (CIIA), Financial Risk Manager (FRM), Professional Risk Manager (PRM), Association of Corporate Treasurers (ACT), Certified Market Analyst (CMA/FAD) Dual Designation, Corporate Finance Qualification (CF), Chartered Alternative Investment Analyst (CAIA), Chartered Investment Manager (CIM)Quantitative Finance qualifications: Master of Financial Engineering (MSFE), Master of Quantitative Finance (MQF), Master of Computational Finance (MCF), Master of Financial Mathematics (MFM), Certificate in Quantitative Finance (CQF).Accountancy qualifications:Qualified accountant: Chartered Certified Accountant (ACCA, UK certification), Chartered Accountant (ACA - England & Wales certification / CA - certification in Scotland and Commonwealth countries), Certified Public Accountant (CPA, US certification), ACMA/FCMA (Associate/Fellow Chartered Management Accountant) from Chartered Institute of Management Accountant (CIMA), UK.Non-statutory qualifications: Chartered Cost Accountant CCA Designation from AAFMBusiness qualifications: Master of Business Administration (MBA), Master of Management (MM), Master of Commerce (M.Comm), Master of Science in Management (MSM), Doctor of Business Administration (DBA) Unsolved problems in finance As the debate to whether finance is an art or a science is still open, there have been recent efforts to organize a list of unsolved problems in finance. See also Outline of financeFinancial crisis of 2007–2010List of unsolved problems in finance References  External links Learn Finance Step by step with infographics toolsOECD work on financial markets Observation of UK Finance MarketWharton Finance Knowledge Project - aimed to offer free access to finance knowledge for students, teachers, and self-learners.Professor Aswath Damodaran (New York University Stern School of Business) - provides resources covering three areas in finance: corporate finance, valuation and investment management and syndicate finance.
An engine or motor is a machine designed to convert one form of energy into mechanical energy. Heat engines burn a fuel to create heat, which is then used to create a force. Electric motors convert electrical energy into mechanical motion; pneumatic motors use compressed air and clockwork motors in wind-up toys use elastic energy. In biological systems, molecular motors, like myosins in muscles, use chemical energy to create forces and eventually motion. Terminology The word engine derives from Old French engin, from the Latin ingenium–the root of the word ingenious. Pre-industrial weapons of war, such as catapults, trebuchets and battering rams, were called siege engines, and knowledge of how to construct them was often treated as a military secret. The word gin, as in cotton gin, is short for engine. Most mechanical devices invented during the industrial revolution were described as engines—the steam engine being a notable example. However, the original steam engines, such as those by Thomas Savery, were not mechanical engines but pumps. In this manner, a fire engine in its original form was merely a water pump, with the engine being transported to the fire by horses.In modern usage, the term engine typically describes devices, like steam engines and internal combustion engines, that burn or otherwise consume fuel to perform mechanical work by exerting a torque or linear force (usually in the form of thrust). Devices converting heat energy into motion are commonly referred to simply as engines. Examples of engines which exert a torque include the familiar automobile gasoline and diesel engines, as well as turboshafts. Examples of engines which produce thrust include turbofans and rockets.When the internal combustion engine was invented, the term motor was initially used to distinguish it from the steam engine—which was in wide use at the time, powering locomotives and other vehicles such as steam rollers. The term motor derives from the Latin verb moto which means to set in motion, or maintain motion. Thus a motor is a device that imparts motion.Motor and engine later came to be used largely interchangeably in casual discourse. However, technically, the two words have different meanings. An engine is a device that burns or otherwise consumes fuel, changing its chemical composition, whereas a motor is a device driven by electricity, air, or hydraulic pressure, which does not change the chemical composition of its energy source. However, rocketry uses the term rocket motor, even though they consume fuel.A heat engine may also serve as a prime mover—a component that transforms the flow or changes in pressure of a fluid into mechanical energy. An automobile powered by an internal combustion engine may make use of various motors and pumps, but ultimately all such devices derive their power from the engine. Another way of looking at it is that a motor receives power from an external source, and then converts it into mechanical energy, while an engine creates power from pressure (derived directly from the explosive force of combustion or other chemical reaction, or secondarily from the action of some such force on other substances such as air, water, or steam). History  Antiquity Simple machines, such as the club and oar (examples of the lever), are prehistoric. More complex engines using human power, animal power, water power, wind power and even steam power date back to antiquity. Human power was focused by the use of simple engines, such as the capstan, windlass or treadmill, and with ropes, pulleys, and block and tackle arrangements; this power was transmitted usually with the forces multiplied and the speed reduced. These were used in cranes and aboard ships in Ancient Greece, as well as in mines, water pumps and siege engines in Ancient Rome. The writers of those times, including Vitruvius, Frontinus and Pliny the Elder, treat these engines as commonplace, so their invention may be more ancient. By the 1st century AD, cattle and horses were used in mills, driving machines similar to those powered by humans in earlier times.According to Strabo, a water powered mill was built in Kaberia of the kingdom of Mithridates during the 1st century BC. Use of water wheels in mills spread throughout the Roman Empire over the next few centuries. Some were quite complex, with aqueducts, dams, and sluices to maintain and channel the water, along with systems of gears, or toothed-wheels made of wood and metal to regulate the speed of rotation. More sophisticated small devices, such as the Antikythera Mechanism used complex trains of gears and dials to act as calendars or predict astronomical events. In a poem by Ausonius in the 4th century AD, he mentions a stone-cutting saw powered by water. Hero of Alexandria is credited with many such wind and steam powered machines in the 1st century AD, including the Aeolipile and the vending machine, often these machines were associated with worship, such as animated altars and automated temple doors. Medieval Medieval Muslim engineers employed gears in mills and water-raising machines, and used dams as a source of water power to provide additional power to watermills and water-raising machines. In the medieval Islamic world, such advances made it possible to mechanize many industrial tasks previously carried out by manual labour.In 1206, al-Jazari employed a crank-conrod system for two of his water-raising machines. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629.In the 13th century, the solid rocket motor was invented in China. Driven by gunpowder, this, the simplest form of internal combustion engine was unable to deliver sustained power, but was useful for propelling weaponry at high speeds towards enemies in battle and for fireworks. After invention, this innovation spread throughout Europe. Industrial Revolution The Watt steam engine was the first type of steam engine to make use of steam at a pressure just above atmospheric to drive the piston helped by a partial vacuum. Improving on the design of the 1712 Newcomen steam engine, the Watt steam engine, developed sporadically from 1763 to 1775, was a great step in the development of the steam engine. Offering a dramatic increase in fuel efficiency, James Watt's design became synonymous with steam engines, due in no small part to his business partner, Matthew Boulton. It enabled rapid development of efficient semi-automated factories on a previously unimaginable scale in places where waterpower was not available. Later development led to steam locomotives and great expansion of railway transportation.As for internal combustion piston engines, these were tested in France in 1807 by de Rivaz and independently, by the Niépce brothers. They were theoretically advanced by Carnot in 1824. In 1853-57 Eugenio Barsanti and Felice Matteucci invented and patented an engine using the free-piston principle that was possibly the first 4-cycle engine.The invention of an internal combustion engine which was later commercially successful was made during 1860 by Etienne Lenoir. The Otto cycle in 1877 was capable of giving a far higher power to weight ratio than steam engines and worked much better for many transportation applications such as cars and aircraft. Automobiles The first commercially successful automobile, created by Karl Benz, added to the interest in light and powerful engines. The lightweight petrol internal combustion engine, operating on a four-stroke Otto cycle, has been the most successful for light automobiles, while the more efficient Diesel engine is used for trucks and buses. However, in recent years, turbo Diesel engines have become increasingly popular, especially outside of the United States, even for quite small cars. Horizontally opposed pistons In 1896, Karl Benz was granted a patent for his design of the first engine with horizontally opposed pistons. His design created an engine in which the corresponding pistons move in horizontal cylinders and reach top dead center simultaneously, thus automatically balancing each other with respect to their individual momentum. Engines of this design are often referred to as flat engines because of their shape and lower profile. They were used in the Volkswagen Beetle, some Porsche and Subaru cars, many BMW and Honda motorcycles, and aircraft engines (for propeller driven aircraft). Advancement Continuance of the use of the internal combustion engine for automobiles is partly due to the improvement of engine control systems (onboard computers providing engine management processes, and electronically controlled fuel injection). Forced air induction by turbocharging and supercharging have increased power outputs and engine efficiencies. Similar changes have been applied to smaller diesel engines giving them almost the same power characteristics as petrol engines. This is especially evident with the popularity of smaller diesel engine propelled cars in Europe. Larger diesel engines are still often used in trucks and heavy machinery, although they require special machining not available in most factories. Diesel engines produce lower hydrocarbon and CO2 emissions, but greater particulate and NOx pollution, than gasoline engines. Diesel engines are also 40% more fuel efficient than comparable gasoline engines. Increasing power The first half of the 20th century saw a trend to increasing engine power, particularly in the American models. Design changes incorporated all known methods of raising engine capacity, including increasing the pressure in the cylinders to improve efficiency, increasing the size of the engine, and increasing the rate at which the engine produces work. The higher forces and pressures created by these changes created engine vibration and size problems that led to stiffer, more compact engines with V and opposed cylinder layouts replacing longer straight-line arrangements. Combustion efficiency The design principles favoured in Europe, because of economic and other restraints such as smaller and twistier roads, leant toward smaller cars and corresponding to the design principles that concentrated on increasing the combustion efficiency of smaller engines. This produced more economical engines with earlier four-cylinder designs rated at 40 horsepower (30 kW) and six-cylinder designs rated as low as 80 horsepower (60 kW), compared with the large volume V-8 American engines with power ratings in the range from 250 to 350 hp, some even over 400 hp (190 to 260 kW). Engine configuration Earlier automobile engine development produced a much larger range of engines than is in common use today. Engines have ranged from 1- to 16-cylinder designs with corresponding differences in overall size, weight, engine displacement, and cylinder bores. Four cylinders and power ratings from 19 to 120 hp (14 to 90 kW) were followed in a majority of the models. Several three-cylinder, two-stroke-cycle models were built while most engines had straight or in-line cylinders. There were several V-type models and horizontally opposed two- and four-cylinder makes too. Overhead camshafts were frequently employed. The smaller engines were commonly air-cooled and located at the rear of the vehicle; compression ratios were relatively low. The 1970s and 1980s saw an increased interest in improved fuel economy, which caused a return to smaller V-6 and four-cylinder layouts, with as many as five valves per cylinder to improve efficiency. The Bugatti Veyron 16.4 operates with a W16 engine, meaning that two V8 cylinder layouts are positioned next to each other to create the W shape sharing the same crankshaft.The largest internal combustion engine ever built is the Wärtsilä-Sulzer RTA96-C, a 14-cylinder, 2-stroke turbocharged diesel engine that was designed to power the Emma Mærsk, the largest container ship in the world. This engine weighs 2,300 tons, and when running at 102 RPM produces 109,000 bhp (80,080 kW) consuming some 13.7 tons of fuel each hour. Types An engine can be put into a category according to two criteria: the form of energy it accepts in order to create motion, and the type of motion it outputs. Heat engine  Combustion engine Combustion engines are heat engines driven by the heat of a combustion process. Internal combustion engine The internal combustion engine is an engine in which the combustion of a fuel (generally, fossil fuel) occurs with an oxidizer (usually air) in a combustion chamber. In an internal combustion engine the expansion of the high temperature and high pressure gases, which are produced by the combustion, directly applies force to components of the engine, such as the pistons or turbine blades or a nozzle, and by moving it over a distance, generates useful mechanical energy. External combustion engine An external combustion engine (EC engine) is a heat engine where an internal working fluid is heated by combustion of an external source, through the engine wall or a heat exchanger. The fluid then, by expanding and acting on the mechanism of the engine produces motion and usable work. The fluid is then cooled, compressed and reused (closed cycle), or (less commonly) dumped, and cool fluid pulled in (open cycle air engine)."Combustion" refers to burning fuel with an oxidizer, to supply the heat. Engines of similar (or even identical) configuration and operation may use a supply of heat from other sources such as nuclear, solar, geothermal or exothermic reactions not involving combustion; but are not then strictly classed as external combustion engines, but as external thermal engines.The working fluid can be a gas as in a Stirling engine, or steam as in a steam engine or an organic liquid such as n-pentane in an Organic Rankine cycle. The fluid can be of any composition; gas is by far the most common, although even single-phase liquid is sometimes used. In the case of the steam engine, the fluid changes phases between liquid and gas. Air-breathing combustion engines Air-breathing combustion engines are combustion engines that use the oxygen in atmospheric air to oxidise ('burn') the fuel, rather than carrying an oxidiser, as in a rocket. Theoretically, this should result in a better specific impulse than for rocket engines.A continuous stream of air flows through the air-breathing engine. This air is compressed, mixed with fuel, ignited and expelled as the exhaust gas.ExamplesTypical air-breathing engines include:Reciprocating engineSteam engineGas turbineairbreathing jet engineTurbo-propeller enginePulse detonation enginePulse jetRamjetScramjetLiquid air cycle engine/Reaction Engines SABRE. Environmental effects The operation of engines typically has a negative impact upon air quality and ambient sound levels. There has been a growing emphasis on the pollution producing features of automotive power systems. This has created new interest in alternate power sources and internal-combustion engine refinements. Though a few limited-production battery-powered electric vehicles have appeared, they have not proved competitive owing to costs and operating characteristics. In the 21st century the diesel engine has been increasing in popularity with automobile owners. However, the gasoline engine and the Diesel engine, with their new emission-control devices to improve emission performance, have not yet been significantly challenged. A number of manufacturers have introduced hybrid engines, mainly involving a small gasoline engine coupled with an electric motor and with a large battery bank, but these too have yet to make much of an inroad into the market shares of gasoline and Diesel engines. Air quality Exhaust from a spark ignition engine consists of the following: nitrogen 70 to 75% (by volume), water vapor 10 to 12%, carbon dioxide 10 to 13.5%, hydrogen 0.5 to 2%, oxygen 0.2 to 2%, carbon monoxide: 0.1 to 6%, unburnt hydrocarbons and partial oxidation products (e.g. aldehydes) 0.5 to 1%, nitrogen monoxide 0.01 to 0.4%, nitrous oxide <100 ppm, sulfur dioxide 15 to 60 ppm, traces of other compounds such as fuel additives and lubricants, also halogen and metallic compounds, and other particles. Carbon monoxide is highly toxic, and can cause carbon monoxide poisoning, so it is important to avoid any build-up of the gas in a confined space. Catalytic converters can reduce toxic emissions, but not completely eliminate them. Also, resulting greenhouse gas emissions, chiefly carbon dioxide, from the widespread use of engines in the modern industrialized world is contributing to the global greenhouse effect – a primary concern regarding global warming. Non-combusting heat engines Some engines convert heat from noncombustive processes into mechanical work, for example a nuclear power plant uses the heat from the nuclear reaction to produce steam and drive a steam engine, or a gas turbine in a rocket engine may be driven by decomposing hydrogen peroxide. Apart from the different energy source, the engine is often engineered much the same as an internal or external combustion engine. Another group of noncombustive engines includes thermoacoustic heat engines (sometimes called "TA engines") which are thermoacoustic devices which use high-amplitude sound waves to pump heat from one place to another, or conversely use a heat difference to induce high-amplitude sound waves. In general, thermoacoustic engines can be divided into standing wave and travelling wave devices. Non-thermal chemically powered motor Non-thermal motors usually are powered by a chemical reaction, but are not heat engines. Examples include:Molecular motor - motors found in living thingsSynthetic molecular motor. Electric motor An electric motor uses electrical energy to produce mechanical energy, usually through the interaction of magnetic fields and current-carrying conductors. The reverse process, producing electrical energy from mechanical energy, is accomplished by a generator or dynamo. Traction motors used on vehicles often perform both tasks. Electric motors can be run as generators and vice versa, although this is not always practical. Electric motors are ubiquitous, being found in applications as diverse as industrial fans, blowers and pumps, machine tools, household appliances, power tools, and disk drives. They may be powered by direct current (for example a battery powered portable device or motor vehicle), or by alternating current from a central electrical distribution grid. The smallest motors may be found in electric wristwatches. Medium-size motors of highly standardized dimensions and characteristics provide convenient mechanical power for industrial uses. The very largest electric motors are used for propulsion of large ships, and for such purposes as pipeline compressors, with ratings in the thousands of kilowatts. Electric motors may be classified by the source of electric power, by their internal construction, and by their application.The physical principle of production of mechanical force by the interactions of an electric current and a magnetic field was known as early as 1821. Electric motors of increasing efficiency were constructed throughout the 19th century, but commercial exploitation of electric motors on a large scale required efficient electrical generators and electrical distribution networks.To reduce the electric energy consumption from motors and their associated carbon footprints, various regulatory authorities in many countries have introduced and implemented legislation to encourage the manufacture and use of higher efficiency electric motors. A well-designed motor can convert over 90% of its input energy into useful power for decades. When the efficiency of a motor is raised by even a few percentage points, the savings, in kilowatt hours (and therefore in cost), are enormous. The electrical energy efficiency of a typical industrial induction motor can be improved by: 1) reducing the electrical losses in the stator windings (e.g., by increasing the cross-sectional area of the conductor, improving the winding technique, and using materials with higher electrical conductivities, such as copper), 2) reducing the electrical losses in the rotor coil or casting (e.g., by using materials with higher electrical conductivities, such as copper), 3) reducing magnetic losses by using better quality magnetic steel, 4) improving the aerodynamics of motors to reduce mechanical windage losses, 5) improving bearings to reduce friction losses, and 6) minimizing manufacturing tolerances. For further discussion on this subject, see Premium efficiency.)By convention, electric engine refers to a railroad electric locomotive, rather than an electric motor. Physically powered motor Some motors are powered by potential or kinetic energy, for example some funiculars, gravity plane and ropeway conveyors have used the energy from moving water or rocks, and some clocks have a weight that falls under gravity. Other forms of potential energy include compressed gases (such as pneumatic motors), springs (clockwork motors) and elastic bands.Historic military siege engines included large catapults, trebuchets, and (to some extent) battering rams were powered by potential energy. Pneumatic motor A pneumatic motor is a machine that converts potential energy in the form of compressed air into mechanical work. Pneumatic motors generally convert the compressed air to mechanical work though either linear or rotary motion. Linear motion can come from either a diaphragm or piston actuator, while rotary motion is supplied by either a vane type air motor or piston air motor. Pneumatic motors have found widespread success in the hand-held tool industry and continual attempts are being made to expand their use to the transportation industry. However, pneumatic motors must overcome efficiency deficiencies before being seen as a viable option in the transportation industry. Hydraulic motor A hydraulic motor is one that derives its power from a pressurized fluid. This type of engine can be used to move heavy loads or produce motion. Performance  Engine speed In the case of engines outputting shaft power, engine speed is measured in revolutions per minute (RPM). Engines may be classified as low-speed, medium-speed or high-speed but these terms are inexact and depend on the type of engine being described. Generally, diesel engines operate at lower speed compared to gasoline engines. Electric motors and turboshafts are capable of very high speeds. In the case of engines producing thrust, it is rather inaccurate to talk of an 'engine speed' since what is moving is not the engine, but the working medium that the engine is accelerating; in this case one talks of an exhaust velocity, which is exactly the Isp outside of a gravitational field and therefore makes one jump straight to a discussion of efficiency; see the article on specific impulse for more information. Thrust Thrust is the force arising from the interaction between two masses which exert equal but opposite forces on each other due to their speed. The force F can be measured either in newtons (N, SI units) or in pounds-thrust (lbf, imperial units). Torque Torque is the force being exerted on a theoretical lever connected to the output shaft of an engine. This is expressed by the formula:                    τ                          |                                      r                          ×                              F                                    |                        r        F        sin        ⁡        (                              r                          ,                              F                          )              {\displaystyle \tau |{\mathbf {r}}\times {\mathbf {F}}|rF\sin({\mathbf {r}},{\mathbf {F}})}  where r is the length of the lever, F is the force applied on it, and r×F is the vector cross product. Torque is measured typically either in newton-metres (N·m, SI units) or in foot-pounds (ft·lb, imperial units). Power Power is the amount of work being done, or energy being produced, per unit of time. This is expressed by the formula:                    P                                                                    d                            W                                                      d                            t                                            {\displaystyle P{\frac {\mathrm {d} W}{\mathrm {d} t}}}  With a quick demonstration, it can be shown that:                    P                                      F                          ⋅                              v                                {\displaystyle P{\mathbf {F}}\cdot {\mathbf {v}}}  This formula with linear forces and speeds can be used equally well for both engines outputting thrust and engines exerting torque.When considering propulsive engines, typically only the raw force of the core mass flow is considered, leading to such engines having their 'power' rated in any of the units discussed above for forces.If the engine in question outputs its power on a shaft, then:                    P                τ        ω              {\displaystyle P\tau \omega }  .This is the reason why any engine outputting its power on a rotating shaft is usually quoted, along with its rated power, the rotational speed at which that rated power is developed. Efficiency Depending on the type of engine employed, different rates of efficiency are attained.For heat engines, efficiency cannot be greater than the Carnot efficiency. Sound levels In the case of sound levels, engine operation is of greatest impact with respect to mobile sources such as automobiles and trucks. Engine noise is a particularly large component of mobile source noise for vehicles operating at lower speeds, where aerodynamic and tire noise is less significant. Generally speaking, petrol and diesel engines emit less noise than turboshafts of equivalent power output; electric motors very often emit less noise than their fossil fuel-powered equivalents. Thrust-outputting engines, such as turbofans, turbojets and rockets emit the greatest amount of noise because their method of producing thrust is directly related to the production of sound. Various methods have been devised to reduce noise. Petrol and diesel engines are fitted with mufflers (silencers); newer turbofans often have outsized fans (the so-called high-bypass technology) in order to reduce the proportion of noisy, hot exhaust from the integrated turboshaft in the exhaust stream, and hushkits exist for older, low-bypass turbofans. No known methods exist for reducing the noise output of rockets without a corresponding reduction in thrust. Engines by use Particularly notable kinds of engines include:Aircraft engineAutomobile engineModel engineMotorcycle engineMarine propulsion engines such as Outboard motorNon-road engine is the term used to define engines that are not used by vehicles on roadways.Railway locomotive engineSpacecraft propulsion engines such as Rocket engineTraction engine See also Timeline of motor and engine technologyTimeline of heat engine technologyElectric motorEngine coolingMultifuelGasoline engineHesselman engineHCCI engineHot bulb engineIRIS engineSolid-state engineAutomobile engine replacementEngine swap Notes  References  External links U.S. Patent 194,047Detailed Engine AnimationsVideo from inside a four-stroke engine cylinder.Working 4-Stroke Engine - AnimationAnimated illustrations of various engines5 Ways to Redesign the Internal Combustion Engine
Poker is a family of card games that combine gambling, strategy, and skill . All poker variants involve betting as an intrinsic part of play, and determine the winner of each hand according to the combinations of players' cards, at least some of which remain hidden until the end of the hand. Poker games vary in the number of cards dealt, the number of shared or "community" cards, the number of cards that remain hidden, and the betting procedures on pokertogelmania.In most modern poker games, the first round of betting begins with one or more of the players making some form of a forced bet (the blind or ante). In standard poker, each player bets according to the rank they believe their hand is worth as compared to the other players. The action then proceeds clockwise as each player in turn must either match, or "call", the maximum previous bet or fold, losing the amount bet so far and all further interest in the hand. A player who matches a bet may also "raise", or increase the bet. The betting round ends when all players have either called the last bet or folded. If all but one player folds on any round, the remaining player collects the pot without being required to reveal their hand. If more than one player remains in contention after the final betting round, a showdown takes place where the hands are revealed, and the player with the winning hand takes the pot.With the exception of initial forced bets, money is only placed into the pot voluntarily by a player who either believes the bet has positive expected value or who is trying to bluff other players for various strategic reasons. Thus, while the outcome of any particular hand significantly involves chance, the long-run expectations of the players are determined by their actions chosen on the basis of probability, psychology, and game theory.Poker has gained in popularity since the beginning of the twentieth century and has gone from being primarily a recreational activity confined to small groups of enthusiasts to a widely popular activity, both for participants and spectators, including online, with many professional players and multimillion-dollar tournament prizes. History  19th century In the 1937 edition of Foster's Complete Hoyle, R. F. Foster wrote: "the game of poker, as first played in the United States, five cards to each player from a twenty-card pack, is undoubtedly the Persian game of As-Nas. By the 1990s some gaming historians including David Parlett started to challenge the notion that poker is a direct derivative of As-Nas. There is evidence that a game called poque, a French game similar to poker, was played around the region where poker is said to have originated. The name of the game likely descended from the Irish Poca (Pron. Pokah) ('Pocket') or even the French poque, which descended from the German pochen ('to brag as a bluff' lit. 'to knock'). Yet it is not clear whether the origins of poker itself lie with the games bearing those names. It is commonly regarded as sharing ancestry with the Renaissance game of primero and the French brelan. The English game brag (earlier bragg) clearly descended from brelan and incorporated bluffing (though the concept was known in other games by that time). It is quite possible that all of these earlier games influenced the development of poker as it exists now.A modern school of thought rejects these ancestries, as they focus on the card play in poker, which is trivial and could have been derived from any number of games or made up on general cardplay principles. The unique features of poker have to do with the betting, and do not appear in any known older game. In this view poker originated much later, in the early or mid-18th century, and spread throughout the Mississippi River region by 1800. It was played in a variety of forms, with 52 cards, and included both straight poker and stud. 20 card poker was a variant for two players (it is a common English practice to reduce the deck in card games when there are fewer players). The development of poker is linked to the historical movement that also saw the invention of commercial gambling.English actor Joseph Crowell reported that the game was played in New Orleans in 1829, with a deck of 20 cards, and four players betting on which player's hand was the most valuable. Jonathan H. Green's book, An Exposure of the Arts and Miseries of Gambling (G. B. Zieber, Philadelphia, 1843), described the spread of the game from there to the rest of the country by Mississippi riverboats, on which gambling was a common pastime. As it spread north along the Mississippi River and to the West during the gold rush, it is thought to have become a part of the frontier pioneer ethos.Soon after this spread, the full 52-card French deck was used and the flush was introduced. The draw was added prior to 1850 (when it was first mentioned in print in a handbook of games). During the American Civil War, many additions were made including stud poker (the five-card variant), and the straight. Further American developments followed, such as the wild card (around 1875), lowball and split-pot poker (around 1900), and community card poker games (around 1925). 20th century Developments in the 1970s led to poker becoming far more popular than it was before. Modern tournament play became popular in American casinos after the World Series of Poker began, in 1970. Notable champions from these early WSOP tournaments include Johnny Moss, Amarillo Slim, Bobby Baldwin, Doyle Brunson, and Puggy Pearson. Later in the 1970s, the first serious poker strategy books appeared, notably Super/System by Doyle Brunson (ISBN 1-58042-081-8) and Caro's Book of Poker Tells by Mike Caro (ISBN 0-89746-100-2), followed later by The Theory of Poker by David Sklansky (ISBN 1-880685-00-0).By the 1980s, poker was being depicted in popular culture as a commonplace recreational activity. For example, it was featured in at least 10 episodes of Star Trek: The Next Generation as a weekly event of the senior staff of the fictional ship's crew.In the 1990s, poker and casino gambling spread across the United States, most notably to Atlantic City, New Jersey. In 1998, Planet Poker dealt the first real money online poker game. In 1999, Late Night Poker debuted on British television. 21st century Poker's popularity experienced an unprecedented spike at the beginning of the 21st century, largely because of the introduction of online poker and hole-card cameras, which turned the game into a spectator sport. Not only could viewers now follow the action and drama of the game on television, they could also play the game in the comfort of their own home. Broadcasts of poker tournaments such as the World Series of Poker and World Poker Tour brought in huge audiences for cable and satellite TV distributors. Because of the increased coverage of poker events, poker pros became celebrities, with poker fans all over the world entering into tournaments for the chance to compete with them. Television coverage also added an important new dimension to the poker professional's game, as any given hand could now be aired later, revealing information not only to the other players at the table, but to anyone who cared to view the broadcast.Following the surge in popularity, new poker tours soon emerged, including the World Poker Tour and European Poker Tour, both televised, and the latter sponsored by online poker company PokerStars. Subsequent tours have since been created by PokerStars, such as Latin American Poker Tour and Asia Pacific Poker Tour, as well as other national tours. Beginning in 2003, major poker tournament fields grew dramatically, in part because of the growing popularity of online satellite-qualifier tournaments where the prize is an entry into a major tournament. The 2003 and 2004 World Series of Poker champions, Chris Moneymaker and Greg Raymer, respectively, won their seats to the main event by winning online satellites. In 2009 the International Federation of Poker was founded in Lausanne, Switzerland, becoming the official governing body for poker and promoting the game as a mind sport. In 2011 it announced plans for two new events: The Nations Cup, a duplicate poker team event, to be staged on the London Eye on the banks of the River Thames and "The Table", the invitation-only IFP World Championship, featuring roughly 130 of the world's best poker players, in an event to find the 2011 official "World Champion".After the passage of the UIGEA in October 2006, attendance at live tournaments as well as participation in live and online cash games initially slowed; however, they are still growing and far more popular today than they were before 2003. The growth and popularity of poker can be seen in the WSOP which had a record 7,319 entrants to the 2010 main event. The only nations in Europe that prohibit live poker are Norway, Poland and Albania, according to Dagbladet in 2011. Gameplay In casual play, the right to deal a hand typically rotates among the players and is marked by a token called a dealer button (or buck). In a casino, a house dealer handles the cards for each hand, but the button (typically a white plastic disk) is rotated clockwise among the players to indicate a nominal dealer to determine the order of betting. The cards are dealt clockwise around the poker table, one at a time.One or more players are usually required to make forced bets, usually either an ante or a blind bet (sometimes both). The dealer shuffles the cards, the player on the chair to his right cuts, and the dealer deals the appropriate number of cards to the players one at a time, beginning with the player to his left. Cards may be dealt either face-up or face-down, depending on the variant of poker being played. After the initial deal, the first of what may be several betting rounds begins. Between rounds, the players' hands develop in some way, often by being dealt additional cards or replacing cards previously dealt. At the end of each round, all bets are gathered into the central pot.At any time during a betting round, if one player bets, no opponents choose to call (match) the bet, and all opponents instead fold, the hand ends immediately, the bettor is awarded the pot, no cards are required to be shown, and the next hand begins. This is what makes bluffing possible. Bluffing is a primary feature of poker, one that distinguishes it from other vying games and from other games that make use of poker hand rankings.At the end of the last betting round, if more than one player remains, there is a showdown, in which the players reveal their previously hidden cards and evaluate their hands. The player with the best hand according to the poker variant being played wins the pot. A poker hand comprises five cards; in variants where a player has more than five cards available to them, only the best five-card combination counts. Variants Poker has many variations, all following a similar pattern of play and generally using the same hand ranking hierarchy. There are four main families of variants, largely grouped by the protocol of card-dealing and betting:StraightA complete hand is dealt to each player, and players bet in one round, with raising and re-raising allowed. This is the oldest poker family; the root of the game as now played was a game known as Primero, which evolved into the game three-card brag, a very popular gentleman's game around the time of the American Revolutionary War and still enjoyed in the U.K. today. Straight hands of five cards are sometimes used as a final showdown, but poker is almost always played in a more complex form to allow for additional strategy.Stud pokerCards are dealt in a prearranged combination of face-down and face-up rounds, or streets, with a round of betting following each. This is the next-oldest family; as poker progressed from three to five-card hands, they were often dealt one card at a time, either face-down or face-up, with a betting round between each. The most popular stud variant today, seven-card stud, deals two extra cards to each player (three face-down, four face-up) from which they must make the best possible 5-card hand.Draw pokerA complete hand is dealt to each player, face-down, and after betting, players are allowed to attempt to change their hand (with the object of improving it) by discarding unwanted cards and being dealt new ones. Five-card draw is the most famous variation in this family.Community card pokerAlso known as "flop poker", community card poker is a variation of stud poker. Players are dealt an incomplete hand of face-down cards, and then a number of face-up community cards are dealt to the center of the table, each of which can be used by one or more of the players to make a 5-card hand. Texas hold 'em and Omaha are two well-known variants of the community card family.Other games that use poker hand rankings may likewise be referred to as poker. Video poker is a single-player video game that functions much like a slot machine; most video poker machines play draw poker, where the player bets, a hand is dealt, and the player can discard and replace cards. Payout is dependent on the hand resulting after the draw and the player's initial bet.Strip poker is a traditional poker variation where players remove clothing when they lose bets. Since it depends only on the basic mechanic of betting in rounds, strip poker can be played with any form of poker; however, it is usually based on simple variants with few betting rounds, like five card draw.Another game with the poker name, but with a vastly different mode of play, is called Acey-Deucey or Red Dog poker. This game is more similar to Blackjack in its layout and betting; each player bets against the house, and then is dealt two cards. For the player to win, the third card dealt (after an opportunity to raise the bet) must have a value in-between the first two. Payout is based on the odds that this is possible, based on the difference in values of the first two cards. Other poker-like games played at casinos against the house include three card poker and pai gow poker. Computer programs In a January 2015 article published in Science, a group of researchers mostly from the University of Alberta announced that they "essentially weakly solved" heads-up limit Texas hold 'em with their development of their Cepheus poker bot. The authors claimed that Cepheus would lose at most 0.001 big blinds per game on average against its worst-case opponent, and the strategy is thus so "close to optimal" that "it can't be beaten with statistical significance within a lifetime of human poker playing".Less autonomous poker programs exist whose primary purpose is not to play poker by themselves, but is instead to calculate the odds of certain hand outcomes. For example, one might input a hand which contains three 7s and two unrelated low cards, the program in question would then return that holding just the 7s results in a 10.37% chance of an improved hand being drawn. See also Glossary of poker termsOnline pokerOutline of pokerList of poker handsUnderground poker References  External links Poker at DMOZ
Accounting or accountancy is the measurement, processing and communication of financial information about economic entities such as businesses and corporations. The modern field was established by the Italian mathematician Luca Pacioli in 1494. Accounting, which has been called the "language of business", measures the results of an organization's economic activities and conveys this information to a variety of users, including investors, creditors, management, and regulators. Practitioners of accounting are known as accountants. The terms "accounting" and "financial reporting" are often used as synonyms.Accounting can be divided into several fields including financial accounting, management accounting, external auditing, and tax accounting. Accounting information systems are designed to support accounting functions and related activities. Financial accounting focuses on the reporting of an organization's financial information, including the preparation of financial statements, to external users of the information, such as investors, regulators and suppliers; and management accounting focuses on the measurement, analysis and reporting of information for internal use by management. The recording of financial transactions, so that summaries of the financials may be presented in financial reports, is known as bookkeeping, of which double-entry bookkeeping is the most common system.Accounting is facilitated by accounting organizations such as standard-setters, accounting firms and professional bodies. Financial statements are usually audited by accounting firms, and are prepared in accordance with generally accepted accounting principles (GAAP). GAAP is set by various standard-setting organizations such as the Financial Accounting Standards Board (FASB) in the United States and the Financial Reporting Council in the United Kingdom. As of 2012, "all major economies" have plans to converge towards or adopt the International Financial Reporting Standards (IFRS). History The history of accounting is thousands of years old and can be traced to ancient civilizations. The early development of accounting dates back to ancient Mesopotamia, and is closely related to developments in writing, counting and money; there is also evidence for early forms of bookkeeping in ancient Iran, and early auditing systems by the ancient Egyptians and Babylonians. By the time of the Emperor Augustus, the Roman government had access to detailed financial information.Double-entry bookkeeping developed in medieval Europe, and accounting split into financial accounting and management accounting with the development of joint-stock companies. The first work on a double-entry bookkeeping system was published in Italy, by Luca Pacioli. Accounting began to transition into an organized profession in the nineteenth century, with local professional bodies in England merging to form the Institute of Chartered Accountants in England and Wales in 1880. Etymology Both the words accounting and accountancy were in use in Great Britain by the mid-1800s, and are derived from the words accompting and accountantship used in the 18th century. In Middle English (used roughly between the 12th and the late 15th century) the verb "to account" had the form accounten, which was derived from the Old French word aconter, which is in turn related to the Vulgar Latin word computare, meaning "to reckon". The base of computare is putare, which "variously meant to prune, to purify, to correct an account, hence, to count or calculate, as well as to think."The word "accountant" is derived from the French word compter, which is also derived from the Italian and Latin word computare. The word was formerly written in English as "accomptant", but in process of time the word, which was always pronounced by dropping the "p", became gradually changed both in pronunciation and in orthography to its present form. Accounting and accountancy Accounting has variously been defined as the keeping or preparation of the financial records of an entity, the analysis, verification and reporting of such records and "the principles and procedures of accounting"; it also refers to the job of being an accountant.Accountancy refers to the occupation or profession of an accountant, particularly in British English. Topics Accounting has several subfields or subject areas, including financial accounting, management accounting, auditing, taxation and accounting information systems. Financial accounting Financial accounting focuses on the reporting of an organization's financial information to external users of the information, such as investors, potencial investors and creditors. It calculates and records business transactions and prepares financial statements for the external users in accordance with generally accepted accounting principles (GAAP). GAAP, in turn, arises from the wide agreement between accounting theory and practice, and change over time to meet the needs of decision-makers.Financial accounting produces past-oriented reports—for example the financial statements prepared in 2006 reports on performance in 2005—on an annual or quarterly basis, generally about the organization as a whole.This branch of accounting is also studied as part of the board exams for qualifying as an actuary. It is interesting to note that these two professionals, accountants and actuaries, have created a culture of being archrivals. Management accounting Management accounting focuses on the measurement, analysis and reporting of information that can help managers in making decisions to fulfil the goals of an organization. In management accounting, internal measures and reports are based on cost-benefit analysis, and are not required to follow the generally accepted accounting principle (GAAP). In 2014 CIMA created the Global Management Accounting Principles (GMAPs). The result of research from across 20 countries in five continents, the principles aim to guide best practice in the discipline.Management accounting produces future-oriented reports—for example the budget for 2006 is prepared in 2005—and the time span of reports varies widely. Such reports may include both financial and non financial information, and may, for example, focus on specific products and departments. Auditing Auditing is the verification of assertions made by others regarding a payoff, and in the context of accounting it is the "unbiased examination and evaluation of the financial statements of an organization".An audit of financial statements aims to express or disclaim an opinion on the financial statements. The auditor expresses an opinion on the fairness with which the financial statements presents the financial position, results of operations, and cash flows of an entity, in accordance with the generally acceptable accounting principle (GAAP) and "in all material respects". An auditor is also required to identify circumstances in which the generally acceptable accounting principles (GAAP) has not been consistently observed. Accounting information systems An accounting information system is a part of an organisation's information system that focuses on processing accounting data. Tax accounting Tax accounting in the United States concentrates on the preparation, analysis and presentation of tax payments and tax returns. The U.S. tax system requires the use of specialised accounting principles for tax purposes which can differ from the generally accepted accounting principles (GAAP) for financial reporting. U.S. tax law covers four basic forms of business ownership: sole proprietorship, partnership, corporation, and limited liability company. Corporate and personal income are taxed at different rates, both varying according to income levels and including varying marginal rates (taxed on each additional dollar of income) and average rates (set as a percentage of overall income). Organizations  Professional bodies Professional accounting bodies include the American Institute of Certified Public Accountants (AICPA) and the other 179 members of the International Federation of Accountants (IFAC), including CPA Australia, Association of Chartered Certified Accountants (ACCA) and Institute of Chartered Accountants in England and Wales (ICAEW). Professional bodies for subfields of the accounting professions also exist, for example the Chartered Institute of Management Accountants (CIMA). Many of these professional bodies offer education and training including qualification and administration for various accounting designations, such as certified public accountant and chartered accountant. Accounting firms Depending on its size, a company may be legally required to have their financial statements audited by a qualified auditor, and audits are usually carried out by accounting firms.Accounting firms grew in the United States and Europe in the late nineteenth and early twentieth century, and through several mergers there were large international accounting firms by the mid-twentieth century. Further large mergers in the late twentieth century led to the dominance by the auditing market by the Big Four accounting firms: Arthur Andersen, Deloitte, Ernst & Young, KPMG and PricewaterhouseCoopers. The demise of Arthur Andersen following the Enron scandal reduced the Big Five to the Big Four. Standard-setters Generally accepted accounting principles (GAAP) are accounting standards issued by national regulatory bodies. In addition, the International Accounting Standards Board (IASB) issues the International Financial Reporting Standards (IFRS) implemented by 147 countries. While standards for international audit and assurance, ethics, education, and public sector accounting are all set by independent standard settings boards supported by IFAC. The International Auditing and Assurance Standards Board sets international standards for auditing, assurance, and quality control; the International Ethics Standards Board for Accountants (IESBA)  sets the internationally appropriate principles- based Code of Ethics for Professional Accounts the International Accounting Education Standards Board (IAESB) sets professional accounting education standards; International Public Sector Accounting Standards Board (IPSASB) sets accrual-based international public sector accounting standards Organizations in individual countries may issue accounting standards unique to the countries. For example, in the United States the Financial Accounting Standards Board (FASB) issues the Statements of Financial Accounting Standards, which form the basis of US GAAP, and in the United Kingdom the Financial Reporting Council (FRC) sets accounting standards. However, as of 2012 "all major economies" have plans to converge towards or adopt the IFRS. Education and qualifications  Accounting degrees At least a bachelor's degree in accounting or a related field is required for most accountant and auditor job positions, and some employers prefer applicants with a master's degree. A degree in accounting may also be required for, or may be used to fulfill the requirements for, membership to professional accounting bodies. For example, the education during an accounting degree can be used to fulfill the American Institute of CPA's (AICPA) 150 semester hour requirement, and associate membership with the Certified Public Accountants Association of the UK is available after gaining a degree in finance or accounting.A doctorate is required in order to pursue a career in accounting academia, for example to work as a university professor in accounting. The Doctor of Philosophy (PhD) and the Doctor of Business Administration (DBA) are the most popular degrees. The PhD is the most common degree for those wishing to pursue a career in academia, while DBA programs generally focus on equipping business executives for business or public careers requiring research skills and qualifications. Professional qualifications Professional accounting qualifications include the Chartered Accountant designations and other qualifications including certificates and diplomas. In the United Kingdom, chartered accountants of the ICAEW undergo annual training, and are bound by the ICAEW's code of ethics and subject to its disciplinary procedures. In the United States, the requirements for joining the AICPA as a Certified Public Accountant are set by the Board of Accountancy of each state, and members agree to abide by the AICPA's Code of Professional Conduct and Bylaws. In India the Apex Accounting body constituted by parliament of India is "Institute of Chartered Accountants of India" (ICAI) was known for its rigorous training and study methodology for granting the Qualification. Accounting research Accounting research is research in the effects of economic events on the process of accounting, and the effects of reported information on economic events. It encompasses a broad range of research areas including financial accounting, management accounting, auditing and taxation.Accounting research is carried out both by academic researchers and practicing accountants. Methodologies in academic accounting research can be classified into archival research, which examines "objective data collected from repositories"; experimental research, which examines data "the researcher gathered by administering treatments to subjects"; and analytical research, which is "based on the act of formally modeling theories or substantiating ideas in mathematical terms". This classification is not exhaustive; other possible methodologies include the use of case studies, computer simulations and field research. Accounting information system Many accounting practices have been simplified with the help of accounting computer-based software. An Enterprise resource planning (ERP) system is commonly used for a large organisation and it provides a comprehensive, centralized, integrated source of information that companies can use to manage all major business processes, from purchasing to manufacturing to human resources.Accounting information systems have reduced the cost of accumulating, storing, and reporting managerial accounting information and have made it possible to produce a more detailed account of all data that is entered into any given system. Accounting scandals The year 2001 witnessed a series of financial information frauds involving Enron, auditing firm Arthur Andersen, the telecommunications company WorldCom, Qwest and Sunbeam, among other well-known corporations. These problems highlighted the need to review the effectiveness of accounting standards, auditing regulations and corporate governance principles. In some cases, management manipulated the figures shown in financial reports to indicate a better economic performance. In others, tax and regulatory incentives encouraged over-leveraging of companies and decisions to bear extraordinary and unjustified risk.The Enron scandal deeply influenced the development of new regulations to improve the reliability of financial reporting, and increased public awareness about the importance of having accounting standards that show the financial reality of companies and the objectivity and independence of auditing firms.In addition to being the largest bankruptcy reorganization in American history, the Enron scandal undoubtedly is the biggest audit failure. It involved a financial scandal of Enron Corporation and their auditors Arthur Andersen, which was revealed in late 2001. The scandal caused the dissolution of Arthur Andersen, which at the time was one of the five largest accounting firms in the world. After a series of revelations involving irregular accounting procedures conducted throughout the 1990s, Enron filed for Chapter 11 bankruptcy protection in December 2001.One consequence of these events was the passage of Sarbanes–Oxley Act in the United States 2002, as a result of the first admissions of fraudulent behavior made by Enron. The act significantly raises criminal penalties for securities fraud, for destroying, altering or fabricating records in federal investigations or any scheme or attempt to defraud shareholders. See also Accounting records References  External links Library resources in your library and in other libraries about accountingOperations Research in Accounting on the Institute for Operations Research and the Management Sciences website
Computer graphics are pictures and films created using computers. Usually, the term refers to computer-generated image data created with help from specialized graphical hardware and software. It is a vast and recent area in computer science. The phrase was coined in 1960, by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, though sometimes erroneously referred to as CGI.Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization with ray tracing, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics.Computer graphics is responsible for displaying art and image data effectively and meaningfully to the user. It is also used for processing image data received from the physical world. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally. Overview The term computer graphics has been used a broad sense to describe "almost everything on computers that is not text or sound". Typically, the term computer graphics refers to several different things:the representation and manipulation of image data by a computerthe various technologies used to create and manipulate imagesthe sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content, see study of computer graphicsToday, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media "such graphs are used to illustrate papers, reports, thesis", and other presentation material.Many tools have been developed to visualize data. Computer generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with "the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component". History  Introduction The precursor sciences to the development of modern computer graphics were the advances in electrical engineering, electronics, and television that took place during the first half of the twentieth century. Screens could display art since the Lumiere brothers' use of mattes to create special effects for the earliest films dating from 1895, but such displays were limited and not interactive. The first cathode ray tube, the Braun tube, was invented in 1897 - it in turn would permit the oscilloscope and the military control panel - the more direct precursors of the field, as they provided the first two-dimensional electronic displays that responded to programmatic or user input. Nevertheless, computer graphics remained relatively unknown as a discipline until the 1950s and the post-World War II period - during which time, the discipline emerged from a combination of both pure university and laboratory academic research into more advanced computers and the United States military's further development of technologies like radar, advanced aviation, and rocketry developed during the war. New kinds of displays were needed to process the wealth of information resulting from such projects, leading to the development of computer graphics as a discipline. 1950s Early projects like the Whirlwind and SAGE Projects introduced the CRT as a viable display and interaction interface and introduced the light pen as an input device. Douglas T. Ross of the Whirlwind SAGE system performed a personal experiment in 1954 in which a small program he wrote captured the movement of his finger and displayed its vector (his traced name) on a display scope. One of the first interactive video games to feature recognizable, interactive graphics – Tennis for Two – was created for an oscilloscope by William Higinbotham to entertain visitors in 1958 at Brookhaven National Laboratory and simulated a tennis match. In 1959, Douglas T. Ross innovated again while working at MIT on transforming mathematic statements into computer generated machine tool vectors, and took the opportunity to create a display scope image of a Disney cartoon character.Electronics pioneer Hewlett-Packard went public in 1957 after incorporating the decade prior, and established strong ties with Stanford University through its founders, who were alumni. This began the decades-long transformation of the southern San Francisco Bay Area into the world's leading computer technology hub - now known as Silicon Valley. The field of computer graphics developed with the emergence of computer graphics hardware.Further advances in computing led to greater advancements in interactive computer graphics. In 1959, the TX-2 computer was developed at MIT's Lincoln Laboratory. The TX-2 integrated a number of new man-machine interfaces. A light pen could be used to draw sketches on the computer using Ivan Sutherland's revolutionary Sketchpad software. Using a light pen, Sketchpad allowed one to draw simple shapes on the computer screen, save them and even recall them later. The light pen itself had a small photoelectric cell in its tip. This cell emitted an electronic pulse whenever it was placed in front of a computer screen and the screen's electron gun fired directly at it. By simply timing the electronic pulse with the current location of the electron gun, it was easy to pinpoint exactly where the pen was on the screen at any given moment. Once that was determined, the computer could then draw a cursor at that location. Sutherland seemed to find the perfect solution for many of the graphics problems he faced. Even today, many standards of computer graphics interfaces got their start with this early Sketchpad program. One example of this is in drawing constraints. If one wants to draw a square for example, they do not have to worry about drawing four lines perfectly to form the edges of the box. One can simply specify that they want to draw a box, and then specify the location and size of the box. The software will then construct a perfect box, with the right dimensions and at the right location. Another example is that Sutherland's software modeled objects - not just a picture of objects. In other words, with a model of a car, one could change the size of the tires without affecting the rest of the car. It could stretch the body of car without deforming the tires. 1960s The phrase “computer graphics” itself was coined in 1960 by William Fetter, a graphic designer for Boeing. This old quote in many secondary sources comes complete with the following sentence: (Fetter has said that the terms were actually given to him by Verne Hudson of the Wichita Division of Boeing.) In 1961 another student at MIT, Steve Russell, created the second video game, Spacewar. Written for the DEC PDP-1, Spacewar was an instant success and copies started flowing to other PDP-1 owners and eventually DEC got a copy. The engineers at DEC used it as a diagnostic program on every new PDP-1 before shipping it. The sales force picked up on this quickly enough and when installing new units, would run the "world's first video game" for their new customers. (Higginbotham's Tennis For Two had beaten Spacewar by almost three years; but it was almost unknown outside of a research or academic setting.)E. E. Zajac, a scientist at Bell Telephone Laboratory (BTL), created a film called "Simulation of a two-giro gravity attitude control system" in 1963. In this computer-generated film, Zajac showed how the attitude of a satellite could be altered as it orbits the Earth. He created the animation on an IBM 7090 mainframe computer. Also at BTL, Ken Knowlton, Frank Sinden and Michael Noll started working in the computer graphics field. Sinden created a film called Force, Mass and Motion illustrating Newton's laws of motion in operation. Around the same time, other scientists were creating computer graphics to illustrate their research. At Lawrence Radiation Laboratory, Nelson Max created the films Flow of a Viscous Fluid and Propagation of Shock Waves in a Solid Form. Boeing Aircraft created a film called Vibration of an Aircraft.Also sometime in the early 1960s, automobiles would also provide a boost through the early work of Pierre Bézier at Renault, who used Paul de Casteljau's curves - now called Bézier curves after Bézier's work in the field - to develop 3d modeling techniques for Renault car bodies. These curves would form the foundation for much curve-modeling work in the field, as curves - unlike polygons - are mathematically complex entities to draw and model well.It was not long before major corporations started taking an interest in computer graphics. TRW, Lockheed-Georgia, General Electric and Sperry Rand are among the many companies that were getting started in computer graphics by the mid-1960s. IBM was quick to respond to this interest by releasing the IBM 2250 graphics terminal, the first commercially available graphics computer. Ralph Baer, a supervising engineer at Sanders Associates, came up with a home video game in 1966 that was later licensed to Magnavox and called the Odyssey. While very simplistic, and requiring fairly inexpensive electronic parts, it allowed the player to move points of light around on a screen. It was the first consumer computer graphics product. David C. Evans was director of engineering at Bendix Corporation's computer division from 1953 to 1962, after which he worked for the next five years as a visiting professor at Berkeley. There he continued his interest in computers and how they interfaced with people. In 1966, the University of Utah recruited Evans to form a computer science program, and computer graphics quickly became his primary interest. This new department would become the world's primary research center for computer graphics.Also in 1966, Ivan Sutherland continued to innovate at MIT when he invented the first computer controlled head-mounted display (HMD). Called the Sword of Damocles because of the hardware required for support, it displayed two separate wireframe images, one for each eye. This allowed the viewer to see the computer scene in stereoscopic 3D. After receiving his Ph.D. from MIT, Sutherland became Director of Information Processing at ARPA (Advanced Research Projects Agency), and later became a professor at Harvard. In 1967 Sutherland was recruited by Evans to join the computer science program at the University of Utah - a development which would turn that department into one of the most important research centers in graphics for nearly a decade thereafter, eventually producing some of the most important pioneers in the field. There Sutherland perfected his HMD; twenty years later, NASA would re-discover his techniques in their virtual reality research. At Utah, Sutherland and Evans were highly sought after consultants by large companies, but they were frustrated at the lack of graphics hardware available at the time so they started formulating a plan to start their own company.In 1968, Arthur Appel described the first algorithm for what would eventually become known as ray casting - a basis point for almost all of modern 3D graphics, as well as the later pursuit of photorealism in graphics.In 1969, the ACM initiated A Special Interest Group on Graphics (SIGGRAPH) which organizes conferences, graphics standards, and publications within the field of computer graphics. By 1973, the first annual SIGGRAPH conference was held, which has become one of the focuses of the organization. SIGGRAPH has grown in size and importance as the field of computer graphics has expanded over time. 1970s An astonishing amount of the breakthroughs in the field in this decade - particularly many important early breakthroughs in the transformation of graphics from utilitarian to realistic - occurred at the University of Utah in the 1970s, which had hired Ivan Sutherland away from MIT. Sutherland was paired with David C. Evans to teach an advanced computer graphics class, which contributed a great deal of founding research to the field and taught several students who would grow to found several of the industry's most important companies - namely Pixar, Silicon Graphics, and Adobe Systems.One of these students was Edwin Catmull. Catmull had just come from The Boeing Company and had been working on his degree in physics. Growing up on Disney, Catmull loved animation yet quickly discovered that he did not have the talent for drawing. Now Catmull (along with many others) saw computers as the natural progression of animation and they wanted to be part of the revolution. The first animation that Catmull saw was his own. He created an animation of his hand opening and closing. He also pioneered texture mapping to paint textures on three-dimensional models in 1974, now considered one of the fundamental techniques in 3D modeling. It became one of his goals to produce a feature-length motion picture using computer graphics - a goal he would achieve two decades later after his founding role in Pixar. In the same class, Fred Parke created an animation of his wife's face.As the UU computer graphics laboratory was attracting people from all over, John Warnock was another of those early pioneers; he would later found Adobe Systems and create a revolution in the publishing world with his PostScript page description language, and Adobe would go on later to create the industry standard photo editing software in Adobe Photoshop and the movie industry's special effects standard in Adobe After Effects. Tom Stockham led the image processing group at UU which worked closely with the computer graphics lab. Jim Clark was also there; he would later found Silicon Graphics.A major advance in 3D computer graphics was created at UU by these early pioneers - hidden surface determination. In order to draw a representation of a 3D object on the screen, the computer must determine which surfaces are "behind" the object from the viewer's perspective, and thus should be "hidden" when the computer creates (or renders) the image. The 3D Core Graphics System (or Core) was the first graphical standard to be developed. A group of 25 experts of the ACM Special Interest Group SIGGRAPH developed this "conceptual framework". The specifications were published in 1977, and it became a foundation for many future developments in the field.Also in the 1970s, Henri Gouraud, Jim Blinn and Bui Tuong Phong contributed to the foundations of shading in CGI via the development of the Gouraud shading and Blinn-Phong shading models, allowing graphics to move beyond a "flat" look to a look more accurately portraying depth. Jim Blinn also innovated further in 1978 by introducing bump mapping, a technique for simulating uneven surfaces, and the predecessor to many more advanced kinds of mapping used today.The modern videogame arcade as is known today was birthed in the 1970s, with the first arcade games using real-time 2D sprite graphics. Pong in 1972 was one of the first hit arcade cabinet games. Speed Race in 1974 featured sprites moving along a vertically scrolling road. Gun Fight in 1975 featured human-looking sprite character graphics, while Space Invaders in 1978 featured a large number of sprites on screen; both used an Intel 8080 microprocessor and Fujitsu MB14241 video shifter to accelerate the drawing of sprite graphics. 1980s The 1980s began to see the modernization and commercialization of computer graphics. As the home computer proliferated, a subject which had previously been an academics-only discipline was adopted by a much larger audience, and the number of computer graphics developers increased significantly.In the early 1980s, the availability of bit-slice and 16-bit microprocessors started to revolutionize high-resolution computer graphics terminals which now increasingly became intelligent, semi-standalone and standalone workstations. Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. Typical of the early move to high-resolution computer graphics intelligent workstations for the computer-aided engineering market were the Orca 1000, 2000 and 3000 workstations, developed by Orcatech of Ottawa, a spin-off from Bell-Northern Research, and led by David Pearson, an early workstation pioneer. The Orca 3000 was based on Motorola 68000 and AMD bit-slice processors and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector. Artists and graphic designers began to see the personal computer, particularly the Commodore Amiga and Macintosh, as a serious design tool, one that could save time and draw more accurately than other methods. The Macintosh remains a highly popular tool for computer graphics among graphic design studios and businesses. Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology.In the field of realistic rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: "The core of 3D image rendering is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an image rendering methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed image rendering, LINKS-1 was able to rapidly render highly realistic images. It was used to create the world's first 3D planetarium-like video of the entire heavens that was made completely with computer graphics. The video was presented at the Fujitsu pavilion at the 1985 International Exposition in Tsukuba." The LINKS-1 was the world's most powerful computer, as of 1984. Also in the field of realistic rendering, the general rendering equation of David Immel and James Kajiya was developed in 1986 - an important step towards implementing global illumination, which is necessary to pursue photorealism in computer graphics.The continuing popularity of Star Wars and other science fiction franchises were relevant in cinematic CGI at this time, as Lucasfilm and Industrial Light & Magic became known as the "go-to" house by many other studios for topnotch computer graphics in film. Important advances in chroma keying ("bluescreening", etc.) were made for the later films of the original trilogy. Two other pieces of video would also outlast the era as historically relevant: Dire Straits' iconic, near-fully-CGI video for their song "Money For Nothing" in 1985, which popularized CGI among music fans of that era, and a scene from Young Sherlock Holmes the same year featuring the first fully CGI character in a feature movie (an animated stained-glass knight). In 1988, the first shaders - small programs designed specifically to do shading as a separate algorithm - were developed by Pixar, which had already spun off from Industrial Light & Magic as a separate entity - though the public would not see the results of such technological progress until the next decade. In the late 1980s, SGI computers were used to create some of the first fully computer-generated short films at Pixar, and Silicon Graphics machines were considered a high-water mark for the field during the decade.The 1980s is also called the golden era of videogames; millions-selling systems from Atari, Nintendo and Sega, among other companies, exposed computer graphics for the first time to a new, young, and impressionable audience - as did MS-DOS-based personal computers, Apple IIs and Macs, and Amigas, which also allowed users to program their own games if skilled enough. Demoscenes and shareware games proliferated; John Carmack, a later 3D innovator, would start out in this period developing sprite-based games. In the arcades, advances were made in commercial, real-time 3D graphics. In 1988, the first dedicated real-time 3D graphics boards were introduced in arcades, with the Namco System 21 and Taito Air System. This innovation would be the precursor of the later home graphics processing unit or GPU, a technology where a separate and very powerful chip is used in parallel processing with a CPU to optimize graphics. 1990s The 1990s' overwhelming note was the emergence of 3D modeling on a mass scale and an impressive rise in the quality of CGI generally. Home computers became able to take on rendering tasks that previously had been limited to workstations costing thousands of dollars; as 3D modelers became available for home systems, the popularity of Silicon Graphics workstations declined and powerful Microsoft Windows and Apple Macintosh machines running Autodesk products like 3D Studio or other home rendering software ascended in importance. By the end of the decade, the GPU would begin its rise to the prominence it still enjoys today.The field began to see the first rendered graphics that could truly pass as photorealistic to the untrained eye (though they could not yet do so with a trained CGI artist) and 3D graphics became far more popular in gaming, multimedia, and animation. At the end of the 1980s and the beginning of the nineties were created, in France, the very first computer graphics TV series: La Vie des bêtes by studio Mac Guff Ligne (1988), Les Fables Géométriques (1989-1991) by studio Fantôme, and Quarxs, the first HDTV computer graphics series by Maurice Benayoun and François Schuiten (studio Z-A production, 1990–1993).In film, Pixar began its serious commercial rise in this era under Edwin Catmull, with its first major film release, in 1995 - Toy Story - a critical and commercial success of nine-figure magnitude. The studio to invent the programmable shader would go on to have many animated hits, and its work on prerendered video animation is still considered an industry leader and research trail breaker.In video games, in 1992, Virtua Racing, running on the Sega Model 1 arcade system board, laid the foundations for fully 3D racing games and popularized real-time 3D polygonal graphics among a wider audience in the video game industry. The Sega Model 2 in 1993 and Sega Model 3 in 1996 subsequently pushed the boundaries of commercial, real-time 3D graphics. Back on the PC, Wolfenstein 3D, Doom and Quake, three of the first massively popular 3D first-person shooter games, were released by id Software to critical and popular acclaim during this decade using a rendering engine innovated primarily by John Carmack. The Sony Playstation and Nintendo 64, among other consoles, sold in the millions and popularized 3D graphics for home gamers. Certain late-90's first-generation 3D titles became seen as influential in popularizing 3D graphics among console users, such as platform games Super Mario 64 and The Legend Of Zelda: Ocarina Of Time, and early 3D fighting games like Virtua Fighter, Battle Arena Toshinden, and Tekken.Technology and algorithms for rendering continued to improve greatly. In 1996, Krishnamurty and Levoy invented normal mapping - an improvement on Jim Blinn's bump mapping. 1999 saw Nvidia release the seminal GeForce 256, the first home video card billed as a graphics processing unit or GPU, which in its own words contained "integrated transform, lighting, triangle setup/clipping, and rendering engines". By the end of the decade, computers adopted common frameworks for graphics processing such as DirectX and OpenGL. Since then, computer graphics have only become more detailed and realistic, due to more powerful graphics hardware and 3D modeling software. AMD also became a leading developer of graphics boards in this decade, creating a "duopoly" in the field which exists this day. 2000s CGI became ubiquitous in earnest during this era. Video games and CGI cinema had spread the reach of computer graphics to the mainstream by the late 1990s and continued to do so at an accelerated pace in the 2000s. CGI was also adopted en masse for television advertisements widely in the late 1990s and 2000s, and so became familiar to a massive audience.The continued rise and increasing sophistication of the graphics processing unit were crucial to this decade, and 3D rendering capabilities became a standard feature as 3D-graphics GPUs became considered a necessity for desktop computer makers to offer. The Nvidia GeForce line of graphics cards dominated the market in the early decade with occasional significant competing presence from ATI. As the decade progressed, even low-end machines usually contained a 3D-capable GPU of some kind as Nvidia and AMD both introduced low-priced chipsets and continued to dominate the market. Shaders which had been introduced in the 1980s to perform specialized processing on the GPU would by the end of the decade become supported on most consumer hardware, speeding up graphics considerably and allowing for greatly improved texture and shading in computer graphics via the widespread adoption of normal mapping, bump mapping, and a variety of other techniques allowing the simulation of a great amount of detail.Computer graphics used in films and video games gradually began to be realistic to the point of entering the uncanny valley. CGI movies proliferated, with traditional animated cartoon films like Ice Age and Madagascar as well as numerous Pixar offerings like Finding Nemo dominating the box office in this field. The Final Fantasy: The Spirits Within, released in 2001, was the first fully computer-generated feature film to use photorealistic CGI characters and be fully made with motion capture. The film was not a box-office success, however. Some commentators have suggested this may be partly because the lead CGI characters had facial features which fell into the "uncanny valley". Other animated films like The Polar Express drew attention at this time as well. Star Wars also resurfaced with its prequel trilogy and the effects continued to set a bar for CGI in film.In videogames, the Sony PlayStation 2 and 3, the Microsoft Xbox line of consoles, and offerings from Nintendo such as the GameCube maintained a large following, as did the Windows PC. Marquee CGI-heavy titles like the series of Grand Theft Auto, Assassin's Creed, Final Fantasy, BioShock, Kingdom Hearts, Mirror's Edge and dozens of others continued to approach photorealism, grow the video game industry and impress, until that industry's revenues became comparable to those of movies. Microsoft made a decision to expose DirectX more easily to the independent developer world with the XNA program, but it was not a success. DirectX itself remained a commercial success, however. OpenGL continued to mature as well, and it and DirectX improved greatly; the second-generation shader languages HLSL and GLSL began to be popular in this decade.In scientific computing, the GPGPU technique to pass large amounts of data bidirectionally between a GPU and CPU was invented; speeding up analysis on many kinds of bioinformatics and molecular biology experiments. The technique has also been used for Bitcoin mining and has applications in computer vision. 2010s In the early half of the 2010s, CGI is nearly ubiquitous in video, pre-rendered graphics are nearly scientifically photorealistic, and real-time graphics on a suitably high-end system may simulate photorealism to the untrained eye.Texture mapping has matured into a multistage process with many layers; generally, it is not uncommon to implement texture mapping, bump mapping or isosurfaces, normal mapping, lighting maps including specular highlights and reflection techniques, and shadow volumes into one rendering engine using shaders, which are maturing considerably. Shaders are now very nearly a necessity for advanced work in the field, providing considerable complexity in manipulating pixels, vertices, and textures on a per-element basis, and countless possible effects. Their shader languages HLSL and GLSL are active fields of research and development. Physically based rendering or PBR, which implements even more maps to simulate real optic light flow, is an active research area as well, along with advanced optics areas like subsurface scattering and photon mapping. Experiments into the processing power required to provide graphics in real time at ultra-high-resolution modes like Ultra HD are beginning, though beyond reach of all but the highest-end hardware.In cinema, most animated movies are CGI now; a great many animated CGI films are made per year, but few, if any, attempt photorealism due to continuing fears of the uncanny valley. Most are 3D cartoons.In videogames, the Xbox One by Microsoft, Sony Playstation 4, and Nintendo Wii U currently dominate the home space and are all capable of highly advanced 3D graphics; the Windows PC is still one of the most active gaming platforms as well. Image types  Two-dimensional 2D computer graphics are the computer-based generation of digital images—mostly from models, such as digital image, and by techniques specific to them.2D computer graphics are mainly used in applications that were originally developed upon traditional printing and drawing technologies such as typography. In those applications, the two-dimensional image is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred because they give more direct control of the image than 3D computer graphics, whose approach is more akin to photography than to typography. Pixel art A large form of digital art, pixel art is created through the use of raster graphics software, where images are edited on the pixel level. Graphics in most old (or relatively limited) computer and video games, graphing calculator games, and many mobile phone games are mostly pixel art. Sprite graphics A sprite is a two-dimensional image or animation that is integrated into a larger scene. Initially including just graphical objects handled separately from the memory bitmap of a video display, this now includes various manners of graphical overlays.Originally, sprites were a method of integrating unrelated bitmaps so that they appeared to be part of the normal bitmap on a screen, such as creating an animated character that can be moved on a screen without altering the data defining the overall screen. Such sprites can be created by either electronic circuitry or software. In circuitry, a hardware sprite is a hardware construct that employs custom DMA channels to integrate visual elements with the main screen in that it super-imposes two discrete video sources. Software can simulate this through specialized rendering methods. Vector graphics Vector graphics formats are complementary to raster graphics. Raster graphics is the representation of images as an array of pixels and is typically used for the representation of photographic images. Vector graphics consists in encoding information about shapes and colors that comprise the image, which can allow for more flexibility in rendering. There are instances when working with vector tools and formats is best practice, and instances when working with raster tools and formats is best practice. There are times when both formats come together. An understanding of the advantages and limitations of each technology and the relationship between them is most likely to result in efficient and effective use of tools. Three-dimensional 3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing.Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques.3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model is the representation of any 3D object. Until visually displayed a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations. Computer animation Computer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.Virtual entities may contain and be controlled by assorted attributes, such as transform values (location, orientation, and scale) stored in an object's transformation matrix. Animation is the change of an attribute over time. Multiple methods of achieving animation exist; the rudimentary form is based on the creation and editing of keyframes, each storing a value at a given time, per attribute to be animated. The 2D/3D graphics software will change with each keyframe, creating an editable curve of a value mapped over time, in which results in animation. Other methods of animation include procedural and expression-based techniques: the former consolidates related elements of animated entities into sets of attributes, useful for creating particle effects and crowd simulations; the latter allows an evaluated result returned from a user-defined logical expression, coupled with mathematics, to automate animation in a predictable way (convenient for controlling bone behavior beyond what a hierarchy offers in skeletal system set up).To create the illusion of movement, an image is displayed on the computer screen then quickly replaced by a new image that is similar to the previous image, but shifted slightly. This technique is identical to the illusion of movement in television and motion pictures. Concepts and principles Images are typically created by devices such as cameras, mirrors, lenses, telescopes, microscopes, etc.Digital images include both vector images and raster images, but raster images are more commonly used. Pixel In digital imaging, a pixel (or picture element) is a single point in a raster image. Pixels are placed on a regular 2-dimensional grid, and are often represented using dots or squares. Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel has typically three components such as red, green, and blue.Graphics are visual presentations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other [[image]]s. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style. Primitives Primitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2d video games, geometric primitives in CAD, or polygons or triangles in 3d rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application. Rendering Rendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term "rendering" may be by analogy with an "artist's rendering" of a scene. Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output.3D projection3D projection is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread, especially in computer graphics, engineering and drafting.Ray tracingRay tracing is a technique for generating an image by tracing the path of light through pixels in an image plane. The technique is capable of producing a very high degree of photorealism; usually higher than that of typical scanline rendering methods, but at a greater computational cost.ShadingShading refers to depicting depth in 3D models or illustrations by varying levels of darkness. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including cross hatching where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that shaders are applied.Texture mappingTexture mapping is a method for adding detail, surface texture, or colour to a computer-generated graphic or 3D model. Its application to 3D graphics was pioneered by Dr Edwin Catmull in 1974. A texture map is applied (mapped) to the surface of a shape, or polygon. This process is akin to applying patterned paper to a plain white box. Multitexturing is the use of more than one texture at a time on a polygon. Procedural textures (created from adjusting parameters of an underlying algorithm that produces an output texture), and bitmap textures (created in an image editing application or imported from a digital camera) are, generally speaking, common methods of implementing texture definition on 3D models in computer graphics software, while intended placement of textures onto a model's surface often requires a technique known as UV mapping (arbitrary, manual layout of texture coordinates) for polygon surfaces, while NURBS surfaces have their own intrinsic parameterization used as texture coordinates. Texture mapping as a discipline also encompasses techniques for creating normal maps and bump maps that correspond to a texture to simulate height and specular maps to help simulate shine and light reflections, as well as environment mapping to simulate mirror-like reflectivity, also called gloss.Anti-aliasingRendering resolution-independent entities (such as 3D models) for viewing on a raster (pixel-based) device such as a liquid-crystal display or CRT television inevitably causes aliasing artifacts mostly along geometric edges and the boundaries of texture details; these artifacts are informally called "jaggies". Anti-aliasing methods rectify such problems, resulting in imagery more pleasing to the viewer, but can be somewhat computationally expensive. Various anti-aliasing algorithms (such as supersampling) are able to be employed, then customized for the most efficient rendering performance versus quality of the resultant imagery; a graphics artist should consider this trade-off if anti-aliasing methods are to be used. A pre-anti-aliased bitmap texture being displayed on a screen (or screen location) at a resolution different than the resolution of the texture itself (such as a textured model in the distance from the virtual camera) will exhibit aliasing artifacts, while any procedurally defined texture will always show aliasing artifacts as they are resolution-independent; techniques such as mipmapping and texture filtering help to solve texture-related aliasing problems. Volume rendering Volume rendering is a technique used to display a 2D projection of a 3D discretely sampled data set. A typical 3D data set is a group of 2D slice images acquired by a CT or MRI scanner.Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This is an example of a regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel. 3D modeling 3D modeling is the process of developing a mathematical, wireframe representation of any three-dimensional object, called a "3D model", via specialized software. Models may be created automatically or manually; the manual modeling process of preparing geometric data for 3D computer graphics is similar to plastic arts such as sculpting. 3D models may be created using multiple approaches: use of NURBS curves to generate accurate and smooth surface patches, polygonal mesh modeling (manipulation of faceted geometry), or polygonal mesh subdivision (advanced tessellation of polygons, resulting in smooth surfaces similar to NURBS models). A 3D model can be displayed as a two-dimensional image through a process called 3D rendering, used in a computer simulation of physical phenomena, or animated directly for other purposes. The model can also be physically created using 3D Printing devices. Pioneers in computer graphics Charles CsuriCharles Csuri is a pioneer in computer animation and digital fine art and created the first computer art in 1964. Csuri was recognized by Smithsonian as the father of digital art and computer animation, and as a pioneer of computer animation by the Museum of Modern Art (MoMA) and Association for Computing Machinery-SIGGRAPH.Donald P. GreenbergDonald P. Greenberg is a leading innovator in computer graphics. Greenberg has authored hundreds of articles and served as a teacher and mentor to many prominent computer graphic artists, animators, and researchers such as Robert L. Cook, Marc Levoy, Brian A. Barsky, and Wayne Lytle. Many of his former students have won Academy Awards for technical achievements and several have won the SIGGRAPH Achievement Award. Greenberg was the founding director of the NSF Center for Computer Graphics and Scientific Visualization.A. Michael NollNoll was one of the first researchers to use a digital computer to create artistic patterns and to formalize the use of random processes in the creation of visual arts. He began creating digital art in 1962, making him one of the earliest digital artists. In 1965, Noll along with Frieder Nake and Georg Nees were the first to publicly exhibit their computer art. During April 1965, the Howard Wise Gallery exhibited Noll's computer art along with random-dot patterns by Bela Julesz. Other pioneers Pierre BézierJim BlinnJack BresenhamJohn CarmackPaul de CasteljauEd CatmullFrank CrowJames D. FoleyWilliam FetterHenry FuchsHenri GouraudCharles LoopNadia Magnenat ThalmannBenoît B. MandelbrotMartin NewellFred ParkeBui Tuong PhongSteve RussellDaniel J. SandinAlvy Ray SmithBob SproullIvan SutherlandDaniel ThalmannAndries van DamJohn WarnockLance WilliamsJim Kajiya Important organizations SIGGRAPHSIGGRAPH AsiaGDCBell Telephone LaboratoriesUnited States Armed Forces, particularly the Whirlwind computer and SAGE ProjectBoeingIBMRenaultThe computer science department of the University of UtahLucasfilm and Industrial Light & MagicAutodeskAdobe SystemsPixarSilicon Graphics, Khronos Group & OpenGLThe DirectX division at MicrosoftNvidiaAMD Study of computer graphics The study of computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.As an academic discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities. Applications Computer graphics may be used in the following areas:Computational biologyComputational physicsComputer-aided designComputer simulationDigital artEducationGraphic designInfographicsInformation visualizationRational drug designScientific visualizationSpecial Effects for cinemaVideo GamesVirtual realityWeb designDesign See also Glossary of computer graphics References  Further reading L. Ammeraal and K. Zhang (2007). Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5.David Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill.James D. Foley, Andries Van Dam, Steven K. Feiner and John F. Hughes (1995). Computer Graphics: Principles and Practice. Addison-Wesley.Donald Hearn and M. Pauline Baker (1994). Computer Graphics. Prentice-Hall.Francis S. Hill (2001). Computer Graphics. Prentice Hall.John Lewell (1985). Computer Graphics: A Survey of Current Techniques and Applications. Van Nostrand Reinhold.Jeffrey J. McConnell (2006). Computer Graphics: Theory Into Practice. Jones & Bartlett Publishers.R. D. Parslow, R. W. Prowse, Richard Elliot Green (1969). Computer Graphics: Techniques and Applications.Peter Shirley and others. (2005). Fundamentals of computer graphics. A.K. Peters, Ltd.M. Slater, A. Steed, Y. Chrysantho (2002). Computer graphics and virtual environments: from realism to real-time. Addison-Wesley.Wolfgang Höhl (2008): Interactive environments with open-source software, Springer Wien New York, ISBN 3-211-79169-8 External links A Critical History of Computer Graphics and AnimationHistory of Computer Graphics series of articlesComputer Graphics research at UC BerkeleyThomas Dreher: History of Computer Art, chap. IV.2 Computer Animation
Volleyball is a team sport in which two teams of six players are separated by a net. Each team tries to score points by grounding a ball on the other team's court under organized rules. It has been a part of the official program of the Summer Olympic Games since 1964.The complete rules are extensive. But simply, play proceeds as follows: a player on one of the teams begins a 'rally' by serving the ball (tossing or releasing it and then hitting it with a hand or arm), from behind the back boundary line of the court, over the net, and into the receiving team's court. The receiving team must not let the ball be grounded within their court. The team may touch the ball up to 3 times but individual players may not touch the ball twice consecutively. Typically, the first two touches are used to set up for an attack, an attempt to direct the ball back over the net in such a way that the serving team is unable to prevent it from being grounded in their court.The rally continues, with each team allowed as many as three consecutive touches, until either (1): a team makes a kill, grounding the ball on the opponent's court and winning the rally; or (2): a team commits a fault and loses the rally. The team that wins the rally is awarded a point, and serves the ball to start the next rally. A few of the most common faults include:causing the ball to touch the ground or floor outside the opponents' court or without first passing over the net;catching and throwing the ball;double hit: two consecutive contacts with the ball made by the same player;four consecutive contacts with the ball made by the same team;net foul: touching the net during play;foot fault: the foot crosses over the boundary line when serving.The ball is usually played with the hands or arms, but players can legally strike or push (short contact) the ball with any part of the body.A number of consistent techniques have evolved in volleyball, including spiking and blocking (because these plays are made above the top of the net, the vertical jump is an athletic skill emphasized in the sport) as well as passing, setting, and specialized player positions and offensive and defensive structures. History  Origin of volleyball On February 9, 1895, in Holyoke, Massachusetts (USA), William G. Morgan, a YMCA physical education director, created a new game called Mintonette as a pastime to be played (preferably) indoors and by any number of players. The game took some of its characteristics from tennis and handball. Another indoor sport, basketball, was catching on in the area, having been invented just ten miles (sixteen kilometers) away in the city of Springfield, Massachusetts, only four years before. Mintonette was designed to be an indoor sport, less rough than basketball, for older members of the YMCA, while still requiring a bit of athletic effort.The first rules, written down by William G Morgan, called for a net 6 ft 6 in (1.98 m) high, a 25 ft × 50 ft (7.6 m × 15.2 m) court, and any number of players. A match was composed of nine innings with three serves for each team in each inning, and no limit to the number of ball contacts for each team before sending the ball to the opponents' court. In case of a serving error, a second try was allowed. Hitting the ball into the net was considered a foul (with loss of the point or a side-out)—except in the case of the first-try serve.After an observer, Alfred Halstead, noticed the volleying nature of the game at its first exhibition match in 1896, played at the International YMCA Training School (now called Springfield College), the game quickly became known as volleyball (it was originally spelled as two words: "volley ball"). Volleyball rules were slightly modified by the International YMCA Training School and the game spread around the country to various YMCAs. Refinements and later developments The first official ball used in volleyball is disputed; some sources say that Spalding created the first official ball in 1896, while others claim it was created in 1900. The rules evolved over time: in the Philippines by 1916, the skill and power of the set and spike had been introduced, and four years later a "three hits" rule and a rule against hitting from the back row were established. In 1917, the game was changed from 21 to 15 points. In 1919, about 16,000 volleyballs were distributed by the American Expeditionary Forces to their troops and allies, which sparked the growth of volleyball in new countries.The first country outside the United States to adopt volleyball was Canada in 1900. An international federation, the Fédération Internationale de Volleyball (FIVB), was founded in 1947, and the first World Championships were held in 1949 for men and 1952 for women. The sport is now popular in Brazil, in Europe (where especially Italy, the Netherlands, and countries from Eastern Europe have been major forces since the late 1980s), in Russia, and in other countries including China and the rest of Asia, as well as in the United States.Beach volleyball, a variation of the game played on sand and with only two players per team, became a FIVB-endorsed variation in 1987 and was added to the Olympic program at the 1996 Summer Olympics. Volleyball is also a sport at the Paralympics managed by the World Organization Volleyball for Disabled.Nudists were early adopters of the game with regular organized play in clubs as early as the late 1920s. By the 1960s, a volleyball court had become standard in almost all nudist/naturist clubs. Volleyball in the Olympics The history of Olympic volleyball traces back to the 1924 Summer Olympics in Paris, where volleyball was played as part of an American sports demonstration event. After the foundation of FIVB and some continental confederations, it began to be considered for official inclusion. In 1957, a special tournament was held at the 53rd IOC session in Sofia, Bulgaria to support such request. The competition was a success, and the sport was officially included in the program for the 1964 Summer Olympics.The Olympic volleyball tournament was originally a simple competition: all teams played against each other team and then were ranked by wins, set average, and point average. One disadvantage of this round-robin system is that medal winners could be determined before the end of the games, making the audience lose interest in the outcome of the remaining matches. To cope with this situation, the competition was split into two phases with the addition of a "final round" elimination tournament consisting of quarterfinals, semifinals, and finals matches in 1972. The number of teams involved in the Olympic tournament has grown steadily since 1964. Since 1996, both men's and women's events count twelve participant nations. Each of the five continental volleyball confederations has at least one affiliated national federation involved in the Olympic Games.The U.S.S.R. won men's gold in both 1964 and 1968. After taking bronze in 1964 and silver in 1968, Japan finally won the gold for men's volleyball in 1972. Women's gold went to Japan in 1964 and again in 1976. That year, the introduction of a new offensive skill, the backrow attack, allowed Poland to win the men's competition over the Soviets in a very tight five-set match. Since the strongest teams in men's volleyball at the time belonged to the Eastern Bloc, the American-led boycott of the 1980 Summer Olympics did not have as great an effect on these events as it had on the women's. The U.S.S.R. collected their third Olympic Gold Medal in men's volleyball with a 3–1 victory over Bulgaria (the Soviet women won that year as well, their third gold as well). With the U.S.S.R. boycotting the 1984 Olympic Games in Los Angeles, the U.S. was able to sweep Brazil in the finals to win the men's gold medal. Italy won its first medal (bronze in the men's competition) in 1984, foreshadowing a rise in prominence for their volleyball teams. The 1984 women's tournament was also won by a rising force, China.At the 1988 Games, Karch Kiraly and Steve Timmons led the U.S. men's team to a second straight gold medal, and the Soviets won the fourth gold in the women's tournament. In 1992, underrated Brazil upset favourites C.I.S., Netherlands, and Italy in the men's competition for the country's first volleyball Olympic gold medal. Runner-up Netherlands, men's silver medalist in 1992, came back under team leaders Ron Zwerver and Olof van der Meulen in the 1996 Games for a five-set win over Italy. A men's bronze medalist in 1996, Serbia and Montenegro (playing in 1996 and 2000 as the Federal Republic of Yugoslavia) beat Russia in the gold medal match in 2000, winning their first gold medal ever. In all three games the strong Cuban female team led by Regla Torres and Mireya Luis won the Gold medal. In 2004, Brazil won its second men's volleyball gold medal beating Italy in the finals, while China beat Russia for its second women's title. In the 2008 Games, the USA beat Brazil in the men's volleyball final. Brazil was runner-up again at the 2012 Summer Olympics, this time losing to Russia after losing two match points in the third set. In both games Brazil's women team beat the United States for the gold medal. Rules of the game  The court dimensions A volleyball court is 9 m × 18 m (29.53 ft × 59.06 ft), divided into equal square halves by a net with a width of one meter (39.4 in). The top of the net is 2.43 m (7 ft 11 21⁄32 in) above the center of the court for men's competition, and 2.24 m (7 ft 4 3⁄16 in) for women's competition, varied for veterans and junior competitions.The minimum height clearance for indoor volleyball courts is 7 m (23 ft), although a clearance of 8 m (26 ft) is recommended.A line 3 m (9.84 ft) from and parallel to the net is considered the "attack line". This "3 meter" (or "10-foot") line divides the court into "back row" and "front row" areas (also back court and front court). These are in turn divided into 3 areas each: these are numbered as follows, starting from area "1", which is the position of the serving player:After a team gains the serve (also known as siding out), its members must rotate in a clockwise direction, with the player previously in area "2" moving to area "1" and so on, with the player from area "1" moving to area "6". Each player rotates only one time after the team gains possession of the serve; the next time each player rotates will be after the other team wins possession of the ball and loses the point.The team courts are surrounded by an area called the free zone which is a minimum of 3 meters wide and which the players may enter and play within after the service of the ball. All lines denoting the boundaries of the team court and the attack zone are drawn or painted within the dimensions of the area and are therefore a part of the court or zone. If a ball comes in contact with the line, the ball is considered to be "in". An antenna is placed on each side of the net perpendicular to the sideline and is a vertical extension of the side boundary of the court. A ball passing over the net must pass completely between the antenna (or their theoretical extensions to the ceiling) without contacting them. The ball FIVB regulations state that the ball must be spherical, made of leather or synthetic leather, have a circumference of 65–67 cm, a weight of 260–280 g and an inside pressure of 0.30–0.325 kg/cm2. Other governing bodies have similar regulations. Game play Each team consists of six players. To get play started, a team is chosen to serve by coin toss. A player from the serving team throws the ball into the air and attempts to hit the ball so it passes over the net on a course such that it will land in the opposing team's court (the serve). The opposing team must use a combination of no more than three contacts with the volleyball to return the ball to the opponent's side of the net. These contacts usually consist first of the bump or pass so that the ball's trajectory is aimed towards the player designated as the setter; second of the set (usually an over-hand pass using wrists to push finger-tips at the ball) by the setter so that the ball's trajectory is aimed towards a spot where one of the players designated as an attacker can hit it, and third by the attacker who spikes (jumping, raising one arm above the head and hitting the ball so it will move quickly down to the ground on the opponent's court) to return the ball over the net. The team with possession of the ball that is trying to attack the ball as described is said to be on offense.The team on defense attempts to prevent the attacker from directing the ball into their court: players at the net jump and reach above the top (and if possible, across the plane) of the net to block the attacked ball. If the ball is hit around, above, or through the block, the defensive players arranged in the rest of the court attempt to control the ball with a dig (usually a fore-arm pass of a hard-driven ball). After a successful dig, the team transitions to offense.The game continues in this manner, rallying back and forth, until the ball touches the court within the boundaries or until an error is made. The most frequent errors that are made are either to fail to return the ball over the net within the allowed three touches, or to cause the ball to land outside the court. A ball is "in" if any part of it touches a sideline or end-line, and a strong spike may compress the ball enough when it lands that a ball which at first appears to be going out may actually be in. Players may travel well outside the court to play a ball that has gone over a sideline or end-line in the air.Other common errors include a player touching the ball twice in succession, a player "catching" the ball, a player touching the net while attempting to play the ball, or a player penetrating under the net into the opponent's court. There are a large number of other errors specified in the rules, although most of them are infrequent occurrences. These errors include back-row or libero players spiking the ball or blocking (back-row players may spike the ball if they jump from behind the attack line), players not being in the correct position when the ball is served, attacking the serve in the front court and above the height of the net, using another player as a source of support to reach the ball, stepping over the back boundary line when serving, taking more than 8 seconds to serve, or playing the ball when it is above the opponent's court. Scoring When the ball contacts the floor within the court boundaries or an error is made, the team that did not make the error is awarded a point, whether they served the ball or not. If the ball hits the line, the ball is counted as in. The team that won the point serves for the next point. If the team that won the point served in the previous point, the same player serves again. If the team that won the point did not serve the previous point, the players of the serving team rotate their position on the court in a clockwise manner. The game continues, with the first team to score 25 points by a two-point margin is awarded the set. Matches are best-of-five sets and the fifth set, if necessary, is usually played to 15 points. (Scoring differs between leagues, tournaments, and levels; high schools sometimes play best-of-three to 25; in the NCAA matches are played best-of-five to 25 as of the 2008 season.)Before 1999, points could be scored only when a team had the serve (side-out scoring) and all sets went up to only 15 points. The FIVB changed the rules in 1999 (with the changes being compulsory in 2000) to use the current scoring system (formerly known as rally point system), primarily to make the length of the match more predictable and to make the game more spectator- and television-friendly.The final year of side-out scoring at the NCAA Division I Women's Volleyball Championship was 2000. Rally point scoring debuted in 2001, and games were played to 30 points through 2007. For the 2008 season, games were renamed "sets" and reduced to 25 points to win. Most high schools in the U.S. changed to rally scoring in 2003, and several states implemented it the previous year on an experimental basis. Libero The libero player was introduced internationally in 1998, and made its debut for NCAA competition in 2002. The libero is a player specialized in defensive skills: the libero must wear a contrasting jersey color from his or her teammates and cannot block or attack the ball when it is entirely above net height. When the ball is not in play, the libero can replace any back-row player, without prior notice to the officials. This replacement does not count against the substitution limit each team is allowed per set, although the libero may be replaced only by the player whom he or she replaced. Most U.S. high schools added the libero position from 2003 to 2005.The libero may function as a setter only under certain restrictions. If she/he makes an overhand set, she/he must be standing behind (and not stepping on) the 3-meter line; otherwise, the ball cannot be attacked above the net in front of the 3-meter line. An underhand pass is allowed from any part of the court.The libero is, generally, the most skilled defensive player on the team. There is also a libero tracking sheet, where the referees or officiating team must keep track of whom the libero subs in and out for. There may only be one libero per set (game), although there may be a different libero in the beginning of any new set (game).Furthermore, a libero is not allowed to serve, according to international rules, with the exception of the NCAA women's volleyball games, where a 2004 rule change allows the libero to serve, but only in a specific rotation. That is, the libero can only serve for one person, not for all of the people for whom she goes in. That rule change was also applied to high school and junior high play soon after. Recent rule changes Other rule changes enacted in 2000 include allowing serves in which the ball touches the net, as long as it goes over the net into the opponents' court. Also, the service area was expanded to allow players to serve from anywhere behind the end line but still within the theoretical extension of the sidelines. Other changes were made to lighten up calls on faults for carries and double-touches, such as allowing multiple contacts by a single player ("double-hits") on a team's first contact provided that they are a part of a single play on the ball.In 2008, the NCAA changed the minimum number of points needed to win any of the first four sets from 30 to 25 for women's volleyball (men's volleyball remained at 30.) If a fifth (deciding) set is reached, the minimum required score remains at 15. In addition, the word "game" is now referred to as "set".Changes in rules have been studied and announced by the FIVB in recent years, and they have released the updated rules in 2009. Skills Competitive teams master six basic skills: serve, pass, set, attack, block and dig. Each of these skills comprises a number of specific techniques that have been introduced over the years and are now considered standard practice in high-level volleyball. Serve A player stands behind the inline and serves the ball, in an attempt to drive it into the opponent's court. The main objective is to make it land inside the court; it is also desirable to set the ball's direction, speed and acceleration so that it becomes difficult for the receiver to handle it properly. A serve is called an "ace" when the ball lands directly onto the court or travels outside the court after being touched by an opponent.In contemporary volleyball, many types of serves are employed:Underhand: a serve in which the player strikes the ball below the waist instead of tossing it up and striking it with an overhand throwing motion. Underhand serves are considered very easy to receive and are rarely employed in high-level competitions.Sky ball serve: a specific type of underhand serve occasionally used in beach volleyball, where the ball is hit so high it comes down almost in a straight line. This serve was invented and employed almost exclusively by the Brazilian team in the early 1980s and is now considered outdated. During the 2016 Olympic Games in Rio de Janeiro, however, the sky ball serve was extensively played by Italian beach volleyball player Adrian Carambula. In Brazil, this serve is called Jornada nas Estrelas (Star Trek)Topspin: an overhand serve where the player tosses the ball high and hits it with a wrist span, giving it topspin which causes it to drop faster than it would otherwise and helps maintain a straight flight path. Topspin serves are generally hit hard and aimed at a specific returner or part of the court. Standing topspin serves are rarely used above the high school level of play.Float: an overhand serve where the ball is hit with no spin so that its path becomes unpredictable, akin to a knuckleball in baseball.Jump serve: an overhand serve where the ball is first tossed high in the air, then the player makes a timed approach and jumps to make contact with the ball, hitting it with much pace and topspin. This is the most popular serve amongst college and professional teams.Jump float: an overhand serve where the ball is tossed high enough that the player may jump before hitting it similarly to a standing float serve. The ball is tossed lower than a topspin jump serve, but contact is still made while in the air. This serve is becoming more popular amongst college and professional players because it has a certain unpredictability in its flight pattern. It is the only serve where the server's feet can go over the inline. Pass Also called reception, the pass is the attempt by a team to properly handle the opponent's serve, or any form of attack. Proper handling includes not only preventing the ball from touching the court, but also making it reach the position where the setter is standing quickly and precisely.The skill of passing involves fundamentally two specific techniques: underarm pass, or bump, where the ball touches the inside part of the joined forearms or platform, at waist line; and overhand pass, where it is handled with the fingertips, like a set, above the head. Either are acceptable in professional and beach volleyball, however there are much tighter regulations on the overhand pass in beach volleyball. Set The set is usually the second contact that a team makes with the ball. The main goal of setting is to put the ball in the air in such a way that it can be driven by an attack into the opponent's court. The setter coordinates the offensive movements of a team, and is the player who ultimately decides which player will actually attack the ball.As with passing, one may distinguish between an overhand and a bump set. Since the former allows for more control over the speed and direction of the ball, the bump is used only when the ball is so low it cannot be properly handled with fingertips, or in beach volleyball where rules regulating overhand setting are more stringent. In the case of a set, one also speaks of a front or back set, meaning whether the ball is passed in the direction the setter is facing or behind the setter. There is also a jump set that is used when the ball is too close to the net. In this case the setter usually jumps off his or her right foot straight up to avoid going into the net. The setter usually stands about ⅔ of the way from the left to the right of the net and faces the left (the larger portion of net that he or she can see).Sometimes a setter refrains from raising the ball for a teammate to perform an attack and tries to play it directly onto the opponent's court. This movement is called a "dump". This can only be performed when the setter is in the front row, otherwise it constitutes an illegal back court attack. The most common dumps are to 'throw' the ball behind the setter or in front of the setter to zones 2 and 4. More experienced setters toss the ball into the deep corners or spike the ball on the second hit.As with a set or an overhand pass, the setter/passer must be careful to touch the ball with both hands at the same time. If one hand is noticeably late to touch the ball this could result in a less effective set, as well as the referee calling a 'double hit' and giving the point to the opposing team. Attack The attack, also known as the spike, is usually the third contact a team makes with the ball. The object of attacking is to handle the ball so that it lands on the opponent's court and cannot be defended. A player makes a series of steps (the "approach"), jumps, and swings at the ball.Ideally the contact with the ball is made at the apex of the hitter's jump. At the moment of contact, the hitter's arm is fully extended above his or her head and slightly forward, making the highest possible contact while maintaining the ability to deliver a powerful hit. The hitter uses arm swing, wrist snap, and a rapid forward contraction of the entire body to drive the ball. A 'bounce' is a slang term for a very hard/loud spike that follows an almost straight trajectory steeply downward into the opponent's court and bounces very high into the air. A "kill" is the slang term for an attack that is not returned by the other team thus resulting in a point.Contemporary volleyball comprises a number of attacking techniques:Backcourt (or backrow)/pipe attack: an attack performed by a back row player. The player must jump from behind the 3-meter line before making contact with the ball, but may land in front of the 3-meter line.Line and Cross-court Shot: refers to whether the ball flies in a straight trajectory parallel to the side lines, or crosses through the court in an angle. A cross-court shot with a very pronounced angle, resulting in the ball landing near the 3-meter line, is called a cut shot.Dip/Dink/Tip/Cheat/Dump: the player does not try to make a hit, but touches the ball lightly, so that it lands on an area of the opponent's court that is not being covered by the defense.Tool/Wipe/Block-abuse: the player does not try to make a hard spike, but hits the ball so that it touches the opponent's block and then bounces off-court.Off-speed hit: the player does not hit the ball hard, reducing its speed and thus confusing the opponent's defense.Quick hit/"One": an attack (usually by the middle blocker) where the approach and jump begin before the setter contacts the ball. The set (called a "quick set") is placed only slightly above the net and the ball is struck by the hitter almost immediately after leaving the setter's hands. Quick attacks are often effective because they isolate the middle blocker to be the only blocker on the hit.Slide: a variation of the quick hit that uses a low back set. The middle hitter steps around the setter and hits from behind him or her.Double quick hit/"Stack"/"Tandem": a variation of quick hit where two hitters, one in front and one behind the setter or both in front of the setter, jump to perform a quick hit at the same time. It can be used to deceive opposite blockers and free a fourth hitter attacking from back-court, maybe without block at all. Block Blocking refers to the actions taken by players standing at the net to stop or alter an opponent's attack.A block that is aimed at completely stopping an attack, thus making the ball remain in the opponent's court, is called offensive. A well-executed offensive block is performed by jumping and reaching to penetrate with one's arms and hands over the net and into the opponent's area. It requires anticipating the direction the ball will go once the attack takes place. It may also require calculating the best foot work to executing the "perfect" block.The jump should be timed so as to intercept the ball's trajectory prior to it crossing over the net. Palms are held deflected downward about 45–60 degrees toward the interior of the opponents court. A "roof" is a spectacular offensive block that redirects the power and speed of the attack straight down to the attacker's floor, as if the attacker hit the ball into the underside of a peaked house roof.By contrast, it is called a defensive, or "soft" block if the goal is to control and deflect the hard-driven ball up so that it slows down and becomes easier to defend. A well-executed soft-block is performed by jumping and placing one's hands above the net with no penetration into the opponent's court and with the palms up and fingers pointing backward.Blocking is also classified according to the number of players involved. Thus, one may speak of single (or solo), double, or triple block.Successful blocking does not always result in a "roof" and many times does not even touch the ball. While it's obvious that a block was a success when the attacker is roofed, a block that consistently forces the attacker away from his or her 'power' or preferred attack into a more easily controlled shot by the defense is also a highly successful block.At the same time, the block position influences the positions where other defenders place themselves while opponent hitters are spiking. Dig Digging is the ability to prevent the ball from touching one's court after a spike or attack, particularly a ball that is nearly touching the ground. In many aspects, this skill is similar to passing, or bumping: overhand dig and bump are also used to distinguish between defensive actions taken with fingertips or with joined arms. It varies from passing however in that is it a much more reflex based skill, especially at the higher levels. It is especially important while digging for players to stay on their toes; several players choose to employ a split step to make sure they're ready to move in any direction.Some specific techniques are more common in digging than in passing. A player may sometimes perform a "dive", i.e., throw his or her body in the air with a forward movement in an attempt to save the ball, and land on his or her chest. When the player also slides his or her hand under a ball that is almost touching the court, this is called a "pancake". The pancake is frequently used in indoor volleyball, but rarely if ever in beach volleyball because the uneven and yielding nature of the sand court limits the chances that the ball will make a good, clean contact with the hand. When used correctly, it is one of the more spectacular defensive volleyball plays.Sometimes a player may also be forced to drop his or her body quickly to the floor to save the ball. In this situation, the player makes use of a specific rolling technique to minimize the chances of injuries. Team play Volleyball is essentially a game of transition from one of the above skills to the next, with choreographed team movement between plays on the ball. These team movements are determined by the teams chosen serve receive system, offensive system, coverage system, and defensive system.The serve receive system is the formation used by the receiving team to attempt to pass the ball to the designated setter. Systems can consist of 5 receivers, 4 receivers, 3 receivers, and in some cases 2 receivers. The most popular formation at higher levels is a 3 receiver formation consisting of two left sides and a libero receiving every rotation. This allows middles and right sides to become more specialized at hitting and blocking.Offensive systems are the formations used by the offense to attempt to ground the ball into the opposing court (or otherwise score points). Formations often include designated player positions with skill specialization (see Player specialization, below). Popular formations include the 4-2, 6-2, and 5-1 systems (see Formations, below). There are also several different attacking schemes teams can use to keep the opposing defense off balance.Coverage systems are the formations used by the offense to protect their court in the case of a blocked attack. Executed by the 5 offensive players not directly attacking the ball, players move to assigned positions around the attacker to dig up any ball that deflects off the block back into their own court. Popular formations include the 2-3 system and the 1-2-2 system. In lieu of a system, some teams just use a random coverage with the players nearest the hitter.Defensive systems are the formations used by the defense to protect against the ball being grounded into their court by the opposing team. The system will outline which players are responsible for which areas of the court depending on where the opposing team is attacking from. Popular systems include the 6-Up, 6-Back-Deep, and 6-Back-Slide defense. There are also several different blocking schemes teams can employ to disrupt the opposing teams offense.Some teams, when they are ready to serve, will line up their other five players in a screen to obscure the view of the receiving team. This action is only illegal if the server makes use of the screen, so the call is made at the referees discretion as to the impact the screen made on the receivers ability to pass the ball. The most common style of screening involves a W formation designed to take up as much horizontal space as possible. Coaching  Basic Coaching for volleyball can be classified under two main categories: match coaching and developmental coaching. The objective of match coaching is to win a match by managing a team's strategy. Developmental coaching emphasizes player development through the reinforcement of basic skills during exercises known as "drills." Drills promote repetition and refinement of volleyball movements, particularly in footwork patterns, body positioning relative to others, and ball contact. A coach will construct drills that simulate match situations thereby encouraging speed of movement, anticipation, timing, communication, and team-work. At the various stages of a player's career, a coach will tailor drills to meet the strategic requirements of the team. The American Volleyball Coaches Association is the largest organization in the world dedicated exclusively to volleyball coaching. Strategy  Player specialization There are 5 positions filled on every volleyball team at the elite level. Setter, Outside Hitter/Left Side Hitter, Middle Hitter, Opposite Hitter/Right Side Hitter and Libero/Defensive Specialist. Each of these positions plays a specific, key role in winning a volleyball match.Setters have the task for orchestrating the offense of the team. They aim for second touch and their main responsibility is to place the ball in the air where the attackers can place the ball into the opponents' court for a point. They have to be able to operate with the hitters, manage the tempo of their side of the court and choose the right attackers to set. Setters need to have swift and skillful appraisal and tactical accuracy, and must be quick at moving around the court.Liberos are defensive players who are responsible for receiving the attack or serve. They are usually the players on the court with the quickest reaction time and best passing skills. Libero means 'free' in Italian—they receive this name as they have the ability to substitute for any other player on the court during each play. They do not necessarily need to be tall, as they never play at the net, which allows shorter players with strong passing and defensive skills to excel in the position and play an important role in the team's success. A player designated as a libero for a match may not play other roles during that match. Liberos wear a different color jersey than their teammates.Middle blockers or Middle hitters are players that can perform very fast attacks that usually take place near the setter. They are specialized in blocking, since they must attempt to stop equally fast plays from their opponents and then quickly set up a double block at the sides of the court. In non-beginners play, every team will have two middle hitters.Outside hitters or Left side hitters attack from near the left antenna. The outside hitter is usually the most consistent hitter on the team and gets the most sets. Inaccurate first passes usually result in a set to the outside hitter rather than middle or opposite. Since most sets to the outside are high, the outside hitter may take a longer approach, always starting from outside the court sideline. In non-beginners play, there are again two outside hitters on every team in every match.Opposite hitters or Right-side hitters carry the defensive workload for a volleyball team in the front row. Their primary responsibilities are to put up a well formed block against the opponents' Outside Hitters and serve as a backup setter. Sets to the opposite usually go to the right side of the antennae.At some levels where substitutions are unlimited, teams will make use of a Defensive Specialist in place of or in addition to a Libero. This position does not have unique rules like the libero position, instead, these players are used to substitute out a poor back row defender using regular substitution rules. A defensive specialist is often used if you have a particularly poor back court defender in right side or left side, but your team is already using a libero to take out your middles. Most often, the situation involves a team using a right side player with a big block who must be subbed out in the back row because they aren't able to effectively play back court defense. Similarly, teams might use a Serving Specialist to sub out a poor server situationally. Formations The three standard volleyball formations are known as "4–2", "6–2" and "5–1", which refers to the number of hitters and setters respectively. 4–2 is a basic formation used only in beginners' play, while 5–1 is by far the most common formation in high-level play. 4–2 The 4–2 formation has four hitters and two setters. The setters usually set from the middle front or right front position. The team will therefore have two front-row attackers at all times. In the international 4–2, the setters set from the right front position. The international 4–2 translates more easily into other forms of offense.The setters line up opposite each other in the rotation. The typical lineup has two outside hitters. By aligning like positions opposite themselves in the rotation, there will always be one of each position in the front and back rows. After service, the players in the front row move into their assigned positions, so that the setter is always in middle front. Alternatively, the setter moves into the right front and has both a middle and an outside attacker; the disadvantage here lies in the lack of an offside hitter, allowing one of the other team's blockers to "cheat in" on a middle block.The clear disadvantage to this offensive formation is that there are only two attackers, leaving a team with fewer offensive weapons.Another aspect is to see the setter as an attacking force, albeit a weakened force, because when the setter is in the front court they are able to 'tip' or 'dump', so when the ball is close to the net on the second touch, the setter may opt to hit the ball over with one hand. This means that the blocker who would otherwise not have to block the setter is engaged and may allow one of the hitters to have an easier attack. 6–2 In the 6–2 formation, a player always comes forward from the back row to set. The three front row players are all in attacking positions. Thus, all six players act as hitters at one time or another, while two can act as setters. So the 6–2 formation is actually a 4–2 system, but the back-row setter penetrates to set.The 6–2 lineup thus requires two setters, who line up opposite to each other in the rotation. In addition to the setters, a typical lineup will have two middle hitters and two outside hitters. By aligning like positions opposite themselves in the rotation, there will always be one of each position in the front and back rows. After service, the players in the front row move into their assigned positions.The advantage of the 6–2 is that there are always three front-row hitters available, maximizing the offensive possibilities. However, not only does the 6–2 require a team to possess two people capable of performing the highly specialized role of setter, it also requires both of those players to be effective offensive hitters when not in the setter position. At the international level, only the Cuban National Women's Team employs this kind of formation. It is also used in Women's NCAA play, partially due to the variant rules used which allow 12 substitutions per set (as opposed to the 6 allowed in the standard rules). 5–1 The 5–1 formation has only one player who assumes setting responsibilities regardless of his or her position in the rotation. The team will therefore have three front-row attackers when the setter is in the back row, and only two when the setter is in the front row, for a total of five possible attackers.The player opposite the setter in a 5–1 rotation is called the opposite hitter. In general, opposite hitters do not pass; they stand behind their teammates when the opponent is serving. The opposite hitter may be used as a third attack option (back-row attack) when the setter is in the front row: this is the normal option used to increase the attack capabilities of modern volleyball teams. Normally the opposite hitter is the most technical skilled hitter of the team. Back-row attacks generally come from the back-right position, known as zone 1, but are increasingly performed from back-center in high-level play.The big advantage of this system is that the setter always has 3 hitters to vary sets with. If the setter does this well, the opponent's middle blocker may not have enough time to block with the outside blocker, increasing the chance for the attacking team to make a point.There is another advantage, the same as that of a 4–2 formation: when the setter is a front-row player, he or she is allowed to jump and "dump" the ball onto the opponent's side. This too can confuse the opponent's blocking players: the setter can jump and dump or can set to one of the hitters. A good setter knows this and thus won't only jump to dump or to set for a quick hit, but when setting outside as well to confuse the opponent.The 5–1 offense is actually a mix of 6–2 and 4–2: when the setter is in the front row, the offense looks like a 4–2; when the setter is in the back row, the offense looks like a 6–2. Variations and related games There are many variations on the basic rules of volleyball. By far the most popular of these is beach volleyball, which is played on sand with two people per team, and rivals the main sport in popularity.Some games related to volleyball include:Beachball volleyball: A game of indoor volleyball played with a beach ball instead of a volleyball.Biribol: an aquatic variant, played in shallow swimming pools. The name comes from the Brazilian city where it was invented, Birigui.Ecua-volley: A variant invented in Ecuador, with some significant variants, such as number of players, and a heavier ball.Footvolley: A sport from Brazil in which the hands and arms are not used but most else is like beach volleyball.Hooverball: Popularized by President Herbert Hoover, it is played with a volleyball net and a medicine ball; it is scored like tennis, but the ball is caught and then thrown back. The weight of the medicine ball can make the sport to be quite physically demanding; annual championship tournaments are held annually in West Branch, Iowa.Newcomb ball (sometimes spelled "Nuke 'Em"): In this game, the ball is caught and thrown instead of hit; it rivaled volleyball in popularity until the 1920s.Prisoner Ball: Also played with volleyball court and a volleyball, prisoner ball is a variation of Newcomb ball where players are "taken prisoner" or released from "prison" instead of scoring points. Usually played by young children.Sepak Takraw: Played in Southeast Asia using a rattan ball and allowing only players' feet, knees, chest, and head to touch the ball.Throwball: became popular with women players at the YMCA College of Physical Education in Chennai (India) in the 1940s.Towel volleyball: towel volleyball is a popular entertainment outdoors. The game takes place in volleyball court, forming pairs that hold in hands towels and trying to throw the ball im the opponent's field. You can also play with blankets, held by four people. There may be some variations.Wallyball: A variation of volleyball played in a racquetball court with a rubber ball. See also List of volleyball playersList of volleyball video gamesVolley squashVolleyball Hall of FameVolleyball jargonVolleyball injuries Notes  External links Fédération Internationale de Volleyball – FIVBUSA VolleyballAVPAmerican Volleyball Coaches Association
Hockey is a family of sports in which two teams play against each other by trying to maneuver a ball or a puck into the opponent's goal using a hockey stick. In many areas, one sport (typically field hockey or ice hockey) is generally referred to simply as "hockey". Etymology The first recorded use of the word hockey is in the 1773 book Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled "New Improvements on the Game of Hockey". The belief that hockey was mentioned in a 1363 proclamation by King Edward III of England is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games "Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam". The English historian and biographer John Strype did not use the word "hockey" when he translated the proclamation in 1720.The word hockey itself is of unknown origin. One supposition is that it is a derivative of hoquet, a Middle French word for a shepherd's stave. The curved, or "hooked" ends of the sticks used for hockey would indeed have resembled these staves. Another supposition derives from the known use of cork bungs, (stoppers) in place of wooden balls to play the game. The stoppers came from barrels containing "hock" ale, also called "hocky". History Games played with curved sticks and a ball can be found in the histories of many cultures. In Egypt, 4000-year-old carvings feature teams with sticks and a projectile, hurling dates to before 1272 BC in Ireland, and there is a depiction from approximately 600 BC in Ancient Greece, where the game may have been called kerētízein or (κερητίζειν) because it was played with a horn or horn-like stick (kéras, κέρας). In Inner Mongolia, the Daur people have been playing beikou, a game similar to modern field hockey, for about 1,000 years.Most evidence of hockey-like games during the Middle Ages is found in legislation concerning sports and games. The Galway Statute enacted in Ireland in 1527 banned certain types of ball games, including games using "hooked" (written "hockie", similar to "hooky") sticks....at no tyme to use ne occupye the horlinge of the litill balle with hockie stickes or staves, nor use no hande ball to play withoute walles, but only greate foote balleBy the 19th century, the various forms and divisions of historic games began to differentiate and coalesce into the individual sports defined today. Organizations dedicated to the codification of rules and regulations began to form, and national and international bodies sprang up to manage domestic and international competition. Subtypes  Bandy Bandy is played with a ball on a football field-sized ice arena (bandy rink), typically outdoors, and with many rules similar to association football. It is played professionally in Russia and Sweden and is considered a national sport in Russia. The sport is recognised by the IOC; its international governing body is the Federation of International Bandy.Bandy has its roots in England in the 19th century, was originally called "hockey on the ice", and spread from England to other European countries around 1900; a similar Russian sport can also be seen as a predecessor and in Russia, bandy is sometimes called "Russian hockey". Bandy World Championships have been played since 1957 and Women's Bandy World Championships since 2004. There are national club championships in many countries and the top clubs in the world play in the Bandy World Cup every year. Field hockey Field hockey is played on gravel, natural grass, or sand-based or water-based artificial turf, with a small, hard ball approximately 73 mm (2.9 in) in diameter. The game is popular among both males and females in many parts of the world, particularly in Europe, Asia, Australia, New Zealand, South Africa, and Argentina. In most countries, the game is played between single-sex sides, although they can be mixed-sex.The governing body is the 126-member International Hockey Federation (FIH). Men's field hockey has been played at each Summer Olympic Games since 1908 except for 1912 and 1924, while women's field hockey has been played at the Summer Olympic Games since 1980.Modern field hockey sticks are constructed of a composite of wood, glass fibre or carbon fibre (sometimes both) and are J-shaped, with a curved hook at the playing end, a flat surface on the playing side and a curved surface on the rear side. All sticks are right-handed – left-handed sticks are not permitted.While field hockey in its current form appeared in mid-18th century England, primarily in schools, it was not until the first half of the 19th century that it became firmly established. The first club was created in 1849 at Blackheath in south-east London. Field hockey is the national sport of Pakistan. It was the national sport of India until the Ministry of Youth Affairs and Sports declared in August 2012 that India has no national sport. Ice hockey Ice hockey is played between two teams of skaters on a large flat area of ice, using a three-inch-diameter (76.2 mm) vulcanized rubber disc called a puck. This puck is often frozen before high-level games to decrease the amount of bouncing and friction on the ice. The game is played all over North America, Europe and to varying extents in many other countries around the world. It is the most popular sport in Canada, Finland, Latvia, the Czech Republic, and Slovakia. Ice hockey is the national sport of Latvia and the national winter sport of Canada. Ice hockey is played at a number of levels, by all ages.The governing body of international play is the 77-member International Ice Hockey Federation (IIHF). Men's ice hockey has been played at the Winter Olympics since 1924, and was in the 1920 Summer Olympics. Women's ice hockey was added to the Winter Olympics in 1998. North America's National Hockey League (NHL) is the strongest professional ice hockey league, drawing top ice hockey players from around the globe. The NHL rules are slightly different from those used in Olympic ice hockey over many categories. International ice hockey rules were adopted from Canadian rules in the early 1900s.The contemporary sport developed in Canada from European and native influences. These included various stick and ball games similar to field hockey, bandy and other games where two teams push a ball or object back and forth with sticks. These were played outdoors on ice under the name "hockey" in England throughout the 19th century, and even earlier under various other names. In Canada, there are 24 reports of hockey-like games in the 19th century before 1875 (five of them using the name "hockey"). The first organized and recorded game of ice hockey was played indoors in Montreal, Quebec, Canada, on March 3, 1875, and featured several McGill University students.Ice hockey sticks are long L-shaped sticks made of wood, graphite, or composites with a blade at the bottom that can lie flat on the playing surface when the stick is held upright and can legally curve either way, for left- or right-handed players. Roller hockey (inline) Inline hockey is a variation of roller hockey very similar to ice hockey, from which it is derived. Inline hockey is played by two teams, consisting of four skaters and one goalie, on a dry rink divided into two halves by a center line, with one net at each end of the rink. The game is played in three 15-minute periods with a variation of the ice hockey off-side rule. Icings are also called, but are usually referred to as illegal clearing. The governing body is the IIHF, as for ice hockey, but some leagues and competitions do not follow the IIHF regulations, in particular USA Inline and Canada Inline. Roller hockey (quad) Roller hockey, also known as quad hockey, international-style ball hockey, and Hoquei em Patins, is an overarching name for a roller sport that has existed since long before inline skates were invented. This sport is played in over sixty countries and has a worldwide following. Roller hockey was a demonstration sport at the 1992 Barcelona Summer Olympics. Sledge hockey Sledge hockey is a form of ice hockey designed for players with physical disabilities affecting their lower bodies. Players sit on double-bladed sledges and use two sticks; each stick has a blade at one end and small picks at the other. Players use the sticks to pass, stickhandle and shoot the puck, and to propel their sledges. The rules are very similar to IIHF ice hockey rules.Canada is a recognized international leader in the development of sledge hockey, and much of the equipment for the sport was first developed there, such as sledge hockey sticks laminated with fiberglass, as well as aluminum shafts with hand-carved insert blades and special aluminum sledges with regulation skate blades.Based on ice sledge hockey, inline sledge hockey is played to the same rules as inline puck hockey (essentially ice hockey played off-ice using inline skates). There is no classification point system dictating who can play inline sledge hockey, unlike the situation with other team sports such as wheelchair basketball and wheelchair rugby. Inline sledge hockey is being developed to allow everyone, regardless of whether they have a disability or not, to complete up to world championship level based solely on talent and ability. The first game of inline sledge hockey was played at Bisley, England, on 19 December 2009 between the Hull Stingrays and the Grimsby Redwings. Matt Lloyd is credited with inventing inline sledge hockey, and Great Britain is seen as the international leader in the game's development. Street hockey Also known as road hockey, this is a dry-land variant of ice and roller hockey played year-round on a hard surface (usually asphalt). A ball is usually used instead of a puck, and protective equipment is not usually worn. Other forms of hockey Other games derived from hockey or its predecessors include the following:Air hockey is played indoors with a puck on an air-cushion table.Beach hockey, a variation of street hockey, is a common sight on Southern California beaches.Ball hockey is played in a gym using sticks and a ball, often a tennis ball with the felt removed.Box hockey is a schoolyard game played by two people. The object of the game is to move a hockey puck from the center of the box out through a hole placed at the end of the box (known as the goal). The players kneel facing one another on either side of the box, and each attempts to move the puck to the hole on their left.Broomball is played on an ice hockey rink, but with a ball instead of a puck and a "broom" (actually a stick with a small plastic implement on the end) in place of the ice hockey stick. Instead of skates, special shoes are used that have very soft rubbery soles to maximize grip while running around.Deck hockey is traditionally played by the Royal Navy on ships' decks, using short wooden L-shaped sticks.Floor hockey is a form of hockey played on foot, on a flat, smooth floor surface, usually indoors in gymnasiums or similar spaces.Floorball is a form of hockey played in a gymnasium or in a sports hall. A whiffle ball is used instead of a plastic ball, and the sticks are only one meter long and made from composite materials.Foot hockey or sock hockey is played using a bald tennis ball or rolled-up pair of socks and using only the feet. It is popular in elementary schools in the winter.Gym hockey is a form of ice hockey played in a gymnasium. It uses sticks with foam ends and a foam ball or a plastic puck.Hurling and Camogie are Irish games bearing some resemblance to – and notable differences from – hockey.Indoor field hockey is an indoor variation of field hockey.Mini hockey (or knee-hockey), also known as "mini-sticks" is a form of hockey played in the United States in the basements of houses. Players kneel and use a miniature plastic stick, usually about 15 inches (38 cm) long, to maneuver a small ball or a soft, fabric-covered mini puck into miniature goals. In England 'mini hockey' refers to a seven-a-side version of field hockey for younger players, played on an area equivalent to half a normal pitch.Nok Hockey is a table-top version of hockey played with no defense and a small block in front of the goal.Pond hockey is a simplified form of ice hockey played on naturally frozen ice.Power hockey is a form of hockey for persons requiring the use of an electric (power) wheelchair in daily life.Ringette is an ice hockey variant that was designed for female players; it uses a straight stick and a rubber ring in place of a puck. The rules differ from those of hockey and resemble a mix of lacrosse and basketball.Rink bandy and Rinkball are team sports of Scandinavian origin that are played like bandy but on an ice hockey rink and with fewer players on each team.Rossall hockey is a variation played at Rossall School on the sea shore in the winter months. Its rules are a mix of field hockey, rugby and the Eton wall game.Shinny is an informal version of ice hockey.Shinty is a Scottish game now played primarily in the HighlandsSkater hockey is a variant of inline hockey, played with a ball.Spongee is a cross between ice hockey and broomball and is most popular in Manitoba, Canada. A stick and puck are used as in hockey (the puck is a softer version called a "sponge puck"), and the same soft-soled shoes are worn as in broomball. The rules are basically the same as for ice hockey, but one variation has an extra player on the ice called a "rover".Table hockey is played indoors on a table.Underwater hockey is played on the bottom of a swimming pool.Unicycle hockey is played on a hard surface using unicycles as the method of player movement. There is generally no dedicated goalkeeper. Equipment Shoulder padsJockstrap with cup pocket and protective cupHockey stickPuck or ball See also The Ultimate Book of Hockey Trivia for Kids References Gidén, Carl; Houda, Patrick; Martel, Jean-Patrice (2014). On the Origin of Hockey. Createspace. ISBN 9780993799808. Podnieks, Andrew; Szemberg, Szymon (2007). World of hockey : celebrating a century of the IIHF. Fenn Publishing. ISBN 9781551683072.  External links 
A fuel is any material that can be made to react with other substances so that it releases chemical or nuclear energy as heat or to be used for work. The concept was originally applied solely to those materials capable of releasing chemical energy but has since also been applied to other sources of heat energy such as nuclear energy (via nuclear fission and nuclear fusion).The heat energy released by reactions of fuels is converted into mechanical energy via a heat engine. Other times the heat itself is valued for warmth, cooking, or industrial processes, as well as the illumination that comes with combustion. Fuels are also used in the cells of organisms in a process known as cellular respiration, where organic molecules are oxidized to release usable energy. Hydrocarbons and related oxygen-containing molecules are by far the most common source of fuel used by humans, but other substances, including radioactive metals, are also utilized.Fuels are contrasted with other substances or devices storing potential energy, such as those that directly release electrical energy (such as batteries and capacitors) or mechanical energy (such as flywheels, springs, compressed air, or water in a reservoir). History The first known use of fuel was the combustion of wood or sticks by Homo erectus near 2,000,000 (two million) years ago. Throughout most of human history fuels derived from plants or animal fat were only used by humans. Charcoal, a wood derivative, has been used since at least 6,000 BCE for melting metals. It was only supplanted by coke, derived from coal, as European forests started to become depleted around the 18th century. Charcoal briquettes are now commonly used as a fuel for barbecue cooking.Coal was first used as a fuel around 1000 BCE in China. With the energy in the form of chemical energy that could be released through combustion, but the concept development of the steam engine in the United Kingdom in 1769, coal came into more common use as a power source. Coal was later used to drive ships and locomotives. By the 19th century, gas extracted from coal was being used for street lighting in London. In the 20th and 21st centuries, the primary use of coal is to generate electricity, providing 40% of the world's electrical power supply in 2005.Fossil fuels were rapidly adopted during the industrial revolution, because they were more concentrated and flexible than traditional energy sources, such as water power. They have become a pivotal part of our contemporary society, with most countries in the world burning fossil fuels in order to produce power.Currently the trend has been towards renewable fuels, such as biofuels like alcohols. Chemical Chemical fuels are substances that release energy by reacting with substances around them, most notably by the process of combustion. Most of the chemical energy released in combustion was not stored in the chemical bonds of the fuel, but in the weak double bond of molecular oxygen.Chemical fuels are divided in two ways. First, by their physical properties, as a solid, liquid or gas. Secondly, on the basis of their occurrence: primary (natural fuel) and secondary (artificial fuel). Thus, a general classification of chemical fuels is: Solid fuel Solid fuel refers to various types of solid material that are used as fuel to produce energy and provide heating, usually released through combustion. Solid fuels include wood (see wood fuel), charcoal, peat, coal, Hexamine fuel tablets, and pellets made from wood (see wood pellets), corn, wheat, rye and other grains. Solid-fuel rocket technology also uses solid fuel (see solid propellants). Solid fuels have been used by humanity for many years to create fire. Coal was the fuel source which enabled the industrial revolution, from firing furnaces, to running steam engines. Wood was also extensively used to run steam locomotives. Both peat and coal are still used in electricity generation today. The use of some solid fuels (e.g. coal) is restricted or prohibited in some urban areas, due to unsafe levels of toxic emissions. The use of other solid fuels such as wood is decreasing as heating technology and the availability of good quality fuel improves. In some areas, smokeless coal is often the only solid fuel used. In Ireland, peat briquettes are used as smokeless fuel. They are also used to start a coal fire. Liquid fuels Liquid fuels are combustible or energy-generating molecules that can be harnessed to create mechanical energy, usually producing kinetic energy; they also must take the shape of their container. It is the fumes of liquid fuels that are flammable instead of the fluid.Most liquid fuels in widespread use are derived from the fossilized remains of dead plants and animals by exposure to heat and pressure in the Earth's crust. However, there are several types, such as hydrogen fuel (for automotive uses), ethanol, jet fuel and biodiesel which are all categorized as a liquid fuel. Emulsified fuels of oil-in-water such as orimulsion have been developed a way to make heavy oil fractions usable as liquid fuels. Many liquid fuels play a primary role in transportation and the economy.Some common properties of liquid fuels are that they are easy to transport, and can be handled with relative ease. Also they are relatively easy to use for all engineering applications, and home use. Fuels like kerosene are rationed in some countries, for example available in government subsidized shops in India for home use.Conventional diesel is similar to gasoline in that it is a mixture of aliphatic hydrocarbons extracted from petroleum. Kerosene is used in kerosene lamps and as a fuel for cooking, heating, and small engines. Natural gas, composed chiefly of methane, can only exist as a liquid at very low temperatures (regardless of pressure), which limits its direct use as a liquid fuel in most applications. LP gas is a mixture of propane and butane, both of which are easily compressible gases under standard atmospheric conditions. It offers many of the advantages of compressed natural gas (CNG), but is denser than air, does not burn as cleanly, and is much more easily compressed. Commonly used for cooking and space heating, LP gas and compressed propane are seeing increased use in motorized vehicles; propane is the third most commonly used motor fuel globally. Gaseous fuels Fuel gas is any one of a number of fuels that under ordinary conditions are gaseous. Many fuel gases are composed of hydrocarbons (such as methane or propane), hydrogen, carbon monoxide, or mixtures thereof. Such gases are sources of potential heat energy or light energy that can be readily transmitted and distributed through pipes from the point of origin directly to the place of consumption. Fuel gas is contrasted with liquid fuels and from solid fuels, though some fuel gases are liquefied for storage or transport. While their gaseous nature can be advantageous, avoiding the difficulty of transporting solid fuel and the dangers of spillage inherent in liquid fuels, it can also be dangerous. It is possible for a fuel gas to be undetected and collect in certain areas, leading to the risk of a gas explosion. For this reason, odorizers are added to most fuel gases so that they may be detected by a distinct smell. The most common type of fuel gas in current use is natural gas. Biofuels Biofuel can be broadly defined as solid, liquid, or gas fuel consisting of, or derived from biomass. Biomass can also be used directly for heating or power—known as biomass fuel. Biofuel can be produced from any carbon source that can be replenished rapidly e.g. plants. Many different plants and plant-derived materials are used for biofuel manufacture.Perhaps the earliest fuel employed by humans is wood. Evidence shows controlled fire was used up to 1.5 million years ago at Swartkrans, South Africa. It is unknown which hominid species first used fire, as both Australopithecus and an early species of Homo were present at the sites. As a fuel, wood has remained in use up until the present day, although it has been superseded for many purposes by other sources. Wood has an energy density of 10–20 MJ/kg.Recently biofuels have been developed for use in automotive transport (for example Bioethanol and Biodiesel), but there is widespread public debate about how carbon efficient these fuels are. Fossil fuels Fossil fuels is hydrocarbons, primarily coal and petroleum (liquid petroleum or natural gas), formed from the fossilized remains of ancient plants and animals by exposure to high heat and pressure in the absence of oxygen in the Earth's crust over hundreds of millions of years. Commonly, the term fossil fuel also includes hydrocarbon-containing natural resources that are not derived entirely from biological sources, such as tar sands. These latter sources are properly known as mineral fuels.Fossil fuels contain high percentages of carbon and include coal, petroleum, and natural gas. They range from volatile materials with low carbon:hydrogen ratios like methane, to liquid petroleum to nonvolatile materials composed of almost pure carbon, like anthracite coal. Methane can be found in hydrocarbon fields, alone, associated with oil, or in the form of methane clathrates. Fossil fuels formed from the fossilized remains of dead plants by exposure to heat and pressure in the Earth's crust over millions of years. This biogenic theory was first introduced by German scholar Georg Agricola in 1556 and later by Mikhail Lomonosov in the 18th century.It was estimated by the Energy Information Administration that in 2007 primary sources of energy consisted of petroleum 36.0%, coal 27.4%, natural gas 23.0%, amounting to an 86.4% share for fossil fuels in primary energy consumption in the world. Non-fossil sources in 2006 included hydroelectric 6.3%, nuclear 8.5%, and others (geothermal, solar, tidal, wind, wood, waste) amounting to 0.9%. World energy consumption was growing about 2.3% per year.Fossil fuels are non-renewable resources because they take millions of years to form, and reserves are being depleted much faster than new ones are being made. So we must conserve these fuels and use them judiciously. The production and use of fossil fuels raise environmental concerns. A global movement toward the generation of renewable energy is therefore under way to help meet increased energy needs. The burning of fossil fuels produces around 21.3 billion tonnes (21.3 gigatonnes) of carbon dioxide (CO2) per year, but it is estimated that natural processes can only absorb about half of that amount, so there is a net increase of 10.65 billion tonnes of atmospheric carbon dioxide per year (one tonne of atmospheric carbon is equivalent to 44/12 or 3.7 tonnes of carbon dioxide). Carbon dioxide is one of the greenhouse gases that enhances radiative forcing and contributes to global warming, causing the average surface temperature of the Earth to rise in response, which the vast majority of climate scientists agree will cause major adverse effects. Fuels are a source of energy. Energy The amount of energy from different types of fuel depends on the stoichiometric ratio, the chemically correct air and fuel ratio to ensure complete combustion of fuel, and its Specific energy, the energy per unit mass.1 MJ ≈ 0.28 kWh ≈ 0.37 HPh. Nuclear Nuclear fuel is any material that is consumed to derive nuclear energy. Technically speaking, All matter can be a nuclear fuel because any element under the right conditions will release nuclear energy, but the materials commonly referred to as nuclear fuels are those that will produce energy without being placed under extreme duress. Nuclear fuel is a material that can be 'burned' by nuclear fission or fusion to derive nuclear energy. Nuclear fuel can refer to the fuel itself, or to physical objects (for example bundles composed of fuel rods) composed of the fuel material, mixed with structural, neutron moderating, or neutron reflecting materials.Most nuclear fuels contain heavy fissile elements that are capable of nuclear fission. When these fuels are struck by neutrons, they are in turn capable of emitting neutrons when they break apart. This makes possible a self-sustaining chain reaction that releases energy with a controlled rate in a nuclear reactor or with a very rapid uncontrolled rate in a nuclear weapon.The most common fissile nuclear fuels are uranium-235 (235U) and plutonium-239 (239Pu). The actions of mining, refining, purifying, using, and ultimately disposing of nuclear fuel together make up the nuclear fuel cycle. Not all types of nuclear fuels create power from nuclear fission. Plutonium-238 and some other elements are used to produce small amounts of nuclear power by radioactive decay in radioisotope thermoelectric generators and other types of atomic batteries. Also, light nuclides such as tritium (3H) can be used as fuel for nuclear fusion. Nuclear fuel has the highest energy density of all practical fuel sources. Fission The most common type of nuclear fuel used by humans is heavy fissile elements that can be made to undergo nuclear fission chain reactions in a nuclear fission reactor; nuclear fuel can refer to the material or to physical objects (for example fuel bundles composed of fuel rods) composed of the fuel material, perhaps mixed with structural, neutron moderating, or neutron reflecting materials. The most common fissile nuclear fuels are 235U and 239Pu, and the actions of mining, refining, purifying, using, and ultimately disposing of these elements together make up the nuclear fuel cycle, which is important for its relevance to nuclear power generation and nuclear weapons. Fusion Fuels that produce energy by the process of nuclear fusion are currently not utilized by humans but are the main source of fuel for stars. Fusion fuels tend to be light elements such as hydrogen which will combine easily. Energy is required to start fusion by raising temperature so high all materials would turn into plasma, and allow nuclei to collide and stick together with each other before repelling due to electric charge. This process is called fusion and it can give out energy.In stars that undergo nuclear fusion, fuel consists of atomic nuclei that can release energy by the absorption of a proton or neutron. In most stars the fuel is provided by hydrogen, which can combine to form helium through the proton-proton chain reaction or by the CNO cycle. When the hydrogen fuel is exhausted, nuclear fusion can continue with progressively heavier elements, although the net energy released is lower because of the smaller difference in nuclear binding energy. Once iron-56 or nickel-56 nuclei are produced, no further energy can be obtained by nuclear fusion as these have the highest nuclear binding energies. The elements then on use up energy instead of giving off energy when fused. Therefore, fusion stops and the star dies. In attempts by humans, fusion is only carried out with hydrogen (isotope of 2 and 3) to form helium-4 as this reaction gives out the most net energy. Electric confinement (ITER), inertial confinement(heating by laser) and heating by strong electric currents are the popular methods used. . World trade The World Bank reported that the USA was the top fuel importer in 2005 followed by the EU and Japan. See also  Footnotes  References Ratcliff, Brian; et al. (2000). Chemistry 1. Cambridge University press. ISBN 0-521-78778-5.  Further reading "Directive 1999/94/EC of the European Parliament and of the council of 13 December 1999, relating to the availability of consumer information on fuel economy and CO2 emissions in respect of the marketing of new passenger cars" (PDF).  (140 KB).Council Directive 80/1268/EEC Fuel consumption of motor vehicles.
Table tennis, also known as ping pong, is a sport in which two or four players hit a lightweight ball back and forth across a table using a small bat. The game takes place on a hard table divided by a net. Except for the initial serve, the rules are generally as follows: players must allow a ball played toward them to bounce one time on their side of the table, and must return it so that it bounces on the opposite side at least once. A point is scored when a player fails to return the ball within the rules. Play is fast and demands quick reactions. Spinning the ball alters its trajectory and limits an opponent's options, giving the hitter a great advantage.Table tennis is governed by the worldwide organization International Table Tennis Federation (ITTF), founded in 1926. ITTF currently includes 220 member associations. The table tennis official rules are specified in the ITTF handbook. Table tennis has been an Olympic sport since 1988, with several event categories. From 1988 until 2004, these were men's singles, women's singles, men's doubles and women's doubles. Since 2008, a team event has been played instead of the doubles. History The sport originated in Victorian England, where it was played among the upper-class as an after-dinner parlour game. It has been suggested that makeshift versions of the game were developed by British military officers in India in around 1860s or 1870s, who brought it back with them. A row of books stood up along the center of the table as a net, two more books served as rackets and were used to continuously hit a golf-ball.It had several different names, including 'whiff-whaff'. The name "ping-pong" was in wide use before British manufacturer J. Jaques & Son Ltd trademarked it in 1901. The name "ping-pong" then came to describe the game played using the rather expensive Jaques's equipment, with other manufacturers calling it table tennis. A similar situation arose in the United States, where Jaques sold the rights to the "ping-pong" name to Parker Brothers. Parker Brothers then enforced its trademark for the term in the 1920s making the various associations change their names to "table tennis" instead of the more common, but trademarked, term.The next major innovation was by James W. Gibb, a British enthusiast of table tennis, who discovered novelty celluloid balls on a trip to the US in 1901 and found them to be ideal for the game. This was followed by E.C. Goode who, in 1901, invented the modern version of the racket by fixing a sheet of pimpled, or stippled, rubber to the wooden blade. Table tennis was growing in popularity by 1901 to the extent that tournaments were being organized, books being written on the subject, and an unofficial world championship was held in 1902.In 1921, the Table Tennis Association was founded in Britain, and the International Table Tennis Federation (ITTF) followed in 1926. London hosted the first official World Championships in 1926. In 1933, the United States Table Tennis Association, now called USA Table Tennis, was formed.In the 1930s, Edgar Snow commented in Red Star Over China that the Communist forces in the Chinese Civil War had a "passion for the English game of table tennis" which he found "bizarre". On the other hand, the popularity of the sport waned in 1930s Soviet Union, partly because of the promotion of team and military sports, and partly because of a theory that the game had adverse health effects.In the 1950s, paddles that used a rubber sheet combined with an underlying sponge layer changed the game dramatically, introducing greater spin and speed. These were introduced to Britain by sports goods manufacturer S.W. Hancock Ltd. The use of speed glue increased the spin and speed even further, resulting in changes to the equipment to "slow the game down". Table tennis was introduced as an Olympic sport at the Olympics in 1988. Rule changes After the 2000 Olympics in Sydney, the ITTF instituted several rule changes that were aimed at making table tennis more viable as a televised spectator sport. First, the older 38 mm (1.50 in) balls were officially replaced by 40 mm (1.57 in) balls in October 2000. This increased the ball's air resistance and effectively slowed down the game. By that time, players had begun increasing the thickness of the fast sponge layer on their paddles, which made the game excessively fast and difficult to watch on television. A few months later, the ITTF changed from a 21-point to an 11-point scoring system (and the serve rotation was reduced from five points to two), effective in September 2001. This was intended to make games more fast-paced and exciting. The ITTF also changed the rules on service to prevent a player from hiding the ball during service, in order to increase the average length of rallies and to reduce the server's advantage, effective in 2002. For the opponent to have time to realize a serve is taking place, the ball must be tossed a minimum of 16 cm in the air. The ITTF states that all events after July 2014 are played with a new poly material ball.  Equipment  Ball The international rules specify that the game is played with a sphere having a mass of 2.7 grams (0.095 oz) and a diameter of 40 millimetres (1.57 in). The rules say that the ball shall bounce up 24–26 cm (9.4–10.2 in) when dropped from a height of 30.5 cm (12.0 in) onto a standard steel block thereby having a coefficient of restitution of 0.89 to 0.92. The ball is made of celluloid plastic as of 2015, colored white or orange, with a matte finish. The choice of ball color is made according to the table color and its surroundings. For example, a white ball is easier to see on a green or blue table than it is on a grey table. Manufacturers often indicate the quality of the ball with a star rating system, usually from one to three, three being the highest grade. As this system is not standard across manufacturers, the only way a ball may be used in official competition is upon ITTF approval (the ITTF approval can be seen printed on the ball).The 40 mm ball was introduced after the 2000 Summer Olympics. However, this created some controversy at the time as the Chinese National Team argued that this was merely to give non-Chinese players a better chance of winning since the new type of ball has a slower speed (a 40 mm table tennis ball is slower and spins less than the original 38 mm one, and at that time, most Chinese players were playing with fast attack and smashes). China won all four Olympic gold medals and three silvers in 2000, and have continued to dominate. Table The table is 2.74 m (9.0 ft) long, 1.525 m (5.0 ft) wide, and 76 cm (2.5 ft) high with any continuous material so long as the table yields a uniform bounce of about 23 cm (9.1 in) when a standard ball is dropped onto it from a height of 30 cm (11.8 in), or about 77%. The table or playing surface is uniformly dark coloured and matte, divided into two halves by a net at 15.25 cm (6.0 in) in height. The ITTF approves only wooden tables or their derivates. Concrete tables with a steel net or a solid concrete partition are sometimes available in outside public spaces, such as parks. Paddle/racket Players are equipped with a laminated wooden racket covered with rubber on one or two sides depending on the grip of the player. The ITTF uses the term "racket", though "bat" is common in Britain, and "paddle" in the U.S. and Canada.The wooden portion of the racket, often referred to as the "blade", commonly features anywhere between one and seven plies of wood, though cork, glass fiber, carbon fiber, aluminum fiber, and Kevlar are sometimes used. According to the ITTF regulations, at least 85% of the blade by thickness shall be of natural wood. Common wood types include balsa, limba, and cypress or "hinoki", which is popular in Japan. The average size of the blade is about 17 centimetres (6.7 in) long and 15 centimetres (5.9 in) wide. Although the official restrictions only focus on the flatness and rigidness of the blade itself, these dimensions are optimal for most play styles.Table tennis regulations allow different surfaces on each side of the racket. Various types of surfaces provide various levels of spin or speed, and in some cases they nullify spin. For example, a player may have a rubber that provides much spin on one side of their racket, and one that provides no spin on the other. By flipping the racket in play, different types of returns are possible. To help a player distinguish between the rubber used by his opposing player, international rules specify that one side must be red while the other side must be black. The player has the right to inspect his opponent's racket before a match to see the type of rubber used and what colour it is. Despite high speed play and rapid exchanges, a player can see clearly what side of the racket was used to hit the ball. Current rules state that, unless damaged in play, the racket cannot be exchanged for another racket at any time during a match. Gameplay  Starting a game According to ITTF rule 2.13.1, the first service is decided by lot, normally a coin toss. It is also common for one player (or the umpire/scorer) to hide the ball in one or the other hand, usually hidden under the table, allowing the other player to guess which hand the ball is in. The correct or incorrect guess gives the "winner" the option to choose to serve, receive, or to choose which side of the table to use. (A common but non-sanctioned method is for the players to play the ball back and forth three times and then play out the point. This is commonly referred to as "serve to play", "rally to serve", "play for serve", or "volley for serve".) Service and return In game play, the player serving the ball commences a play. The server first stands with the ball held on the open palm of the hand not carrying the paddle, called the freehand, and tosses the ball directly upward without spin, at least 16 cm (6.3 in) high. The server strikes the ball with the racket on the ball's descent so that it touches first his court and then touches directly the receiver's court without touching the net assembly. In casual games, many players do not toss the ball upward; however, this is technically illegal and can give the serving player an unfair advantage.The ball must remain behind the endline and above the upper surface of the table, known as the playing surface, at all times during the service. The server cannot use his/her body or clothing to obstruct sight of the ball; the opponent and the umpire must have a clear view of the ball at all times. If the umpire is doubtful of the legality of a service they may first interrupt play and give a warning to the server. If the serve is a clear failure or is doubted again by the umpire after the warning, the receiver scores a point.If the service is "good", then the receiver must make a "good" return by hitting the ball back before it bounces a second time on receiver's side of the table so that the ball passes the net and touches the opponent's court, either directly or after touching the net assembly. Thereafter, the server and receiver must alternately make a return until the rally is over. Returning the serve is one of the most difficult parts of the game, as the server's first move is often the least predictable and thus most advantageous shot due to the numerous spin and speed choices at his or her disposal. Let A Let is a rally of which the result is not scored, and is called in the following circumstances:The ball touches the net in service (service), provided the service is otherwise correct or the ball is obstructed by the player on the receiving side. Obstruction means a player touches the ball when it is above or traveling towards the playing surface, not having touched the player's court since last being struck by the player.When the player on the receiving side is not ready and the service is delivered.Player's failure to make a service or a return or to comply with the Laws is due to a disturbance outside the control of the player.Play is interrupted by the umpire or assistant umpire.A let is also called foul service, if the ball hits the server's side of the table, if the ball does not pass further than the edge and if the ball hits the table edge and hits the net. Scoring A point is scored by the player for any of several results of the rally:The opponent fails to make a correct service or return.After making a service or a return, the ball touches anything other than the net assembly before being struck by the opponent.The ball passes over the player's court or beyond his end line without touching his court, after being struck by the opponent.The opponent obstructs the ball.The opponent strikes the ball twice successively. Note that the hand that is holding the racket counts as part of the racket and that making a good return off one's hand or fingers is allowed. It is not a fault if the ball accidentally hits one's hand or fingers and then subsequently hits the racket.The opponent strikes the ball with a side of the racket blade whose surface is not covered with rubber.The opponent moves the playing surface or touches the net assembly.The opponent's free hand touches the playing surface.As a receiver under the expedite system, completing 13 returns in a rally.The opponent that has been warned by the umpire commits a second offense in the same individual match or team match. If the third offence happens, 2 points will be given to the player. If the individual match or the team match has not ended, any unused penalty points can be transferred to the next game of that match.A game shall be won by the player first scoring 11 points unless both players score 10 points, when the game shall be won by the first player subsequently gaining a lead of 2 points. A match shall consist of the best of any odd number of games. In competition play, matches are typically best of five or seven games. Alternation of services and ends Service alternates between opponents every two points (regardless of winner of the rally) until the end of the game, unless both players score ten points or the expedite system is operated, when the sequences of serving and receiving stay the same but each player serves for only one point in turn (Deuce). The player serving first in a game receives first in the next game of the match.After each game, players switch sides of the table. In the last possible game of a match, for example the seventh game in a best of seven matches, players change ends when the first player scores five points, regardless of whose turn it is to serve. Service is subject to change on game point of the match. Upon the possible last point of the match, the player with the lesser score serves. If the sequence of serving and receiving is out of turn or the ends are not changed, points scored in the wrong situation are still calculated and the game shall be resumed with the order at the score that has been reached. Doubles game In addition to games between individual players, pairs may also play table tennis. Singles and doubles are both played in international competition, including the Olympic Games since 1988 and the Commonwealth Games since 2002. In 2005, the ITTF announced that doubles table tennis only was featured as a part of team events in the 2008 Olympics.In doubles, all the rules of single play are applied except for the following.ServiceA line painted along the long axis of the table to create doubles courts bisects the table. This line's only purpose is to facilitate the doubles service rule, which is that service must originate from the right hand "box" in such a way that the first bounce of the serve bounces once in said right hand box and then must bounce at least once in the opponent side's right hand box (far left box for server), or the receiving pair score a point.Order of play, serving and receivingPlayers must hit the ball in turn. For example, if A is paired with B, X is paired with Y, A is the server and X is the receiver. The order of play shall be A→X→B→Y. The rally proceeds this way until one side fails to make a legal return and the other side scores.At each change of service, the previous receiver shall become the server and the partner of the previous server shall become the receiver. For example, if the previous order of play is A→X→B→Y, the order becomes X→B→Y→A after the change of service.In the second or the latter games of a match, the game begins in reverse order of play. For example, if the order of play is A→X→B→Y at beginning of the first game, the order begins with X→A→Y→B or Y→B→X→A in the second game depending on either X or Y being chosen as the first server of the game. That means the first receiver of the game is the player who served to the first server of the game in the preceding game. In each game of a doubles match, the pair having the right to serve first shall choose which of them will do so. The receiving pair, however, can only choose in the first game of the match.When a pair reaches 5 points in the final game, the pairs must switch ends of the table and change the receiver to reverse the order of play. For example, when the last order of play before a pair score 5 points in the final game is A→X→B→Y, the order after change shall be A→Y→B→X if A still has the second serve. Otherwise, X is the next server and the order becomes X→A→Y→B. Expedite system If a game is unfinished after 10 minutes' play and fewer than 18 points have been scored, the expedite system is initiated. The umpire interrupts the game, and the game resumes with players serving for one point in turn. If the expedite system is introduced while the ball is not in play, the previous receiver shall serve first. Under the expedite system, the server must win the point before the opponent makes 13 consecutive returns or the point goes to the opponent. The system can also be initiated at any time at the request of both players or pairs. Once introduced, the expedite system remains in force until the end of the match. A rule to shorten the time of a match, it is mainly seen in defensive players' games. Grips Though table tennis players grip their rackets in various ways, their grips can be classified into two major families of styles, penhold and shakehand. The rules of table tennis do not prescribe the manner in which one must grip the racket, and numerous grips are employed. Penhold The penhold grip is so-named because one grips the racket similarly to the way one holds a writing instrument. The style of play among penhold players can vary greatly from player to player. The most popular style, usually referred to as the Chinese penhold style, involves curling the middle, ring, and fourth finger on the back of the blade with the three fingers always touching one another. Chinese penholders favour a round racket head, for a more over-the-table style of play. In contrast, another style, sometimes referred to as the Japanese/Korean penhold grip, involves splaying those three fingers out across the back of the racket, usually with all three fingers touching the back of the racket, rather than stacked upon one another. Sometimes a combination of the two styles occurs, wherein the middle, ring and fourth fingers are straight, but still stacked, or where all fingers may be touching the back of the racket, but are also in contact with one another. Japanese/Korean penholders will often use a square-headed racket for an away-from-the-table style of play. Traditionally these square-headed rackets feature a block of cork on top of the handle, as well as a thin layer of cork on the back of the racket, for increased grip and comfort. Penhold styles are popular among players originating from East Asian regions such as China, Taiwan, Japan, and South Korea.Traditionally, penhold players use only one side of the racket to hit the ball during normal play, and the side which is in contact with the last three fingers is generally not used. This configuration is sometimes referred to as "traditional penhold" and is more commonly found in square-headed racket styles. However, the Chinese developed a technique in the 1990s in which a penholder uses both sides of the racket to hit the ball, where the player produces a backhand stroke (most often topspin) known as a reverse penhold backhand by turning the traditional side of the racket to face one's self, and striking the ball with the opposite side of the racket. This stroke has greatly improved and strengthened the penhold style both physically and psychologically, as it eliminates the strategic weakness of the traditional penhold backhand. Shakehand The shakehand grip is so-named because the racket is grasped as if one is performing a handshake. Though it is sometimes referred to as the "tennis" or "Western" grip, it bears no relation to the Western tennis grip, which was popularized on the West Coast of the United States in which the racket is rotated 90°, and played with the wrist turned so that on impact the knuckles face the target. In table tennis, "Western" refers to Western nations, for this is the grip that players native to Europe and the Americas have almost exclusively employed.The shakehand grip’s simplicity and versatility, coupled with the acceptance among top-level Chinese trainers that the European style of play should be emulated and trained against, has established it as a common grip even in China. Many world class European and Asian players currently use the shakehand grip, and it is generally accepted that shakehands is easier to learn than penholder, allowing a broader range of playing styles both offensive and defensive. Seemiller The Seemiller grip is named after the American table tennis champion Danny Seemiller, who used it. It is achieved by placing your thumb and index finger on either side of the bottom of the racquet head and holding the handle with the rest of your fingers. Since only one side of the racquet is used to hit the ball, two contrasting rubber types can be applied to the blade, offering the advantage of "twiddling" the racket to fool the opponent. Seemiller paired inverted rubber with anti-spin rubber; many players today combine inverted and long-pipped rubber. The grip is considered exceptional for blocking, especially on the backhand side, and for forehand loops of backspin balls. The Seemiller grip's popularity reached its apex in 1985 when four (Danny Seemiller, Ricky Seemiller, Eric Boggan and Brian Masters) of the United States' five participants in the World Championships used it. Types of strokes Table tennis strokes generally break down into offensive and defensive categories. Offensive strokes  Hit A direct hit on the ball propelling it forward back to the opponent. This stroke differs from speed drives in other racket sports like tennis because the racket is primarily perpendicular to the direction of the stroke and most of the energy applied to the ball results in speed rather than spin, creating a shot that does not arc much, but is fast enough that it can be difficult to return. A speed drive is used mostly for keeping the ball in play, applying pressure on the opponent, and potentially opening up an opportunity for a more powerful attack. Loop Perfected during the 1960s, the loop is essentially the reverse of the speed drive. The racket is much more parallel to the direction of the stroke ("closed") and the racket thus grazes the ball, resulting in a large amount of topspin. A good loop drive will arc quite a bit, and once striking the opponent's side of the table will jump forward, much like a kick serve in tennis. Counter-hit The counter-hit is usually a counterattack against drives, normally high loop drives. The racket is held closed and near to the ball, which is hit with a short movement "off the bounce" (immediately after hitting the table) so that the ball travels faster to the other side. A well-timed, accurate counter-drive can be as effective as a smash. Flip When a player tries to attack a ball that has not bounced beyond the edge of the table, the player does not have the room to wind up in a backswing. The ball may still be attacked, however, and the resulting shot is called a flip because the backswing is compressed into a quick wrist action. A flip is not a single stroke and can resemble either a loop drive or a loop in its characteristics. What identifies the stroke is that the backswing is compressed into a short wrist flick. Smash The offensive trump card is the smash. A player will typically execute a smash when his or her opponent has returned a ball that bounces too high or too close to the net. Smashing consists of using a large backswing and rapid acceleration to impart as much speed on the ball as possible. The goal of a smash is to get the ball to move so quickly that the opponent simply cannot return it. Because the ball speed is the main aim of this shot, often the spin on the ball is something other than topspin. Sidespin can be used effectively with a smash to alter the ball's trajectory significantly, although most intermediate players will smash the ball with little or no spin. An offensive table tennis player will think of a rally as a build-up to a winning smash. Defensive strokes  Push The push (or "slice" in Asia) is usually used for keeping the point alive and creating offensive opportunities. A push resembles a tennis slice: the racket cuts underneath the ball, imparting backspin and causing the ball to float slowly to the other side of the table. While not obvious, a push can be difficult to attack because the backspin on the ball causes it to drop toward the table upon striking the opponent's racket. In order to attack a push, a player must usually loop the ball back over the net. Often, the best option for beginners is to simply push the ball back again, resulting in pushing rallies. Against good players, it may be the worst option because the opponent will counter with a loop, putting the first player in a defensive position. Another response to pushing is flipping the ball when it is close to the net. Pushing can have advantages in some circumstances, such as when the opponent makes easy mistakes. Chop A chop is the defensive, backspin counterpart to the offensive loop drive. A chop is essentially a bigger, heavier push, taken well back from the table. The racket face points primarily horizontally, perhaps a little bit upward, and the direction of the stroke is straight down. The object of a defensive chop is to match the topspin of the opponent's shot with backspin. A good chop will float nearly horizontally back to the table, in some cases having so much backspin that the ball actually rises. Such a chop can be extremely difficult to return due to its enormous amount of backspin. Some defensive players can also impart no-spin or sidespin variations of the chop. Block The block is a simple shot, but nonetheless can be devastating against an attacking opponent. A block is executed by simply placing the racket in front of the ball right after the ball bounces; thus, the ball rebounds back toward the opponent with nearly as much energy as it came in with. This is not as easy as it sounds, because the ball's spin, speed, and location all influence the correct angle of a block. It is very possible for an opponent to execute a perfect loop, drive, or smash, only to have the blocked shot come back at him just as fast. Due to the power involved in offensive strokes, often an opponent simply cannot recover quickly enough, and will be unable to return the blocked shot. Blocks almost always produce the same spin as was received, many times topspin. Depending on the spin of the ball, the block may be returned to an unexpected side of the table. This may come to your advantage, as the opponent may not expect this. Lob The defensive lob is possibly the most impressive shot, since it propels the ball about five metres in height, only to land on the opponent's side of the table with great amounts of spin. To execute, a defensive player first backs-off the table 4–6 meters; then, the stroke itself consists of lifting the ball to an enormous height before it falls back to the opponent's side of the table. A lob is inherently a creative shot, and can have nearly any kind of spin. Top-quality players use this to their advantage in order to control the spin of the ball. For instance, though the opponent may smash the ball hard and fast, a good defensive lob could be more difficult to return due to the unpredictability and heavy amounts of the spin on the ball. Thus, though backed off the table by tens of feet and running to reach the ball, a good defensive player can still win the point using good lobs. However, at the professional level, lobbers will lose the point most of the time, so the lob is not used unless it is really necessary. Effects of spin Adding spin onto the ball causes major changes in table tennis gameplay. Although nearly every stroke or serve creates some kind of spin, understanding the individual types of spin allows players to defend against and use different spins effectively. Backspin Backspin is where the bottom half of the ball is rotating away from the player, and is imparted by striking the base of the ball with a downward movement. At the professional level, backspin is usually used defensively in order to keep the ball low. Backspin is commonly employed in service because it is harder to produce an offensive return, though at the professional level most people serve sidespin with either backspin or topspin. Due to the initial lift of the ball, there is a limit on how much speed with which one can hit the ball without missing the opponent's side of the table. However, backspin also makes it harder for the opponent to return the ball with great speed because of the required angular precision of the return. Alterations are frequently made to regulations regarding equipment in an effort to maintain a balance between defensive and offensive spin choices. It is actually possible to smash with backspin offensively, but only on high balls that are close to the net. Topspin The topspin stroke has a smaller influence on the first part of the ball-curve. Like the backspin stroke, however, the axis of spin remains roughly perpendicular to the trajectory of the ball thus allowing for the Magnus effect to dictate the subsequent curvature. After the apex of the curve, the ball dips downwards as it approaches the opposing side, before bouncing. On the bounce, the topspin will accelerate the ball, much in the same way that a wheel which is already spinning would accelerate upon making contact with the ground. When the opponent attempts to return the ball, the topspin causes the ball to jump upwards and the opponent is forced to compensate for the topspin by adjusting the angle of his or her racket. This is known as "closing the racket". The speed limitation of the topspin stroke is minor compared to the backspin stroke. This stroke is the predominant technique used in professional competition because it gives the opponent less time to respond. In table tennis topspin is regarded as an offensive technique due to increased ball speed, lower bio-mechanical efficiency and the pressure that it puts on the opponent by reducing reaction time. (It is possible to play defensive topspin-lobs from far behind the table, but only highly skilled players use this stroke with any tactical efficiency.) Topspin is the least common type of spin to be found in service at the professional level, simply because it is much easier to attack a top-spin ball that is not moving at high speed. Sidespin This type of spin is predominantly employed during service, wherein the contact angle of the racket can be more easily varied. Unlike the two aforementioned techniques, sidespin causes the ball to spin on an axis which is vertical, rather than horizontal. The axis of rotation is still roughly perpendicular to the trajectory of the ball. In this circumstance, the Magnus effect will still dictate the curvature of the ball to some degree. Another difference is that unlike backspin and topspin, sidespin will have relatively very little effect on the bounce of the ball, much in the same way that a spinning top would not travel left or right if its axis of rotation were exactly vertical. This makes sidespin a useful weapon in service, because it is less easily recognized when bouncing, and the ball "loses" less spin on the bounce. Sidespin can also be employed in offensive rally strokes, often from a greater distance, as an adjunct to topspin or backspin. This stroke is sometimes referred to as a "hook". The hook can even be used in some extreme cases to circumvent the net when away from the table. Corkspin This type of spin is almost exclusively employed in service, but it is also used from time to time in the lob at the professional level. Unlike any of the aforementioned techniques, corkspin (sometimes referred to as "drill-spin") features a unique situation in which the axis of spin is more or less parallel to the trajectory of the ball. This means that the Magnus effect will have little to no effect on the trajectory of a cork-spun ball. Upon bouncing, the ball will dart right or left, depending on the direction of the spin, making it very difficult to return. Although in theory this type of spin produces the most obnoxious effects, it is not as strategically practical as sidespin or backspin in terms of the limitations that it imposes upon the opponent during their return. Aside from the initial direction change when bouncing, provided that it does not exceed the reach of the opponent, a cork-spun ball is easily countered with topspin or backspin. Similar to a backspin stroke, the corkspin stroke has a lower maximum velocity, simply due to the contact angle of the racket when producing the stroke. To impart a spin on the ball which is parallel to its trajectory, the racket must be swung more or less perpendicular to the trajectory of the ball. This greatly limits the amount of forward momentum that can be transferred to the ball by the racket. Corkspin is almost always mixed with another variety of spin, as it is less effective and harder to produce on its own. Competition Competitive table tennis is popular in East Asia and Europe and has been gaining attention in the United States. The most important international competitions are the World Table Tennis Championships, the Table Tennis World Cup, the Olympics and the ITTF World Tour. Continental competitions include the European Championships, Europe Top-16, the Asian Championships and the Asian Games. Chinese players have won the men's World Championships 60% of the time since 1959; in the women's competition, Chinese players have won all but three of the World Championships since 1971. Other strong teams come from East Asia and European countries, including Austria, Belarus, Germany, Hong Kong, Portugal, Japan, South Korea, Singapore, Sweden, and Taiwan.There are also professional competitions at the clubs level. The national league of countries like Austria, Belgium, China (the China Table Tennis Super League), France, Germany and Russia are some highest level examples. There are also some important international club teams competitions such as the European Champions League and its former competition, the European Club Cup, where the top club teams from European countries compete. Notable players An official hall of fame exists at the ITTF Museum. A Grand Slam is earned by a player who wins singles crowns at Olympic Games, World Championships, and World Cup. Jan-Ove Waldner of Sweden first completed the grand slam at 1992 Olympic Games. Deng Yaping of China is the first female recorded at the inaugural Women's World Cup in 1996.Jean-Philippe Gatien (France) and Wang Hao (China) won both the World Championships and the World Cup, but lost in the gold medal matches at the Olympics. Jörgen Persson (Sweden) also won the titles except the Olympic Games. Persson is one of the three table tennis players to have competed at seven Olympic Games. Ma Lin (China) won both the Olympic gold and the World Cup, but lost (three times, in 1999, 2005, and 2007) in the finals of the World Championships. Governance Founded in 1926, the International Table Tennis Federation (ITTF) is the worldwide governing body for table tennis, which maintains an international ranking system in addition to organizing events like the World Table Tennis Championships. In 2007, the governance for table tennis for persons with a disability was transferred from the International Paralympic Committee to the ITTF.On many continents, there is a governing body responsible for table tennis on that continent. For example, the European Table Tennis Union (ETTU) is the governing body responsible for table tennis in Europe. There are also national bodies and other local authorities responsible for the sport, such as USA Table Tennis (USATT), which is the national governing body for table tennis in the United States. See also International Table Tennis FederationList of ITTF Pro Tour winnersList of table tennis playersTable tennis terminologyDisability table tennis classificationTable squashPingpongoHeadis References  Bibliography Uzorinac, Zdenko (2001). ITTF 1926 - 2001 Table Tennis Legends. ITTF. ISBN 2-94031-200-1. OCLC 248920627. Charyn, Jerome (2002). Sizzling Chops & Devilish Spins: Ping-Pong and the Art of Staying Alive. Four Walls Eight Windows. ISBN 1-56858-242-0. Hodges, Larry (1993). Table Tennis: Steps to Success. Human Kinetics. ISBN 0-87322-403-5. International Table Tennis Federation (2011). ITTF Handbook 2011/2012. Retrieved 25 December 2011. Seemiller, Dan (1996). Winning Table Tennis: Skills, Drills, and Strategies. Human Kinetics. ISBN 0-88011-520-3.  External links Official ITTF websiteOfficial website of USA Table Tennis
Computer software, or simply software, is that part of a computer system that consists of data or computer instructions, in contrast to the physical hardware from which the system is built. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.At the lowest level, executable code consists of machine language instructions specific to an individual processor—typically a central processing unit (CPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also (indirectly) cause something to appear on a display of the computer system—a state change which should be visible to the user. The processor carries out the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or is interrupted (by now multi-core processors are dominant, where each core can run instructions in order; then, however, each application software runs only on one core by default, but some software has been made to run on many).The majority of software is written in high-level programming languages that are easier and more efficient for programmers, meaning closer to a natural language. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, essentially, a vaguely mnemonic representation of a machine language using a natural language alphabet, which is translated into machine language using an assembler. History An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. However, neither the Analytical Engine nor any software for it were ever created.The first theory about software—prior to creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay Computable numbers with an application to the Entscheidungsproblem (decision problem).This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering focuses on more practical concerns.However, prior to 1946, software as we now understand it—programs stored in the memory of stored-program digital computers—did not yet exist. The first electronic computing devices were instead rewired in order to "reprogram" them. Types of software On virtually all computer platforms, software can be grouped into a few broad categories. Purpose, or domain of use Based on the goal, computer software can be divided into:Application software, which is software that uses the computer system to perform special functions or provide entertainment functions beyond the basic operation of the computer itself. There are many different types of application software, because the range of tasks that can be performed with a modern computer is so large—see list of software.System software, which is software that directly operates the computer hardware, to provide basic functionality needed by users and other software, and to provide a platform for running application software. System software includes:Operating systems, which are essential collections of software that manage resources and provides common services for other software that runs "on top" of them. Supervisory programs, boot loaders, shells and window systems are core parts of operating systems. In practice, an operating system comes bundled with additional software (including application software) so that a user can potentially do some work with a computer that only has an operating system.Device drivers, which operate or control a particular type of device that is attached to a computer. Each device needs at least one corresponding device driver; because a computer typically has at minimum at least one input device and at least one output device, a computer typically needs more than one device driver.Utilities, which are computer programs designed to assist users in the maintenance and care of their computers.Malicious software or malware, which is software that is developed to harm and disrupt computers. As such, malware is undesirable. Malware is closely associated with computer-related crimes, though some malicious programs may have been designed as practical jokes. Nature or domain of execution Desktop applications such as web browsers and Microsoft Office, as well as smartphone and tablet applications (called "apps"). (There is a push in some parts of the software industry to merge desktop applications with mobile apps, to some extent. Windows 8, and later Ubuntu Touch, tried to allow the same style of application user interface to be used on desktops, laptops and mobiles.)JavaScript scripts are pieces of software traditionally embedded in web pages that are run directly inside the web browser when a web page is loaded without the need for a web browser plugin. Software written in other programming languages can also be run within the web browser if the software is either translated into JavaScript, or if a web browser plugin that supports that language is installed; the most common example of the latter is ActionScript scripts, which are supported by the Adobe Flash plugin.Server software, including:Web applications, which usually run on the web server and output dynamically generated web pages to web browsers, using e.g. PHP, Java, ASP.NET, or even JavaScript that runs on the server. In modern times these commonly include some JavaScript to be run in the web browser as well, in which case they typically run partly on the server, partly in the web browser.Plugins and extensions are software that extends or modifies the functionality of another piece of software, and require that software be used in order to function;Embedded software resides as firmware within embedded systems, devices dedicated to a single use or a few uses such as cars and televisions (although some embedded devices such as wireless chipsets can themselves be part of an ordinary, non-embedded computer system such as a PC or smartphone). In the embedded system context there is sometimes no clear distinction between the system software and the application software. However, some embedded systems run embedded operating systems, and these systems do retain the distinction between system software and application software (although typically there will only be one, fixed, application which is always run).Microcode is a special, relatively obscure type of embedded software which tells the processor itself how to execute machine code, so it is actually a lower level than machine code. It is typically proprietary to the processor manufacturer, and any necessary correctional microcode software updates are supplied by them to users (which is much cheaper than shipping replacement processor hardware). Thus an ordinary programmer would not expect to ever have to deal with it. Programming tools Programming tools are also software in the form of programs or applications that software developers (also known as programmers, coders, hackers or software engineers) use to create, debug, maintain (i.e. improve or fix), or otherwise support software. Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE. Software topics  Architecture Users often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.Platform software: The Platform includes the firmware, device drivers, an operating system, and typically a graphical user interface which, in total, allow a user to interact with the computer and its peripherals (associated equipment). Platform software often comes bundled with the computer. On a PC one will usually have the ability to change the platform software.Application software: Application software or Applications are what most people think of when they think of software. Typical examples include office suites and video games. Application software is often purchased separately from computer hardware. Sometimes applications are bundled with the computer, but that does not change the fact that they run as independent applications. Applications are usually independent programs from the operating system, though they are often tailored for specific platforms. Most users think of compilers, databases, and other "system software" as applications.User-written software: End-user development tailors systems to meet users' specific needs. User software include spreadsheet templates and word processor templates. Even email filters are a kind of user software. Users create this software themselves and often overlook how important it is. Depending on how competently the user-written software has been integrated into default application packages, many users may not be aware of the distinction between the original packages, and what has been added by co-workers. Execution Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using "pointers" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together. Quality and reliability Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.Many bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that "every program has at least one more bug" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together. License The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.Proprietary software can be divided into two types:freeware, which includes the category of "free trial" software or "freemium" software (in the past, the term shareware was often used for free trial/freemium software). As the name suggests, freeware can be used for free, although in the case of free trials or freemium software, this is sometimes only true for a limited period of time or with limited functionality.software available for a fee, often inaccurately termed "commercial software", which can only be legally used on purchase of a license.Open source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software. Patents Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a detailed idea (e.g. an algorithm) on how to implement a piece of software, or a component of a piece of software. Ideas for useful things that software could do, and user requirements, are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since all useful software has effects on the physical world, this requirement may be open to debate.Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over any programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents. Design and implementation Design and implementation of software varies depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the latter has much more basic functionality.Software is usually designed and created (aka coded/written/programmed) in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software (if applicable). As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, if a user is designing a Microsoft Windows desktop application, he or she might use the .NET Windows Forms library to design the desktop application and call its APIs like Form1.Close() and Form1.Show() to close or open the application, and write the additional operations him/herself that it needs to have. Without these APIs, the programmer needs to write these APIs him/herself. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as "coder" and "hacker" – although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems. Industry and organizations A great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the co-founder of Microsoft was the richest person in the world in 2009, largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products.Non-profit software organizations include the Free Software Foundation, GNU Project and Mozilla Foundation. Software standard organizations like the W3C, IETF develop recommended software standards such as XML, HTTP and HTML, so that software can interoperate through these standards.Other well-known large software companies include Oracle, Novell, SAP, Symantec, Adobe Systems, and Corel, while small companies often provide innovation. See also Software release life cycleIndependent software vendorList of softwareSoftware asset management References  External links Software at DMOZ
A sale is the exchange of a commodity or money as the price of a good or a service. Sales (plural only) is activity related to selling or the amount of goods or services sold in a given time period.The seller or the provider of the goods or services completes a sale in response to an acquisition, appropriation, requisition or a direct interaction with the buyer at the point of sale. There is a passing of title (property or ownership) of the item, and the settlement of a price, in which agreement is reached on a price for which transfer of ownership of the item will occur. The seller, not the purchaser generally executes the sale and it may be completed prior to the obligation of payment. In the case of indirect interaction, a person who sells goods or service on behalf of the owner is known as a salesman or saleswoman or salesperson, but this often refers to someone selling goods in a store/shop, in which case other terms are also common, including salesclerk, shop assistant, and retail clerk.In common law countries, sales are governed generally by the common law and commercial codes. In the United States, the laws governing sales of goods are somewhat uniform to the extent that most jurisdictions have adopted Article 2 of the Uniform Commercial Code, albeit with some non-uniform variations. Definition A person or organization expressing an interest in acquiring the offered item of value is referred to as a potential buyer, prospective customer or prospect. Buying and selling are understood to be two sides of the same "coin" or transaction. Both seller and buyer engage in a process of negotiation to consummate the exchange of values. The exchange, or selling, process has implied rules and identifiable stages. It is implied that the selling process will proceed fairly and ethically so that the parties end up nearly equally rewarded. The stages of selling, and buying, involve getting acquainted, assessing each party's need for the other's item of value, and determining if the values to be exchanged are equivalent or nearly so, or, in buyer's terms, "worth the price". Sometimes, sellers have to use their own experiences when selling products with appropriate discounts.From a management viewpoint it is thought of as a part of marketing, although the skills required are different. Sales often forms a separate grouping in a corporate structure, employing separate specialist operatives known as salespersons (singular: salesperson). Selling is considered by many to be a sort of persuading "art". Contrary to popular belief, the methodological approach of selling refers to a systematic process of repetitive and measurable milestones, by which a salesman relates his or her offering of a product or service in return enabling the buyer to achieve their goal in an economic way. While the sales process refers to a systematic process of repetitive and measurable milestones, the definition of the selling is somewhat ambiguous due to the close nature of advertising, promotion, public relations, and direct marketing.Selling is the profession-wide term, much like marketing defines a profession. Recently, attempts have been made to clearly understand who is in the sales profession, and who is not. There are many articles looking at marketing, advertising, promotions, and even public relations as ways to create a unique transaction.Two common terms used to describe a salesperson are "Farmer" and "Hunter". The reality is that most professional sales people have a little of both. A hunter is often associated with aggressive personalities who use aggressive sales technique. In terms of sales methodology a hunter refers to a person whose focus is on bringing in and closing deals. This process is called "sales capturing". An example is a commodity sale such as a long distance sales person, shoe sales person and to a degree a car sales person. Their job is to find and convert buyers. A sales farmer is someone who creates sales demand by activities that directly influence and alter the buying process.Many believe that the focus of selling is on the human agents involved in the exchange between buyer and seller. Effective selling also requires a systems approach, at minimum involving roles that sell, enable selling, and develop sales capabilities. Selling also involves salespeople who possess a specific set of sales skills and the knowledge required to facilitate the exchange of value between buyers and sellers that is unique from marketing, advertising, etc.Within these three tenets, the following definition of professional selling is offered by the American Society for Training and Development (ASTD):Team selling is one way to influence sales. Team selling is "a group of people representing the sales department and other functional areas in the firm, such as finance, production, and research and development". (Spiro) Team selling came about in the 1990s through total quality management (TQM). TQM occurs when companies work to improve their customer satisfaction by constantly improving all of their operations. Relationships with marketing Marketing and sales differ greatly, but generally have the same goal. Selling is the final stage in marketing, which also includes pricing, promotion, place and product (the 4 P's). A marketing department in an organization has the goals of increasing the desirability and value to the customer and increasing the number and engagement of interactions between potential customers and the organization. Achieving this goal may involve the sales team using promotional techniques such as advertising, sales promotion, publicity, and public relations, creating new sales channels, or creating new products (new product development), among other things. It can also include bringing the potential customer to visit the organization's website(s) for more information, or to contact the organization for more information, or to interact with the organization via social media such as Twitter, Facebook and blogs. Social values also play a major role in consumer decision processes.The field of sales process engineering views "sales" as the output of a larger system, not just as the output of one department. The larger system includes many functional areas within an organization. From this perspective, "sales" and "marketing" (among others, such as "customer service") label for a number of processes whose inputs and outputs supply one another to varying degrees. In this context, improving an "output" (such as sales) involves studying and improving the broader sales process, as in any system, since the component functional areas interact and are interdependent.Many large corporations structure their marketing departments so they are directly integrated with all lines of business. They create multiple teams with a singular focus and the managers of these teams must coordinate efforts in order to drive profits and business success. For example, an "inbound" focused campaign seeks to drive more customers "through the door", giving the sales department a better chance of selling their product to the consumer. A good marketing program would address any potential downsides as well.The sales department would aim to improve the interaction between the customer and the sales facility or mechanism (example, web site) and/or salesperson. Sales management would break down the selling process and then increase the effectiveness of the discrete processes as well as the interaction between processes. For example, in many out-bound sales environments, the typical process includes out-bound calling, the sales pitch, handling objections, opportunity identification, and the close. Each step of the process has sales-related issues, skills, and training needs, as well as marketing solutions to improve each discrete step, as well as the whole process.One further common complication of marketing involves the inability to measure results for a great deal of marketing initiatives. In essence, many marketing and advertising executives often lose sight of the objective of sales/revenue/profit, as they focus on establishing a creative/innovative program, without concern for the top or bottom lines – a fundamental pitfall of marketing for marketing's sake.Many companies find it challenging to get marketing and sales on the same page. The two departments, although different in nature, handle very similar concepts and have to work together for sales to be successful. Building a good relationship between the two that encourages communication can be the key to success – even in a down economy. Industrial marketing The idea that marketing can potentially eliminate the need for sales people depends entirely on context. For example, this may be possible in some B2C situations; however, for many B2B transactions (for example, those involving industrial organizations) this is mostly impossible. Another dimension is the value of the goods being sold. Fast-moving consumer-goods (FMCG) require no sales people at the point of sale to get them to jump off the supermarket shelf and into the customer's trolley. However, the purchase of large mining equipment worth millions of dollars will require a sales person to manage the sales process – particularly in the face of competitors. Small and medium businesses selling such large ticket items to a geographically-disperse client base use manufacturers' representatives to provide these highly personal service while avoiding the large expense of a captive sales force. Sales and marketing alignment and integration Another area of discussion involves the need for alignment and integration between corporate sales and marketing functions. According to a report from the Chief Marketing Officer (CMO) Council, only 40 percent of companies have formal programs, systems or processes in place to align and integrate the two critical functions.Traditionally, these two functions, as referenced above, have operated separately, left in siloed areas of tactical responsibility. Glen Petersen's book The Profit Maximization Paradox sees the changes in the competitive landscape between the 1950s and the time of writing as so dramatic that the complexity of choice, price and opportunities for the customer forced this seemingly simple and integrated relationship between sales and marketing to change forever. Petersen goes on to highlight that salespeople spend approximately 40 percent of their time preparing customer-facing deliverables while leveraging less than 50 percent of the materials created by marketing, adding to perceptions that marketing is out of touch with the customer and that sales is resistant to messaging and strategy. Methods  List of methods A sale can take place through:Direct sales, involving person to person contactChannel sales, an indirect sales model, which differs from direct sales. Channel selling is a way for ("B2B") sellers to reach the ("B2B") and ("B2C") markets through distributors, re-sellers or value added re-sellers VARS.Pro forma salesAgency-basedSales agents (for example in real estate or in manufacturing)Sales outsourcing through direct branded representationTransaction salesConsultative salesComplex salesConsignmentTelemarketing or telesalesRetail or consumerTraveling salesmanDoor-to-door methodshawkingRequest for proposal – An invitation for suppliers, through a bidding process, to submit a proposal on a specific product or service. An RFP usually represents part of a complex sales process, also known as "enterprise sales".Business-to-business – Business-to-business ("B2B") sales are likely to be larger in terms of volume, economic value and complexity than business-to-consumer ("B2C") sales. Because of this complexity, there is a need to manage the relationships between the buying and selling organisations, for example using Peter Cheverton's relationship models and the stakeholder map by Anderson, Bryson and CrosbyElectronicWeb – Business-to-business ("B2B") and business-to-consumer ("B2C")Electronic Data Interchange (EDI) – A set of standard for structuring information to be electronically exchanged between and within businessesIndirect, human-mediated but with indirect contactMail-ordervending machineSales Techniques:Selling techniqueConsultative sellingSales enablementSolution sellingConceptual sellingStrategic sellingTransactional sellingSales NegotiationReverse SellingUpsellingCross-sellingPaint-the-Picturetake awaySales HabitsRelationship SellingSales outsourcingCold callingGuaranteed saleNeeds-based sellingProfessional Selling SkillsPersuasive sellingHard SellingPrice based sellingTarget account sellingSandler Selling SystemChallenger SalesAction SellingAuctionsSocial SellingPersonal selling Sales agents Agents in the sales process can represent either of two parties in the sales process; for example:Sales broker, seller agency, seller agent, seller representative: This is a traditional role where the salesman represents a person or company on the selling end of a deal.Buyers broker or Buyer brokerage: This is where the salesman represents the consumer making the purchase. This is most often applied in large transactions.Disclosed dual agent:This is where the salesman represents both parties in the sale and acts as a mediator for the transaction. The role of the salesman here is to oversee that both parties receive an honest and fair deal, and is responsible to both.Transaction broker: This is where the salesperson represents neither party but handles the transaction only. The seller owes no responsibility to either party getting a fair or honest deal, just that all of the papers are handled properly.Sales outsourcing involves direct branded representation where the sales representatives are recruited, hired, and managed by an external entity but hold quotas, represent themselves as the brand of the client, and report all activities (through their own sales management channels) back to the client. It is akin to a virtual extension of a sales force (see sales outsourcing).Sales managers aim to implement various sales strategies and management techniques in order to facilitate improved profits and increased sales volume. They are also responsible for coordinating the sales and marketing department as well as oversight concerning the fair and honest execution of the sales process by their agents.Salesperson: The primary function of professional salespeople is to generate and close business resulting in revenue. The sales person will accomplish their primary function through a variety of means including phone calls, email, social media, networking, and cold calling. The primary objective of the successful salesperson is to find the consumers to sell to. Sales is often referred to as a "numbers game" because a general law of averages and pattern of successful closing of business will emerge through heightened sales activity. These activities include but are not limited to: locating prospects, fostering relationships with prospects, building trust with future clients, identifying and filling needs of consumers, and therefore turning prospective customers into actual ones. Many tools are used by successful salespeople, the most important of which is questioning which can be defined as a series of questions and resulting answers allowing the salesperson to understand a customer's goals and requirements relevant to the product. The creation of value or perceived value is the result of taking the information gathered, analyzing the goals and needs of the prospective customer and leveraging the products and/or services the salesperson's firm represents or sells in a way that most effectively achieves the prospective clients goals and/or suits their needs. Effective salespeople will package their offering and present their proposed solution in a way that leads the prospective customer to the conclusion that they acquire the solution, resulting in revenue and profit for the salesperson and the organization they represent.Internet Sales Professionals: These people are primarily responsible for ensuring immediate response to the leads generated via social media, website or email campaigns. Inside sales vs. Outside sales Since the advent of the telephone, a distinction has been made between "inside sales" and "outside sales" although it is generally agreed that those terms have no hard-and-fast definition. In the United States, the Fair Labor Standards Act defines outside sales representatives as "employees [who] sell their employer's products, services, or facilities to customers away from their employer's place(s) of business, in general, either at the customer's place of business or by selling door-to-door at the customer's home" while defining those who work "from the employer's location" as inside sales. Inside sales generally involves attempting to close business primarily over the phone via telemarketing, while outside sales (or "field" sales) will usually involve initial phone work to book sales calls at the potential buyer's location to attempt to close the deal in person. Some companies have an inside sales department that works with outside representatives and book their appointments for them. Inside sales sometimes refers to upselling to existing customers. See also BuzzwordChoice architectureDemand chainFinancial transactionPersonal sellingSales (accounting)Sales effectivenessSales incentive planSales contestSales territorySales varianceTradeVendor References  External links 
Microsoft Windows (or simply Windows) is a metafamily of graphical operating systems developed, marketed, and sold by Microsoft. It consists of several families of operating systems, each of which cater to a certain sector of the computing industry with the OS typically associated with IBM PC compatible architecture. Active Windows families include Windows NT and Windows Embedded; these may encompass subfamilies, e.g. Windows Embedded Compact (Windows CE) or Windows Server. Defunct Windows families include Windows 9x, Windows Mobile and Windows Phone.Microsoft introduced an operating environment named Windows on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993). On PCs, Windows is still the most popular operating system. However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android, because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison however may not be fully relevant, as the two operating systems traditionally target different platforms.As of September 2016, the most recent version of Windows for PCs, tablets, smartphones and embedded devices is Windows 10. The most recent versions for server computers is Windows Server 2016. A specialized version of Windows runs on the Xbox One game console. Genealogy  By marketing role Microsoft, the developer of Windows, has registered several trademarks each of which denote a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families are being actively developed:Windows NT: Started as a family of operating system with Windows NT 3.1, an operating system for server computers and workstations. It now consists of three operating system subfamilies that are released almost at the same time and share the same kernel. It is almost impossible for someone unfamiliar with the subject to identify the members of this family by name because they do not adhere to any specific rule; e.g. Windows Vista, Windows 7, Windows 8/8.1 and Windows RT are members of this family but Windows 3.1 is not.Windows: The operating system for mainstream personal computers, tablets and smartphones. The latest version is Windows 10. The main competitor of this family is macOS by Apple Inc. for personal computers and Android for mobile devices (c.f. Usage share of operating systems § Market share by category).Windows Server: The operating system for server computers. The latest version is Windows Server 2016. Unlike its clients sibling, it has adopted a strong naming scheme. The main competitor of this family is Linux. (c.f. Usage share of operating systems § Market share by category)Windows PE: A lightweight version of its Windows sibling meant to operate as a live operating system, used for installing Windows on bare-metal computers (especially on many computers at once), recovery or troubleshooting purposes. The latest version is Windows PE 10.0.10586.0.Windows Embedded: Initially, Microsoft developed Windows CE as a general-purpose operating system for every device that was too resource-limited to be called a full-fledged computer. Eventually, however, Windows CE was renamed Windows Embedded Compact and was folded under Windows Compact trademark which also consists of Windows Embedded Industry, Windows Embedded Professional, Windows Embedded Standard, Windows Embedded Handheld and Windows Embedded Automotive.The following Windows families are no longer being developed:Windows 9x: An operating system that targeted consumers market. Discontinued because of suboptimal performance. (PC World called its last version, Windows ME, one of the worst products of all times.) Microsoft now caters to the consumers market with Windows NT.Windows Mobile: The predecessor to Windows Phone, it was a mobile phone operating system. The first version was called Pocket PC 2000; the third version, Windows Mobile 2003 is the first version to adopt the Windows Mobile trademark. The last version is Windows Mobile 6.5.Windows Phone: An operating system sold only to manufacturers of smartphones. The first version was Windows Phone 7, followed by Windows Phone 8, and the last version Windows Phone 8.1. It was succeeded by Windows 10 Mobile. Version history The term Windows collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows: Early versions The history of Windows dates back to September 1981, when Chase Bishop, a computer scientist, designed the first model of an electronic device and project Interface Manager was started. It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows.Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area.In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control. Windows 3.x Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities. Windows 9x The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.On September 14, 2000, Microsoft released Windows ME (Millennium Edition), the last DOS-based version of Windows. Windows ME incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows ME was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. PC World considered Windows ME to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time. Windows NT  Early versions In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand. Windows XP The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005, by Windows Server 2003 R2. Windows Vista After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008. Windows 7 On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements. Windows 8 and 8.1 Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 has been subject to some criticism, such as removal of Start Menu. Windows 10 On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1 and Windows 8.1 computers from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).On November 12, 2015, an update to Windows 10, version 1511, was released. This update can be activated with a Windows 7, 8 or 8.1 product key as well as Windows 10 product keys. Features include new icons and right-click context menus, default printer management, four times as many tiles allowed in the Start menu, Find My Device, and Edge updates. Multilingual support Multilingual support is built into Windows. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) –  they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets. Platform support Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Window NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Windows CE Windows CE (officially known as Windows Embedded Compact), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel. Xbox OS Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications. Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC. The Windows 10-based Core had replaced the Windows 8-based one in this update, and the new system is sometimes referred to as "Windows 10 on Xbox One" or "OneCore". Xbox One's system also allows backward compatibility with Xbox 360, and the Xbox 360's system is backwards compatible with the original Xbox. Timeline of releases  Usage share and device sales According to Net Applications, that tracks use based on web use, Windows is the most-used operating system family for personal computers as of June 2016 with close to 90% usage share. When including both personal computers of all kinds, e.g. mobile devices, in July 2016, according to StatCounter, that also tracks use based on web use, Windows OSes accounted for 46.87% of usage share, compared to 36.48% for Android, 12.26% for iOS, and 4.81% for macOS. The below 50% usage share of Windows, also applies to developed countries, such as the US, the UK and Ireland. These numbers are easiest (monthly numbers) to find that track real use, but they may not mirror installed base or sales numbers (in recent years) of devices.In terms of the number of devices shipped with the operating system installed, on smartphones, Windows Phone was the third-most-shipped OS (2.6%) after Android (82.8%) and iOS (13.9%) in the second quarter of 2015 according IDC. Across both PCs and mobile devices, in 2014, Windows OSes were the second-most-shipped (333 million devices, or 14%) after Android (1.2 billion, 49%) and ahead of iOS and macOS combined (263 million, 11%). Security Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's Counterpane Internet Security reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs—viruses, Trojans, back-doors, and exploits written for Windows.Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token. File permissions All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGLP (Accounts, Global, Local, Permissions) AGDLP which in essence where file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directory to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders. Windows Defender On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7. In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials, borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution. Windows Malicious Software Removal Tool and the optional Microsoft Safety Scanner are two other free security products offered by Microsoft. Third-party analysis In an article based on a report by Symantec, internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004, found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only four minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours. The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004, determined that 80% of Windows users were infected by at least one spyware/adware product. Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update. Alternative implementations Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:Wine – a free and open-source implementation of the Windows API, allowing one to run many Windows applications on x86-based platforms, including UNIX, Linux and macOS. Wine developers refer to it as a "compatibility layer" and use Windows-style APIs to emulate Windows environment.CrossOver – a Wine package with licensed fonts. Its developers are regular contributors to Wine, and focus on Wine running officially supported applications.Cedega – a proprietary fork of Wine by TransGaming Technologies, designed specifically for running Microsoft Windows games on Linux. A version of Cedega known as Cider allows Windows games to run on macOS. Since Wine was licensed under the LGPL, Cedega has been unable to port the improvements made to Wine to their proprietary codebase. Cedega ceased its service in February 2011.Darwine – a port of Wine for macOS and Darwin. Operates by running Wine on QEMU.Linux Unified Kernel – a set of patches to the Linux kernel allowing many Windows executable files in Linux (using Wine DLLs); and some Windows drivers to be used.ReactOS – an open-source OS intended to run the same software as Windows, originally designed to simulate Windows NT 4.0, now aiming at Windows 7 compatibility. It has been in the development stage since 1996.Linspire – formerly LindowsOS, a commercial Linux distribution initially created with the goal of running major Windows software. Changed its name to Linspire after Microsoft v. Lindows. Discontinued in favor of Xandros Desktop, that was also later discontinued.Freedows OS – an open-source attempt at creating a Windows clone for x86 platforms, intended to be released under the GNU General Public License. Started in 1996, by Reece K. Sellin, the project was never completed, getting only to the stage of design discussions which featured a number of novel concepts until it was suspended in 2002. See also Architecture of Windows NTWintelDe facto standardDominant design References  External links Official websiteMicrosoft Developer NetworkWindows Client Developer ResourcesMicrosoft Windows History TimelinePearson Education, InformIT – History of Microsoft WindowsMicrosoft Windows 7 for Government
Sport (British English) or sports (American English), are all usually forms of competitive physical activity or games which, through casual or organised participation, aim to use, maintain or improve physical ability and skills while providing enjoyment to participants, and in some cases, entertainment for spectators. Usually the contest or game is between two sides, each attempting to exceed the other. Some sports allow a tie game; others provide tie-breaking methods, to ensure one winner and one loser. A number of such two-sided contests may be arranged in a tournament producing a champion. Many sports leagues make an annual champion by arranging games in a regular sports season, followed in some cases by playoffs. Hundreds of sports exist, from those between single contestants, through to those with hundreds of simultaneous participants, either in teams or competing as individuals. In certain sports such as racing, many contestants may compete, each against each other, with one winner.Sport is generally recognised as system of activities which are based in physical athleticism or physical dexterity, with the largest major competitions such as the Olympic Games admitting only sports meeting this definition, and other organisations such as the Council of Europe using definitions precluding activities without a physical element from classification as sports. However, a number of competitive, but non-physical, activities claim recognition as mind sports. The International Olympic Committee (through ARISF) recognises both chess and bridge as bona fide sports, and SportAccord, the international sports federation association, recognises five non-physical sports: bridge, chess, draughts (checkers), Go and xiangqi, and limits the number of mind games which can be admitted as sports.Sports are usually governed by a set of rules or customs, which serve to ensure fair competition, and allow consistent adjudication of the winner. Winning can be determined by physical events such as scoring goals or crossing a line first. It can also be determined by judges who are scoring elements of the sporting performance, including objective or subjective measures such as technical performance or artistic impression.Records of performance are often kept, and for popular sports, this information may be widely announced or reported in sport news. Sport is also a major source of entertainment for non-participants, with spectator sport drawing large crowds to sport venues, and reaching wider audiences through broadcasting. Sports betting is in some cases severely regulated, and in some cases is central to the sport.According to A.T. Kearney, a consultancy, the global sporting industry is worth up to $620 billion as of 2013. The world's most accessible and practiced sport is running, while association football is the most popular spectator sport. Meaning and usage  Etymology The word "Sport" comes from the Old French desport meaning "leisure", with the oldest definition in English from around 1300 being "anything humans find amusing or entertaining".Other meanings include gambling and events staged for the purpose of gambling; hunting; and games and diversions, including ones that require exercise. Roget's defines the noun sport as an "activity engaged in for relaxation and amusement" with synonyms including diversion and recreation. Nomenclature The singular term "sport" is used in most English dialects to describe the overall concept (e.g. "children taking part in sport"), with "sports" used to describe multiple activities (e.g. "football and rugby are the most popular sports in England"). American English uses "sports" for both terms. Definition The precise definition of what separates a sport from other leisure activities varies between sources. The closest to an international agreement on a definition is provided by SportAccord, which is the association for all the largest international sports federations (including association football, athletics, cycling, tennis, equestrian sports, and more), and is therefore the de facto representative of international sport.SportAccord uses the following criteria, determining that a sport should:have an element of competitionbe in no way harmful to any living creaturenot rely on equipment provided by a single supplier (excluding proprietary games such as arena football)not rely on any "luck" element specifically designed into the sport.They also recognise that sport can be primarily physical (such as rugby or athletics), primarily mind (such as chess or go), predominantly motorised (such as Formula 1 or powerboating), primarily co-ordination (such as billiard sports), or primarily animal-supported (such as equestrian sport).The inclusion of mind sports within sport definitions has not been universally accepted, leading to legal challenges from governing bodies in regards to being denied funding available to sports. Whilst SportAccord recognises a small number of mind sports, it is not open to admitting any further mind sports.There has been an increase in the application of the term "sport" to a wider set of non-physical challenges such as video games, also called esports, especially due to the large scale of participation and organised competition, but these are not widely recognised by mainstream sports organisations. According to Council of Europe, European Sports Charter, article 2.i, " "Sport" means all forms of physical activity which, through casual or organised participation, aim at expressing or improving physical fitness and mental well-being, forming social relationships or obtaining results in competition at all levels.". Competition There are opposing views on the necessity of competition as a defining element of a sport, with almost all professional sport involving competition, and governing bodies requiring competition as a prerequisite of recognition by the International Olympic Committee (IOC) or SportAccord.Other bodies advocate widening the definition of sport to include all physical activity. For instance, the Council of Europe include all forms of physical exercise, including those competed just for fun.In order to widen participation, and reduce the impact of losing on less able participants, there has been an introduction of non-competitive physical activity to traditionally competitive events such as school sports days, although moves like this are often controversial.In competitive events, participants are graded or classified based on their "result" and often divided into groups of comparable performance, (e.g. gender, weight and age). The measurement of the result may be objective or subjective, and corrected with "handicaps" or penalties. In a race, for example, the time to complete the course is an objective measurement. In gymnastics or diving the result is decided by a panel of judges, and therefore subjective. There are many shades of judging between boxing and mixed martial arts, where victory is assigned by judges if neither competitor has lost at the end of the match time. History Artifacts and structures suggest sport in China as early as 2000 BC. Gymnastics appears to have been popular in China's ancient past. Monuments to the Pharaohs indicate that a number of sports, including swimming and fishing, were well-developed and regulated several thousands of years ago in ancient Egypt. Other Egyptian sports included javelin throwing, high jump, and wrestling. Ancient Persian sports such as the traditional Iranian martial art of Zourkhaneh had a close connection to warfare skills. Among other sports that originate in ancient Persia are polo and jousting.A wide range of sports were already established by the time of Ancient Greece and the military culture and the development of sports in Greece influenced one another considerably. Sports became such a prominent part of their culture that the Greeks created the Olympic Games, which in ancient times were held every four years in a small village in the Peloponnesus called Olympia.Sports have been increasingly organised and regulated from the time of the ancient Olympics up to the present century. Industrialisation has brought increased leisure time, letting people attend and follow spectator sports and participate in athletic activities. These trends continued with the advent of mass media and global communication. Professionalism became prevalent, further adding to the increase in sport's popularity, as sports fans followed the exploits of professional athletes — all while enjoying the exercise and competition associated with amateur participation in sports. Since the turn of the 21st century, there has been increasing debate about whether transgender sportpersons should be able to participate in sport events that conform with their post-transition gender identity. Fair play  Sportsmanship Sportsmanship is an attitude that strives for fair play, courtesy toward teammates and opponents, ethical behaviour and integrity, and grace in victory or defeat.Sportsmanship expresses an aspiration or ethos that the activity will be enjoyed for its own sake. The well-known sentiment by sports journalist Grantland Rice, that it's "not that you won or lost but how you played the game", and the modern Olympic creed expressed by its founder Pierre de Coubertin: "The most important thing... is not winning but taking part" are typical expressions of this sentiment. Cheating Key principles of sport include that the result should not be predetermined, and that both sides should have equal opportunity to win. Rules are in place to ensure that fair play to occur, but participants can break these rules in order to gain advantage.Participants may choose to cheat in order to satisfy their desire to win, or in order to achieve an ulterior motive. The widespread existence of gambling on the results of sports fixtures creates the motivation for match fixing, where a participant or participants deliberately work to ensure a given outcome. Doping and drugs The competitive nature of sport encourages some participants to attempt to enhance their performance through the use of medicines, or through other means such as increasing the volume of blood in their bodies through artificial means.All sports recognised by the IOC or SportAccord are required to implement a testing programme, looking for a list of banned drugs, with suspensions or bans being placed on participants who test positive for banned substances. Violence Violence in sports involves crossing the line between fair competition and intentional aggressive violence. Athletes, coaches, fans, and parents sometimes unleash violent behaviour on people or property, in misguided shows of loyalty, dominance, anger, or celebration. Rioting or hooliganism by fans in particular is a problem at some national and international sporting contests. Participation  Gender participation Female participation in sports continues to rise alongside the opportunity for involvement and the value of sports for child development and physical fitness. Despite gains during the last three decades, a gap persists in the enrollment figures between male and female players. Female players account for 39% of the total participation in US interscholastic athletics. Gender balance has been accelerating from a 32% increase in 1973–74 to a 63% increase in 1994–95. Hessel (2000). Youth participation Youth sports present children with opportunities for fun, socialization, forming peer relationships, physical fitness, and athletic scholarships. Activists for education and the war on drugs encourage youth sports as a means to increase educational participation and to fight the illegal drug trade. According to the Center for Injury Research and Policy at Nationwide Children's Hospital, the biggest risk for youth sports is death or serious injury including concussion. These risks come from running, basketball, association football, volleyball, gridiron, gymnastics, and ice hockey. Disabled participation Disabled sports also adaptive sports or parasports, are sports played by persons with a disability, including physical and intellectual disabilities. As many of these based on existing sports modified to meet the needs of persons with a disability, they are sometimes referred to as adapted sports. However, not all disabled sports are adapted; several sports that have been specifically created for persons with a disability have no equivalent in able-bodied sports. Spectator involvement The competition element of sport, along with the aesthetic appeal of some sports, result in the popularity of people attending to watch sport being played. This has led to the specific phenomenon of spectator sport.Both amateur and professional sports attract spectators, both in person at the sport venue, and through broadcast mediums including radio, television and internet broadcast. Both attendance in person and viewing remotely can incur a sometimes substantial charge, such as an entrance ticket, or pay-per-view television broadcast.It is common for popular sports to attract large broadcast audiences, leading to rival broadcasters bidding large amounts of money for the rights to show certain fixtures. The football World Cup attracts a global television audience of hundreds of millions; the 2006 final alone attracted an estimated worldwide audience of well over 700 million and the 2011 Cricket World Cup Final attracted an estimated audience of 135 million in India alone .In the United States, the championship game of the NFL, the Super Bowl, has become one of the most watched television broadcasts of the year. Super Bowl Sunday is a de facto national holiday in America; the viewership being so great that in 2015, advertising space was reported as being sold at $4.5m for a 30-second slot. Issues and considerations  Amateur and professional Sport can be undertaken on an amateur, professional or semi-professional basis, depending on whether participants are incentivised for participation (usually through payment of a wage or salary). Amateur participation in sport at lower levels is often called "grassroots sport".The popularity of spectator sport as a recreation for non-participants has led to sport becoming a major business in its own right, and this has incentivised a high paying professional sport culture, where high performing participants are rewarded with pay far in excess of average wages, which can run into millions of dollars.Some sports, or individual competitions within a sport, retain a policy of allowing only amateur sport. The Olympic Games started with a principle of amateur competition with those who practiced a sport professionally considered to have an unfair advantage over those who practiced it merely as a hobby. From 1971, Olympic athletes were allowed to receive compensation and sponsorship, and from 1986, the IOC decided to make all professional athletes eligible for the Olympics, with the exceptions of boxing, and wrestling. Technology Technology plays an important part in modern sports. With it being a necessary part of some sports (such as motorsport), it is used in others to improve performance. Some sports also use it to allow off-field decision making.Sports science is a widespread academic discipline, and can be applied to areas including athlete performance, such as the use of video analysis to fine-tune technique, or to equipment, such as improved running shoes or competitive swimwear. Sports engineering emerged as a discipline in 1998 with an increasing focus not just on materials design but also the use of technology in sport, from analytics and big data to wearable technology. In order to control the impact of technology on fair play, governing bodies frequently have specific rules that are set to control the impact of technical advantage between participants. For example, in 2010, full-body, non-textile swimsuits were banned by FINA, as they were enhancing swimmers' performances.The increase in technology has also allowed many decisions in sports matches to be taken, or reviewed, off-field, with another official using instant replays to make decisions. In some sports, players can now challenge decisions made by officials. In football, Goal-line technology makes decisions on whether a ball has crossed the goal line or not. The technology is not compulsory, but was used in the 2014 FIFA World Cup in Brazil, and the 2015 FIFA Women's World Cup in Canada, as well as in the Premier League from 2013–14, and the Bundesliga from 2015–16. In the NFL, a referee can ask for a review from the replay booth, or a head coach can issue a challenge to review the play using replays. The final decision rests with the referee. A video referee (commonly known as a Television Match Official or TMO) can also use replays to help decision-making in rugby (both league and union). In international cricket, an umpire can ask the Third umpire for a decision, and the third umpire makes the final decision. Since 2008, a decision review system for players to review decisions has been introduced and used in ICC-run tournaments, and optionally in other matches. Depending on the host broadcaster, a number of different technologies are used during an umpire or player review, including instant replays, Hawk-Eye, Hot Spot and Real Time Snickometer. Hawk-Eye is also used in tennis to challenge umpiring decisions. Politics Sports and politics can influence each other greatly.Benito Mussolini used the 1934 FIFA World Cup, which was held in Italy, to showcase Fascist Italy. Adolf Hitler also used the 1936 Summer Olympics held in Berlin, and the 1936 Winter Olympics held in Garmisch-Partenkirchen, to promote the Nazi ideology of the superiority of the Aryan race, and inferiority of the Jews and other "undesirables". Germany used the Olympics to give of itself a peaceful image while it was very actively preparing the war.When apartheid was the official policy in South Africa, many sports people, particularly in rugby union, adopted the conscientious approach that they should not appear in competitive sports there. Some feel this was an effective contribution to the eventual demolition of the policy of apartheid, others feel that it may have prolonged and reinforced its worst effects.In the history of Ireland, Gaelic sports were connected with cultural nationalism. Until the mid 20th century a person could have been banned from playing Gaelic football, hurling, or other sports administered by the Gaelic Athletic Association (GAA) if she/he played or supported football, or other games seen to be of British origin. Until recently the GAA continued to ban the playing of football and rugby union at Gaelic venues. This ban, also known as Rule 42, is still enforced, but was modified to allow football and rugby to be played in Croke Park while Lansdowne Road was redeveloped into Aviva Stadium. Until recently, under Rule 21, the GAA also banned members of the British security forces and members of the RUC from playing Gaelic games, but the advent of the Good Friday Agreement in 1998 led to the eventual removal of the ban.Nationalism is often evident in the pursuit of sports, or in its reporting: people compete in national teams, or commentators and audiences can adopt a partisan view. On occasion, such tensions can lead to violent confrontation among players or spectators within and beyond the sporting venue, as in the Football War. These trends are seen by many as contrary to the fundamental ethos of sports being carried on for its own sake and for the enjoyment of its participants.A very famous case when sports and politics collided was the 1972 Olympics in Munich. Masked men entered the hotel of the Israeli olympic team and killed many of their men. This was known as the Munich massacre.A study of US elections has shown that the result of sports events can affect the results. A study published in the Proceedings of the National Academy of Sciences showed that when the home team wins the game before the election, the incumbent candidates can increase their share of the vote by 1.5 percent. A loss had the opposite effect, and the effect is greater for higher-profile teams or unexpected wins and losses. Also, when Washington Redskins win their final game before an election, then the incumbent President is more likely to win, and if the Redskins lose, then the opposition candidate is more likely to win; this has become known as the Redskins Rule. Sports as a means of controlling and subduing populations Étienne de La Boétie, in his essay Discourse on Voluntary Servitude describes athletic spectacles as means for tyrants to control their subjects by distracting them.Do not imagine that there is any bird more easily caught by decoy, nor any fish sooner fixed on the hook by wormy bait, than are all these poor fools neatly tricked into servitude by the slightest feather passed, so to speak, before their mouths. Truly it is a marvelous thing that they let themselves be caught so quickly at the slightest tickling of their fancy. Plays, farces, spectacles, gladiators, strange beasts, medals, pictures, and other such opiates, these were for ancient peoples the bait toward slavery, the price of their liberty, the instruments of tyranny. By these practices and enticements the ancient dictators so successfully lulled their subjects under the yoke, that the stupefied peoples, fascinated by the pastimes and vain pleasures flashed before their eyes, learned subservience as naïvely, but not so creditably, as little children learn to read by looking at bright picture books. Religious views on sports The practice of athletic competitions has been criticized by some Christian thinkers as a form of idolatry, in which "human beings extol themselves, adore themselves, sacrifice themselves and reward themselves." Sports are seen by these critics as a manifestation of "collective pride" and "national self-deification" in which feats of human power are idolized at the expense of divine worship.Tertullian condemns the athletic performances of his day, insisting "the entire apparatus of the shows is based upon idolatry." The shows, says Tertullian, excite passions foreign to the calm temperament cultivated by the Christian:God has enjoined us to deal calmly, gently, quietly, and peacefully with the Holy Spirit, because these things are alone in keeping with the goodness of His nature, with His tenderness and sensitiveness. ... Well, how shall this be made to accord with the shows? For the show always leads to spiritual agitation, since where there is pleasure, there is keenness of feeling giving pleasure its zest; and where there is keenness of feeling, there is rivalry giving in turn its zest to that. Then, too, where you have rivalry, you have rage, bitterness, wrath and grief, with all bad things which flow from them—the whole entirely out of keeping with the religion of Christ. See also Related topics References European Commission (2007), The White Paper on Sport.Council of Europe (2001), The Europien sport charter. Further reading The Meaning of Sports by Michael Mandel (PublicAffairs, ISBN 1-58648-252-1).Journal of the Philosophy of SportSullivan, George. The Complete Sports Dictionary. New York: Scholastic Book Services, 1979. 199 p. ISBN 0-590-05731-6 External links 
A rocket (from Italian rocchetto "bobbin") is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket before use. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space.In fact, rockets work more efficiently in space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, and/or gravity.Rockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Earth's moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellants), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react, a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks. History The first gunpowder-powered rockets were developed in Song China, by the 13th century. The Chinese rocket technology was adopted by the Mongols and the invention was spread via the Mongol invasions to the Near East and Europe in the mid 13th century. Medieval and early modern rockets were used militarily as incendiary weapons in sieges.An early Chinese text to mention the use of rockets was the Huolongjing, written by the Chinese artillery officer Jiao Yu in the mid-14th century. Between 1270 and 1280, Hasan al-Rammah wrote al-furusiyyah wa al-manasib al-harbiyya (The Book of Military Horsemanship and Ingenious War Devices), which included 107 gunpowder recipes, 22 of which are for rockets. In Europe, Konrad Kyeser described rockets in his military treatise Bellifortis around 1405.The name Rocket comes from the Italian rocchetta, meaning "bobbin" or "little spindle", given due to the similarity in shape to the bobbin or spool used to hold the thread to be fed to a spinning wheel. The Italian term was adopted into German in the mid 16th century by Leonhard Fronsperger and Conrad Haas, and by the early 17th century into English. Artis Magnae Artilleriae pars prima, an important early modern work on rocket artillery, by Kazimierz Siemienowicz, was first printed in Amsterdam in 1650.The first iron-cased rockets were developed in the late 18th century in the Kingdom of Mysore. In 1814, Francis Scott Key wrote the line "rockets' red glare" while held captive on a British ship that was laying siege to Fort McHenry. The rockets he witnessed were an invention of William Congreve, who built a compressed-powder rocket encased in metal, increasing the effective range from 100 to 2,000 yards, first used in the Napoleanic Wars. The first mathematical treatment of the dynamics of rocket propulsion is due to William Moore (1813). In 1815, Alexander Dmitrievich Zasyadko constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices.William Hale in 1844 greatly increased the accuracy of rocket artillery. The Congreve rocket was further improved by Edward Mounier Boxer in 1865.Konstantin Tsiolkovsky (1903) first speculated on the possibility of manned spaceflight with rocket technology. Robert Goddard in 1920 published proposed improvements to rocket technology in A Method of Reaching Extreme Altitudes. In 1923, Hermann Oberth (1894–1989) published Die Rakete zu den Planetenräumen ("The Rocket into Planetary Space")Modern rockets originated when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-propellant rocket. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. Use of liquid propellants instead of gunpowder greatly improved the effectiveness of rocket artillery in World War II, and opened up the possibility of manned spaceflight after 1945.In 1943, production of the V-2 rocket began in Germany. In parallel with the guided missile programme, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 "Natter") or for powering them (Me 163, see list of World War II guided missiles of Germany). The Allies' rocket programs were less sophisticated, relying mostly on unguided missiles like the Soviet Katyusha rocket. The Americans captured a large number of German rocket scientists, including Wernher von Braun, and brought them to the United States as part of Operation Paperclip. After the war, rockets were used to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further research; notably the Bell X-1, the first manned vehicle to break the sound barrier. Independently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev.During the Cold War, rockets became extremely important militarily as modern intercontinental ballistic missiles (ICBMs). The 1960s became the decade of rapid development of rocket technology particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15). Rockets were now used for space exploration, with the American manned programs Project Mercury, Project Gemini and later the Apollo programme culminated in 1969 with the first manned landing on the moon via the Saturn V. Types Vehicle configurationsRocket vehicles are often constructed in the archetypal tall thin "rocket" shape that takes off vertically, but there are actually many different types of rockets including:tiny models such as balloon rockets, water rockets, skyrockets or small solid rockets that can be purchased at a hobby storemissilesspace rockets such as the enormous Saturn V used for the Apollo programrocket carsrocket bikerocket-powered aircraft (including rocket assisted takeoff of conventional aircraft- RATO)rocket sledsrocket trainsrocket torpedoesrocket-powered jet packsrapid escape systems such as ejection seats and launch escape systemsspace probes Design A rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion. Components Rockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.As well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems. Engines Rocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types; a comprehensive list can be found in rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force ("thrust") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket. Propellant Rocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.Sometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.Alternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.For smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle. Uses Rockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.However, they are also useful in other situations: Military Some military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a missile when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a rocket if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military. Science and research Sounding rockets are commonly used to carry instruments that take readings from 50 kilometers (31 mi) to 1,500 kilometers (930 mi) above the surface of the Earth.Rocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5. Spaceflight Larger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity—2,500 to 4,500 m/s (9,000 to 16,200 km/h; 5,600 to 10,100 mph)—rockets are particularly useful when very high speeds are required, such as orbital speed at approximately 7,800 m/s (28,000 km/h; 17,000 mph). Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket). Rescue Rockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.Some crewed rockets, notably the Saturn V and Soyuz have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.This was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit unmanned, was saved from destruction. It should be noted that only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.A successful escape of a manned capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.Solid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost. Hobby, sport, and entertainment Hobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A Water Powered Rocket is a type of model rocket using water as its reaction mass. The pressure vessel (the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.The scale of amateur rocketry can range from a small rocket launched in your own backyard to a rocket that reached space. Amateur rocketry is split into three categories: low power, mid power, and high power.Australia, Austria, Canada, Germany, New Zealand, Switzerland, the United Kingdom, and the United States have high power rocket associations which provide certifications to its members to fly different rocket motor sizes. While joining these organizations is not a requirement, they often provide insurance and flight waivers for their members.Hydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.Corpulent Stump is the most powerful non commercial rocket ever launched on an Aerotech engine in the United Kingdom. Noise Rocket exhaust generates a significant amount of acoustic energy. As the supersonic exhaust collides with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust velocity. The sound intensity of large, high performance rockets could potentially kill at close range.The Space Shuttle generates 180 dB of noise around its base. To combat this, NASA developed a sound suppression system which can flow water at rates up to 900,000 gallons per minute (57 m3/s) onto the launch pad. The water reduces the noise level from 180 dB down to 142 dB (the design requirement is 145 dB). Without the sound suppression system, acoustic waves reflect off of the launch pad towards the rocket, vibrating the sensitive payload and crew. These acoustic waves can be so severe that they can destroy the rocket.Noise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.For crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle. Physics  Operation The effect of the combustion of propellant in the rocket engine is to increase the velocity of the resulting gases to very high speeds, hence producing a thrust. Initially, the gases of combustion are sent in every direction, but only those that produce a net thrust have any effect. The ideal direction of motion of the exhaust is in the direction so as to cause thrust. At the top end of the combustion chamber the hot, energetic gas fluid cannot move forward, and so, it pushes upward against the top of the rocket engine's combustion chamber. As the combustion gases approach the exit of the combustion chamber, they increase in speed. The effect of the convergent part of the rocket engine nozzle on the high pressure fluid of combustion gases, is to cause the gases to accelerate to high speed. The higher the speed of the gases, the lower the pressure of the gas (Bernoulli's principle or conservation of energy) acting on that part of the combustion chamber. In a properly designed engine, the flow will reach Mach 1 at the throat of the nozzle. At which point the speed of the flow increases. Beyond the throat of the nozzle, a bell shaped expansion part of the engine allows the gases that are expanding to push against that part of the rocket engine. Thus, the bell part of the nozzle gives additional thrust. Simply expressed, for every action there is an equal and opposite reaction, according to Newton's third law with the result that the exiting gases produce the reaction of a force on the rocket causing it to accelerate the rocket.In a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.The shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is pulled by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that in the case of liquid propellant engines, the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber -typically on the order of 100 atmospheres.As a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's specific impulse. Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass. Forces on a rocket in flight The general study of the forces on a rocket is part of the field of ballistics. Spacecraft are further studied in the subfield of astrodynamics.Flying rockets are primarily affected by the following:Thrust from the engine(s)Gravity from celestial bodiesDrag if moving in atmosphereLift; usually relatively small effect except for rocket-powered aircraftRockets that must travel through the air are usually tall and thin as this shape gives a high ballistic coefficient and minimizes drag losses.In addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.These forces, with a stabilizing tail (the empennage) present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle. Drag Drag is a force opposite to the direction of the rocket's motion. This decreases acceleration of the vehicle and produces structural loads. Deceleration force for fast-moving rockets are calculated using the drag equation.Drag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the "classic" rocket shape—long and thin), and by keeping the rocket's angle of attack as low as possible.During a rocket launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called Max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces. Net thrust A typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.It can be shown that the net thrust of a rocket is:                              F                      n                                                                      m              ˙                                                        v                      e                                {\displaystyle F_{n}{\dot {m}}\;v_{e}}  where:                                                        m              ˙                                                            {\displaystyle {\dot {m}}\,}  propellant flow (kg/s or lb/s)                              v                      e                                                {\displaystyle v_{e}\,}  the effective exhaust velocity (m/s or ft/s)The effective exhaust velocity                               v                      e                                {\displaystyle v_{e}}   is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.The rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of g-forces due to the reduction in propellant load. Total impulse Impulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or "power". The total impulse of a rocket (stage) burning its propellant is:                    I                ∫        F        d        t              {\displaystyle I\int Fdt}  When there is fixed thrust, this is simply:                    I                F        t                      {\displaystyle IFt\;}  The total impulse of a multi-stage rocket is the sum of the impulses of the individual stages. Specific impulse As can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.An equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse,                               I                      s            p                                {\displaystyle I_{sp}}  , and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:                              v                      e                                            I                      s            p                          ⋅                  g                      0                                {\displaystyle v_{e}I_{sp}\cdot g_{0}}  where:                              I                      s            p                                {\displaystyle I_{sp}}   has units of seconds                              g                      0                                {\displaystyle g_{0}}   is the acceleration at the surface of the EarthThus, the greater the specific impulse, the greater the net thrust and performance of the engine.                               I                      s            p                                {\displaystyle I_{sp}}   is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air. Delta-v (rocket equation) The delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).When                               v                      e                                {\displaystyle v_{e}}   is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:                    Δ        v                                   v                      e                          ln        ⁡                                            m                              0                                                    m                              1                                                          {\displaystyle \Delta v\ v_{e}\ln {\frac {m_{0}}{m_{1}}}}  where:                              m                      0                                {\displaystyle m_{0}}   is the initial total mass, including propellant, in kg (or lb)                              m                      1                                {\displaystyle m_{1}}   is the final total mass in kg (or lb)                              v                      e                                {\displaystyle v_{e}}   is the effective exhaust velocity in m/s (or ft/s)                    Δ        v                       {\displaystyle \Delta v\ }   is the delta-v in m/s (or ft/s)When launched from the Earth practical delta-vs for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-vs over 9 km/s.The required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to Low earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.The ratio                                                         m                              0                                                    m                              1                                                          {\displaystyle {\frac {m_{0}}{m_{1}}}}   is sometimes called the mass ratio. Mass ratios Almost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and its final mass. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.Rockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20g.Achievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.The highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.Some notable mass fractions are found in the following table (some aircraft are included for comparison purposes): Staging Thus far, the required velocity (delta-v) to achieve orbit has been unattainable by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v. Since Single-stage-to-orbit has so far not been achievable, orbital rockets always have more than one stage.For example, the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.This problem is frequently solved by staging — the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either serial where the rockets light after the previous stage has fallen away, or parallel, where rockets are burning together and then detach when they burn out.The maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive. Acceleration and thrust-to-weight ratio From Newton's second law, the acceleration,                     a              {\displaystyle a}  , of a vehicle is simply:                    a                                                    F                              n                                      m                                {\displaystyle a{\frac {F_{n}}{m}}}  Where m is the instantaneous mass of the vehicle and                               F                      n                                {\displaystyle F_{n}}   is the net force acting on the rocket (mostly thrust but air drag and other forces can play a part.)As the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.Peak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.The minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine, some solid rockets are over 1000), and nearly all really high-g vehicles employ or have employed rockets.The high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff, and in some cases, with suitable guidance and control of the engines, also vertical landing. For these operations to be done it is necessary for a vehicle's engines to provide more than the local gravitational acceleration. Energy  Energy efficiency Rocket launch vehicles take-off with a great deal of flames, noise and drama, and it might seem obvious that they are grievously inefficient. However, while they are far from perfect, their energy efficiency is not as bad as might be supposed.The energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.Energy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.In a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:                              η                      c                                                                                                        1                  2                                                                                                  m                    ˙                                                                              v                                  e                                                  2                                                                                    η                                  c                  o                  m                  b                  u                  s                  t                  i                  o                  n                                                            P                                  c                  h                  e                  m                                                                          {\displaystyle \eta _{c}{\frac {{\frac {1}{2}}{\dot {m}}v_{e}^{2}}{\eta _{combustion}P_{chem}}}}  100% efficiency within the engine (engine efficiency                               η                      c                                  100        %              {\displaystyle \eta _{c}100\%}  ) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered. The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.However, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency (                              η                      p                                {\displaystyle \eta _{p}}  ).However, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up travelling forwards- trailing behind the vehicle.From these principles it can be shown that the propulsive efficiency                               η                      p                                {\displaystyle \eta _{p}}   for a rocket moving at speed                     u              {\displaystyle u}   with an exhaust velocity                     c              {\displaystyle c}   is:                              η                      p                                                                      2                                                u                  c                                                                    1              +              (                                                u                  c                                                            )                                  2                                                                          {\displaystyle \eta _{p}{\frac {2{\frac {u}{c}}}{1+({\frac {u}{c}})^{2}}}}  And the overall (instantaneous) energy efficiency                     η              {\displaystyle \eta }   is:                    η                          η                      p                                    η                      c                                {\displaystyle \eta \eta _{p}\eta _{c}}  For example, from the equation, with an                               η                      c                                {\displaystyle \eta _{c}}   of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.Since the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example, NASA's space shuttle fires its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle has a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle is about 16% energy efficient at launching the orbiter.Thus jet engines, with a better match between speed and jet exhaust speed (such as turbofans—in spite of their worse                               η                      c                                {\displaystyle \eta _{c}}  )—dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range relatively low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations. Oberth effect One subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.This fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory. Safety, reliability and accidents The reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.Because of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some problems. In 1986, following the Space Shuttle Challenger disaster, American physicist Richard Feynman, having served on the Rogers Commission estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%. Costs and economics The costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.Most of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about $1/kg [$0.45/lb] or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing $0.15 per kilogram ($0.068/lb) and liquid hydrogen $2.20/kg ($1.00/lb), the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).Even though a rocket's non-propellant, dry mass is often only between 5-20% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000–$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.Extreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent amortization of R&D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.To change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or reducing rocket performance requirements by constructing a hypothetical non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with most methods involving some rocket use).The costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.Rockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive. See also  Notes  External links Governing agenciesFAA Office of Commercial Space TransportationNational Aeronautics and Space Administration (NASA)National Association of Rocketry (USA)Tripoli Rocketry AssociationAsoc. Coheteria Experimental y Modelista de ArgentinaUnited Kingdom Rocketry AssociationIMR - German/Austrian/Swiss Rocketry AssociationCanadian Association of RocketryIndian Space Research OrganisationInformation sitesEncyclopedia Astronautica - Rocket and Missile Alphabetical IndexRocket and Space TechnologyGunter's Space Page - Complete Rocket and Missile ListsRocketdyne Technical ArticlesRelativity Calculator - Learn Tsiolkovsky's rocket equationsRobert Goddard--America's Space Pioneer
A business (also known as an enterprise, a company, or a firm) is an organizational entity involved in the provision of goods and services to consumers. Businesses serve as a form of economic activity, and are prevalent in capitalist economies, where most of them are privately owned and provide goods and services allocated through a market to consumers and customers in exchange for other goods, services, money, or other forms of exchange that hold intrinsic economic value. Businesses may also be social non-profit enterprises or state-owned public enterprises operated by governments with specific social and economic objectives. A business owned by multiple private individuals may form as an incorporated company or jointly organise as a partnership. Countries have different laws that may ascribe different rights to the various business entities.The word "business" can refer to a particular organization or to an entire market sector (for example: "the financial sector") or to the sum of all economic activity ("the business sector"). Compound forms such as "agribusiness" represent subsets of the concept's broader meaning, which encompasses all activity by suppliers of goods and services.Businesses aim to maximize sales to have their income exceed their expenditures, resulting in a profit, gain or surplus. Basic forms of ownership Forms of business ownership vary by jurisdiction, but several common entities exist:Sole proprietorship: A sole proprietorship, also known as a sole trader, is owned by one person and operates for their benefit. The owner operates the business alone and may hire employees. A sole proprietor has unlimited liability for all obligations incurred by the business, whether from operating costs or judgements against the business. All assets of the business belong to a sole proprietor, including, for example, computer infrastructure, any inventory, manufacturing equipment, or retail fixtures, as well as any real property owned by the sole proprietor.Partnership: A partnership is a business owned by two or more people. In most forms of partnerships, each partner has unlimited liability for the debts incurred by the business. The three most prevalent types of for-profit partnerships are: general partnerships, limited partnerships, and limited liability partnerships.Corporation: The owners of a corporation have limited liability and the business has a separate legal personality from its owners. Corporations can be either government-owned or privately owned. They can organize either for profit or as nonprofit organizations. A privately owned, for-profit corporation is owned by its shareholders, who elect a board of directors to direct the corporation and hire its managerial staff. A privately owned, for-profit corporation can be either privately held by a small group of individuals, or publicly held, with publicly traded shares listed on a stock exchange.Cooperative: Often referred to as a "co-op", a cooperative is a limited-liability business that can organize as for-profit or not-for-profit. A cooperative differs from a corporation in that it has members, not shareholders, and they share decision-making authority. Cooperatives are typically classified as either consumer cooperatives or worker cooperatives. Cooperatives are fundamental to the ideology of economic democracy.Limited liability companies (LLC), limited liability partnerships, and other specific types of business organization protect their owners or shareholders from business failure by doing business under a separate legal entity with certain legal protections. In contrast, unincorporated businesses or persons working on their own are usually not as protected.Franchises: A franchise is a system in which entrepreneurs purchase the rights to open and run a business from a larger corporation. Franchising in the United States is widespread and is a major economic powerhouse. One out of twelve retail businesses in the United States are franchised and 8 million people are employed in a franchised business. Classifications Agriculture such as the domestication of fish, animals and livestock, as well as lumber, oil and mining businesses that extract natural resources and raw materials, such as wood, petroleum, natural gas, ores, plants or minerals.Financial services businesses include banks, brokerage firms, credit unions, credit cards, insurance companies, asset and investment companies such as private equity firms, real estate investment trusts, sovereign wealth funds, pension funds, mutual funds, index funds, and hedge funds, stock exchanges, and other companies that generate profits through investment and management of capital.Entertainment and mass media companies generate profits primarily from the sale of intellectual property – they include film studios and production houses, mass media companies such as cable television networks, online digital media agencies, mobile media outlets, newspapers, book and magazine publishing houses.Industrial manufacturers produce products, either from raw materials or from component parts, then export the finished products at a profit - they include tangible goods such as cars, glass, or aircraft.Real estate businesses sell, invest, construct and develop properties – including land, residential homes, and other buildings.Retailers, wholesalers, and distributors act as middlemen and get goods produced by manufacturers to the intended consumers; they make their profits by marking up their prices. Most stores and catalog companies are distributors or retailers.Transportation businesses such as railways, airlines, shipping companies that deliver goods and individuals to their destinations for a fee.Utilities produce public services such as electricity, waste management or sewage treatment, usually under the charge of a government.Service businesses offer intangible goods or services and typically charge for labor or other services provided to government, to consumers, or to other businesses. Interior decorators, hairstylists, tanning salons, laundromats, and pest controllers are service businesses. Management The efficient and effective operation of a business, and study of this subject, is called management. The major branches of management are financial management, marketing management, human resource management, strategic management, production management, operations management, service management, and information technology management.Owners may manage their businesses themselves, or employ managers to do so for them. Whether they are owners or employees, managers administer three primary components of the business' value: financial resources, capital (tangible resources), and human resources. These resources are administered in at least five functional areas: legal contracting, manufacturing or service production, marketing, accounting, financing, and human resources. Restructuring state enterprises In recent decades, states modeled some of their assets and enterprises after business enterprises. In 2003, for example, the People's Republic of China modeled 80% of its state-owned enterprises on a company-type management system. Many state institutions and enterprises in China and Russia have transformed into joint-stock companies, with part of their shares being listed on public stock markets.Business process management (BPM) is a holistic management approach focused on aligning all aspects of an organization with the wants and needs of clients. It promotes business effectiveness and efficiency while striving for innovation, flexibility, and integration with technology. BPM attempts to improve processes continuously. It can therefore be described as a "process optimization process." It is argued that BPM enables organizations to be more efficient, effective and capable of change than a functionally focused, traditional hierarchical management approach. Organization and regulation Most legal jurisdictions specify the forms of ownership that a business can take, creating a body of commercial law for each type.The major factors affecting how a business is organized are usually:The size and scope of the business firm and its structure, management, and ownership, broadly analyzed in the theory of the firm. Generally, a smaller business is more flexible, while larger businesses, or those with wider ownership or more formal structures, will usually tend to be organized as corporations or (less often) partnerships. In addition, a business that wishes to raise money on a stock market or to be owned by a wide range of people will often be required to adopt a specific legal form to do so.The sector and country. Private profit-making businesses are different from government-owned bodies. In some countries, certain businesses are legally obliged to be organized in certain ways.Tax advantages. Different structures are treated differently in tax law, and may have advantages for this reason.Disclosure and compliance requirements. Different business structures may be required to make less or more information public (or report it to relevant authorities), and may be bound to comply with different rules and regulations.Many businesses are operated through a separate entity such as a corporation or a partnership (either formed with or without limited liability). Most legal jurisdictions allow people to organize such an entity by filing certain charter documents with the relevant Secretary of State or equivalent, and complying with certain other ongoing obligations. The relationships and legal rights of shareholders, limited partners, or members are governed partly by the charter documents and partly by the law of the jurisdiction where the entity is organized. Generally speaking, shareholders in a corporation, limited partners in a limited partnership, and members in a limited liability company are shielded from personal liability for the debts and obligations of the entity, which is legally treated as a separate "person". This means that unless there is misconduct, the owner's own possessions are strongly protected in law if the business does not succeed.Where two or more individuals own a business together but have failed to organize a more specialized form of vehicle, they will be treated as a general partnership. The terms of a partnership are partly governed by a partnership agreement if one is created, and partly by the law of the jurisdiction where the partnership is located. No paperwork or filing is necessary to create a partnership, and without an agreement, the relationships and legal rights of the partners will be entirely governed by the law of the jurisdiction where the partnership is located. A single person who owns and runs a business is commonly known as a sole proprietor, whether that person owns it directly or through a formally organized entity. Depending on the business needs, an adviser can decide what kind is proprietorship will be most suitable.A few relevant factors to consider in deciding how to operate a business include:General partners in a partnership (other than a limited liability partnership), plus anyone who personally owns and operates a business without creating a separate legal entity, are personally liable for the debts and obligations of the business.Generally, corporations are required to pay tax just like "real" people. In some tax systems, this can give rise to so-called double taxation, because first the corporation pays tax on the profit, and then when the corporation distributes its profits to its owners, individuals have to include dividends in their income when they complete their personal tax returns, at which point a second layer of income tax is imposed.In most countries, there are laws which treat small corporations differently from large ones. They may be exempt from certain legal filing requirements or labor laws, have simplified procedures in specialized areas, and have simplified, advantageous, or slightly different tax treatment."Going public" through a process known as an initial public offering (IPO) means that part of the business will be owned by members of the public. This requires the organization as a distinct entity, to disclose information to the public, and adhering to a tighter set of laws and procedures. Most public entities are corporations that have sold shares, but increasingly there are also public LLC's that sell units (sometimes also called shares), and other more exotic entities as well, such as, for example, real estate investment trusts in the USA, and unit trusts in the UK. A general partnership cannot "go public." Commercial law A very detailed and well-established body of rules that evolved over a very long period of time applies to commercial transactions. The need to regulate trade and commerce and resolve business disputes helped shape the creation of law and courts. The Code of Hammurabi dates back to about 1772 BC for example, and contains provisions that relate, among other matters, to shipping costs and dealings between merchants and brokers. The word "corporation" derives from the Latin corpus, meaning body, and the Maurya Empire in Iron-Age India accorded legal rights to business entities.In many countries, it is difficult to compile all the laws that can affect a business into a single reference source. Laws can govern treatment of labour and employee relations, worker protection and safety, discrimination on the basis of age, gender, disability, race, and in some jurisdictions, sexual orientation, and the minimum wage, as well as unions, worker compensation, and working hours and leave.Some specialized businesses may also require licenses, either due to laws governing entry into certain trades, occupations or professions, that require special education, or to raise revenue for local governments. Professions that require special licenses include law, medicine, piloting aircraft, selling liquor, radio broadcasting, selling investment securities, selling used cars, and roofing. Local jurisdictions may also require special licenses and taxes just to operate a business.Some businesses are subject to ongoing special regulation, for example, public utilities, investment securities, banking, insurance, broadcasting, aviation, and health care providers. Environmental regulations are also very complex and can affect many businesses. Capital When businesses need to raise money (called capital), they sometimes offer securities for sale.Capital may be raised through private means, by an initial public offering or IPO on a stock exchange, or in other ways.Major stock exchanges include the Shanghai Stock Exchange, Singapore Exchange, Hong Kong Stock Exchange, New York Stock Exchange and NASDAQ (USA), the London Stock Exchange (UK), the Tokyo Stock Exchange (Japan), and Bombay Stock Exchange (India). Most countries with capital markets have at least one.Businesses that have gone public are subject to regulations concerning their internal governance, such as how executive officers' compensation is determined, and when and how information is disclosed to shareholders and to the public. In the United States, these regulations are primarily implemented and enforced by the United States Securities and Exchange Commission (SEC). Other western nations have comparable regulatory bodies. The regulations are implemented and enforced by the China Securities Regulation Commission (CSRC) in China. In Singapore, the regulation authority is the Monetary Authority of Singapore (MAS), and in Hong Kong, it is the Securities and Futures Commission (SFC).The proliferation and increasing complexity of the laws governing business have forced increasing specialization in corporate law. It is not unheard of for certain kinds of corporate transactions to require a team of five to ten attorneys due to sprawling regulation. Commercial law spans general corporate law, employment and labor law, health-care law, securities law, mergers and acquisitions, tax law, employee benefit plans, food and drug regulation, intellectual property law on copyrights, patents, trademarks, telecommunications law, and financing.Other types of capital sourcing includes crowd sourcing on the Internet, venture capital, bank loans, and debentures. Intellectual property Businesses often have important "intellectual property" that needs protection from competitors for the company to stay profitable. This could require patents, copyrights, trademarks, or preservation of trade secrets. Most businesses have names, logos, and similar branding techniques that could benefit from trademarking. Patents and copyrights in the United States are largely governed by federal law, while trade secrets and trademarking are mostly a matter of state law. Because of the nature of intellectual property, a business needs protection in every jurisdiction in which they are concerned about competitors. Many countries are signatories to international treaties concerning intellectual property, and thus companies registered in these countries are subject to national laws bound by these treaties. In order to protect trade secrets, companies may require employees to sign non-compete clauses which will impose limitations on an employee's interactions with stakeholders, and competitors. See also  References  Media related to Business at Wikimedia Commons
Blackjack, also known as twenty-one, is the most widely played casino banking game in the world. Blackjack is a comparing card game between a player and dealer, meaning players compete against the dealer but not against other players. It is played with one or more decks of 52 cards. The objective of the game is to beat the dealer in one of the following ways:Get 21 points on the player's first two cards (called a "blackjack" or "natural"), without a dealer blackjack;Reach a final score higher than the dealer without exceeding 21; orLet the dealer draw additional cards until his or her hand exceeds 21.The player or players are dealt a two-card hand and add together the value of their cards. Face cards (kings, queens, and jacks) are counted as ten points. A player and the dealer can count an ace as 1 point or 11 points. All other cards are counted as the numeric value shown on the card. After receiving their first two cards, players have the option of getting a "hit", or taking an additional card. In a given round, the player or the dealer wins by having a score of 21 or by having the higher score that is less than 21. Scoring higher than 21 (called "busting" or "going bust") results in a loss. A player may win by having any final score equal to or less than 21 if the dealer busts. If a player holds an ace valued as 11, the hand is called "soft", meaning that the player cannot go bust by taking an additional card; 11 plus the value of any other card will always be less than or equal to 21. Otherwise, the hand is "hard".The dealer must hit until the cards total 17 or more points. (At many tables the dealer also hits on a "soft" 17, i.e. a hand containing an ace and one or more other cards totaling six.) Players win by not busting and having a total higher than the dealer's. The dealer loses by busting or having a lesser hand than the player who has not busted. If the player and dealer have the same total, this is called a "push", and the player typically does not win or lose money on that hand. If all available players bust, the hand ends automatically without the dealer having to play his or her hand.Blackjack has many rule variations. Since the 1960s, blackjack has been a high-profile target of advantage players, particularly card counters, who track the profile of cards that have been dealt and adapt their wagers and playing strategies accordingly.Blackjack has inspired other casino games, including Spanish 21 and pontoon. History Blackjack's precursor was twenty-one, a game of unknown origin. The first written reference is found in a book by the Spanish author Miguel de Cervantes, most famous for writing Don Quixote. Cervantes was a gambler, and the main characters of his tale "Rinconete y Cortadillo", from Novelas Ejemplares, are a couple of cheats working in Seville. They are proficient at cheating at ventiuna (Spanish for twenty-one), and state that the object of the game is to reach 21 points without going over and that the ace values 1 or 11. The game is played with the Spanish baraja deck, which lacks eights and nines. This short story was written between 1601 and 1602, implying that ventiuna was played in Castilla since the beginning of the 17th century or earlier. Later references to this game are found in France and Spain.When twenty-one was introduced in the United States, gambling houses offered bonus payouts to stimulate players' interest. One such bonus was a ten-to-one payout if the player's hand consisted of the ace of spades and a black jack (either the jack of clubs or the jack of spades). This hand was called a "blackjack", and the name stuck to the game even though the ten-to-one bonus was soon withdrawn. In the modern game, a blackjack refers to any hand of an ace plus a ten or face card regardless of suits or colours. Rules of play at casinos At a casino blackjack table, the dealer faces five to seven playing positions from behind a semicircular table. Between one and eight standard 52-card decks are shuffled together. At the beginning of each round, up to three players can place their bets in the "betting box" at each position in play. That is, there could be up to three players at each position at a table in jurisdictions that allow back betting. The player whose bet is at the front of the betting box is deemed to have control over the position, and the dealer will consult the controlling player for playing decisions regarding the hand; the other players of that box are said to "play behind". Any player is usually allowed to control or bet in as many boxes as desired at a single table, but it is prohibited for an individual to play on more than one table at a time or to place multiple bets within a single box. In many U.S. casinos, however, players are limited to playing two or three positions at a table and often only one person is allowed to bet on each position.The dealer deals cards from his/her left (the position on the dealer's far left is often referred to as "first base") to his/her far right ("third base"). Each box is dealt an initial hand of two cards visible to the people playing on it, and often to any other players. The dealer's hand receives its first card face up, and in "hole card" games immediately receives its second card face down (the hole card), which the dealer peeks at but does not reveal unless it makes the dealer's hand a blackjack. Hole card games are sometimes played on tables with a small mirror or electronic sensor that is used to peek securely at the hole card. In European casinos, "no hole card" games are prevalent; the dealer's second card is neither drawn nor consulted until the players have all played their hands.Cards are dealt either from one or two handheld decks, from a dealer's shoe, or from a shuffling machine. Single cards are dealt to each wagered-on position clockwise from the dealer's left, followed by a single card to the dealer, followed by an additional card to each of the positions in play. The players' initial cards may be dealt face up or face down (more common in single-deck games).The players' object is to win money by creating card totals that turn out to be higher than the dealer's hand but do not exceed 21 ("busting"/"breaking"), or alternatively by allowing the dealer to take additional cards until he/she busts. On their turn, players must choose whether to "hit" (take a card), "stand" (end their turn), "double" (double wager, take a single card and finish), "split" (if the two cards have the same value, separate them to make two hands) or "surrender" (give up a half-bet and retire from the game). Number cards count as their natural value; the jack, queen, and king (also known as "face cards" or "pictures") count as 10; aces are valued as either 1 or 11 according to the player's choice. If the hand value exceeds 21 points, it busts, and all bets on it are immediately forfeit. After all boxes have finished playing, the dealer's hand is resolved by drawing cards until the hand busts or achieves a value of 17 or higher (a dealer total of 17 including an ace, or "soft 17", must be drawn to in some games and must stand in others). The dealer never doubles, splits, or surrenders. If the dealer busts, all remaining player hands win. If the dealer does not bust, each remaining bet wins if its hand is higher than the dealer's, and loses if it is lower. If a player receives 21 on the 1st and 2nd card it is considered a "natural 21" or "blackjack" and the player is paid out immediately unless dealer also has a natural, in which case the hand ties. In the case of a tied score, known as "push" or "standoff", bets are normally returned without adjustment; however, a blackjack beats any hand that is not a blackjack, even one with a value of 21. An outcome of blackjack vs. blackjack results in a push. Wins are paid out at 1:1, or equal to the wager, except for winning blackjacks, which are traditionally paid at 3:2 (meaning the player receives three dollars for every two bet), or one-and-a-half times the wager. Many casinos today pay blackjacks at less than 3:2 at some tables; for instance, single-deck blackjack tables often pay 6:5 for a blackjack instead of 3:2.Blackjack games almost always provide a side bet called insurance, which may be played when dealer's upcard is an ace. Additional side bets, such as "Dealer Match" which pays when the player's cards match the dealer's up card, are sometimes available. Player decisions After receiving an initial two cards, the player has up to four standard options: "hit", "stand", "double down", or "split". Each option has a corresponding hand signal. Some games give the player a fifth option, "surrender".Hit: Take another card from the dealer.Signal: Scrape cards against table (in handheld games); tap the table with finger or wave hand toward body (in games dealt face up).Stand: Take no more cards, also known as "stand pat", "stick", or "stay".Signal: Slide cards under chips (in handheld games); wave hand horizontally (in games dealt face up).Double down: The player is allowed to increase the initial bet by up to 100% in exchange for committing to stand after receiving exactly one more card. The additional bet is placed in the betting box next to the original bet. Some games do not permit the player to increase the bet by amounts other than 100%. Non-controlling players may double their wager or decline to do so, but they are bound by the controlling player's decision to take only one card.Signal: Place additional chips beside the original bet outside the betting box, and point with one finger.Split: If the first two cards of a hand have the same value, the player can split them into two hands, by moving a second bet equal to the first into an area outside the betting box. The dealer separates the two cards and draws an additional card on each, placing one bet with each hand. The player then plays out the two separate hands in turn; except for a few restrictions, the hands are treated as independent new hands, with the player winning or losing their wager separately for each hand. Occasionally, in the case of ten-valued cards, some casinos allow splitting only when the cards have the identical ranks; for instance, a hand of 10-10 may be split, but not one of 10-king. However, usually all 10-value cards are treated the same. Doubling and further splitting of post-split hands may be restricted, and blackjacks after a split are counted as non-blackjack 21 when comparing against the dealer's hand. Hitting split aces is usually not allowed. Non-controlling players may follow the controlling player by putting down an additional bet or decline to do so, instead associating their existing wager with one of the two post-split hands. In that case they must choose which hand to play behind before the second cards are drawn. Some casinos do not give non-controlling players this option, and require that the wager of a player not electing to split remains with the first of the two post-split hands.Signal: Place additional chips next to the original bet outside the betting box; point with two fingers spread into a V formation.Surrender (only available as first decision of a hand): Some games offer the option to "surrender", usually in hole-card games and directly after the dealer has checked for blackjack (but see below for variations). When the player surrenders, the house takes half the player's bet and returns the other half to the player; this terminates the player's interest in the hand.Signal: The request to surrender is made verbally, there being no standard hand signal.Hand signals are used to assist the "eye in the sky", a person or video camera located above the table and sometimes concealed behind one-way glass. The eye in the sky usually makes a video recording of the table, which helps in resolving disputes and identifying dealer mistakes, and is also used to protect the casino against dealers who steal chips or players who cheat. The recording can further be used to identify advantage players whose activities, while legal, make them undesirable customers. In the event of a disagreement between a player's hand signals and their words, the hand signal takes precedence.Each hand may normally "hit" as many times as desired so long as the total is not above hard 20. On reaching 21 (including soft 21), the hand is normally required to stand; busting is an irrevocable loss and the players' wagers are immediately forfeited to the house. After a bust or a stand, play proceeds to the next hand clockwise around the table. When the last hand has finished being played, the dealer reveals the hole card, and stands or draws further cards according to the rules of the game for dealer drawing. When the outcome of the dealer's hand is established, any hands with bets remaining on the table are resolved (usually in counterclockwise order): bets on losing hands are forfeited, the bet on a push is left on the table, and winners are paid out. Insurance If the dealer's upcard is an ace, the player is offered the option of taking "insurance" before the dealer checks the hole card.Insurance is a side bet that the dealer has blackjack and is treated independently of the main wager. It pays 2:1 (meaning that the player receives two dollars for every dollar bet) and is available when the dealer's exposed card is an ace. The idea is that the dealer's second card has a fairly high probability (nearly one-third) to be ten-valued, giving the dealer blackjack and disappointment for the player. It is attractive (although not necessarily wise) for the player to insure against the possibility of a dealer blackjack by making a maximum "insurance" bet, in which case the "insurance proceeds" will make up for the concomitant loss on the original bet. The player may add up to half the value of their original bet to the insurance and these extra chips are placed on a portion of the table usually marked "Insurance pays 2 to 1".Players with a blackjack may also take insurance, and in taking maximum insurance they commit themselves to winning an amount exactly equal to their main wager, regardless of the dealer's outcome. Fully insuring a blackjack against blackjack is thus referred to as "taking even money", and paid out immediately, before the dealer's hand is resolved; the players do not need to place more chips for the insurance wager.Insurance bets are expected to lose money in the long run, because the dealer is likely to have blackjack less than one-third of the time. However the insurance outcome is strongly anti-correlated with that of the main wager, and if the player's priority is to reduce variation, they might choose to pay for this.Furthermore, the insurance bet is susceptible to advantage play. It is advantageous to make an insurance bet whenever the hole card has more than a chance of one in three of being a ten. Advantage play techniques can sometimes identify such situations. In a multi-hand, face-up, single deck game, it is possible to establish whether insurance is a good bet simply by observing the other cards on the table after the deal; even if there are just 2 player hands exposed, and neither of their two initial cards is a ten, then 16 in 47 of the remaining cards are tens, which is larger than 1 in 3, so insurance is a good bet. This is an elementary example of the family of advantage play techniques known as card counting.Bets to insure against blackjack are slightly less likely to be advantageous than insurance bets in general, since the ten in the player's blackjack makes it less likely that the dealer has blackjack too. Rule variations and their consequences for the house edge The rules of casino blackjack are generally determined by law or regulation, which establishes certain rule variations allowed at the discretion of the casino. The rules of any particular game are generally posted on or near the table, failing which there is an expectation that casino staff will provide them on request. Over 100 variations of blackjack have been documented.As with all casino games, blackjack incorporates a "house edge", a statistical advantage for the casino that is built into the game. The advantage of the dealer's position in blackjack relative to the player comes from the fact that if the player busts, the player loses, regardless of whether the dealer subsequently busts. Nonetheless, blackjack players using basic strategy will lose less than 1% of their total wagered amount with strictly average luck; this is very favorable to the player compared to other casino games. The loss rate of players who deviate from basic strategy through ignorance is generally expected to be greater.Dealer hits soft 17Each game has a rule about whether the dealer must hit or stand on soft 17, which is generally printed on the table surface. The variation where the dealer must hit soft 17 is abbreviated "H17" in blackjack literature, with "S17" used for the stand-on-soft-17 variation. Substituting an "H17" rule with an "S17" rule in a game benefits the player, decreasing the house edge by about 0.2%.Number of decksAll things being equal, using fewer decks decreases the house edge. This mainly reflects an increased likelihood of player blackjack, since if the players draws a ten on their first card, the subsequent probability of drawing an ace is higher with fewer decks. It also reflects a decreased likelihood of blackjack-blackjack push in a game with fewer decks.Casinos generally compensate by tightening other rules in games with fewer decks, in order to preserve the house edge or discourage play altogether. When offering single deck blackjack games, casinos are more likely to disallow doubling on soft hands or after splitting, to restrict resplitting, require higher minimum bets, and to pay the player less than 3:2 for a winning blackjack.The following table illustrates the mathematical effect on the house edge of the number of decks, by considering games with various deck counts under the following ruleset: double after split allowed, resplit to four hands allowed, no hitting split aces, no surrender, double on any two cards, original bets only lost on dealer blackjack, dealer hits soft 17, and cut-card used. The increase in house edge per unit increase in the number of decks is most dramatic when comparing the single deck game to the two-deck game, and becomes progressively smaller as more decks are added.Late/early surrenderSurrender, for those games that allow it, is usually not permitted against a dealer blackjack; if the dealer's first card is an ace or ten, the hole card is checked to make sure there is no blackjack before surrender is offered. This rule protocol is consequently known as "late" surrender. The alternative, "early" surrender, gives player the option to surrender before the dealer checks for blackjack, or in a no-hole-card game. Early surrender is much more favorable to the player than late surrender. Most medium-strength hands should be surrendered against a dealer Ace if the hole card has not been checked.For late surrender, however, while it is tempting opt for surrender on any hand which will probably lose, the correct strategy is to only surrender on the very worst hands, because having even a one in four chance of winning the full bet is better than losing half the bet and pushing the other half, as entailed by surrendering.ResplittingIf the cards of a post-split hand have the same value, most games allow the player to split again, or "resplit". The player places a further wager and the dealer separates the new pair dealing a further card to each as before. Some games allow unlimited resplitting, while others may limit it to a certain number of hands, such as four hands (for example, "resplit to 4").Hit/resplit split acesAfter splitting aces, the common rule is that only one card will be dealt to each ace; the player cannot split, double, or take another hit on either hand. Rule variants include allowing resplitting aces or allowing the player to hit split aces. Games allowing aces to be resplit are not uncommon, but those allowing the player to hit split aces are extremely rare. Allowing the player to hit hands resulting from split aces reduces the house edge by about 0.13%; allowing resplitting of aces reduces house edge by about 0.03%. Note that a ten-value card dealt on a split ace (or vice versa) is a "soft 21" and not a "natural".No double after splitAfter a split, most games allow doubling down on the new two-card hands. Disallowing doubling after a split increases the house edge by about 0.12%.Double on 9/10/11 or 10/11 onlyUnder the "Reno rule", double down is only permitted on hard totals of 9, 10, or 11 (under a similar European rule, only 10 or 11). Basic strategy would otherwise call for some doubling down with hard 9 and soft 13–18, and advanced players can identify situations where doubling on soft 19–20 and hard 8,7 and even 6 is advantageous. The Reno rule prevents the player from taking advantage of double down in these situations and thereby increases the player's expected loss. The Reno rule increases the house edge by around one in 1000, and its European version by around two in 1000.No hole card and OBOIn most non-U.S. casinos, a 'no hole card' game is played, meaning that the dealer does not draw nor consult his or her second card until after all players have finished making decisions. With no hole card, it is almost never correct basic strategy to double or split against a dealer ten or ace, since a dealer blackjack will result in the loss of the split and double bets; the only exception is with a pair of A's against a dealer 10, where it is still correct to split. In all other cases, a stand, hit or surrender is called for. For instance, holding 11 against a dealer 10, the correct strategy is to double in a hole card game (where the player knows the dealer's second card is not an ace), but to hit in a no hole card game. The no hole card rule adds approximately 0.11% to the house edge.The "original bets only" rule variation appearing in certain no hole card games states that if the player's hand loses to a dealer blackjack, only the mandatory initial bet ("original") is forfeited, and all optional bets, meaning doubles and splits, are pushed. "Original bets only" is also known by the acronym OBO; it has the same effect on basic strategy and house edge as reverting to a hole card game.Altered payout for a winning blackjackIn many casinos, a blackjack pays only 6:5 or even 1:1 instead of the usual 3:2. This is usually at tables with the lowest table minimums and single-deck games. Among common rule variations in the U.S., these altered payouts for blackjack are the most damaging to the player, causing the greatest increase in house edge. Since blackjack occurs in approximately 4.8% of hands, the 1:1 game increases the house edge by 2.3%, while the 6:5 game adds 1.4% to the house edge. Video blackjack machines generally pay 1:1 payout for a blackjack. The 6:5 rule is most commonly employed on table blackjack at single deck games, where they help the house to compensate for low house edge intrinsic in using one deck only.Dealer wins tiesThe rule that bets on tied hands are lost rather than pushed is catastrophic to the player. Though rarely used in standard blackjack, it is sometimes seen in "blackjack-like" games such as in some charity casinos. Blackjack strategy  Basic strategy Each blackjack game has a basic strategy, which is playing a hand of any total value against any dealer's up-card, which loses the least money to the house in the long term.An example of basic strategy is shown in the table below, and includes the following parameters:Four to eight decksThe dealer stands on a soft 17A double is allowed after a splitOnly original bets are lost on dealer blackjackKey:S  StandH  HitDh  Double (if not allowed, then hit)Ds  Double (if not allowed, then stand)SP  SplitSU  Surrender (if not allowed, then hit)The bulk of basic strategy is common to all blackjack games, with most rule variations calling for changes in only a few situations. For example, if the above game used the hit on soft 17 rule, common in Las Vegas Strip casinos, only 6 cells of the table would need to be changed: double on 11 vs. A, surrender 15 or 17 vs. A, double on A,7 vs. 2, double on A,8 vs. 6, surrender (if not allowed, then hit) on 8,8 vs. A. Also when playing basic strategy never take insurance or "even money."Estimates of the house edge for blackjack games quoted by casinos and gaming regulators are generally based on the assumption that the players follow basic strategy and do not systematically change their bet size.Most blackjack games have a house edge of between 0.5% and 1%, placing blackjack among the cheapest casino table games. Casino promotions such as complimentary matchplay vouchers or 2:1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy. Composition-dependent strategy Basic strategy is based upon a player's point total and the dealer's visible card. Players may be able to improve on this decision by considering the precise composition of their hand, not just the point total. For example, players should ordinarily stand when holding 12 against a dealer 4. However, in a single deck game, players should hit if their 12 consists of a 10 and a 2. The presence of a 10 in the player's hand has two consequences:It makes the player's 12 a worse hand to stand on (since the only way to avoid losing is for the dealer to go bust, which is less likely if there are fewer 10s left in the shoe).It makes hitting safer, since the only way of going bust is to draw a 10, and this is less likely with a 10 already in the hand.However, even when basic and composition-dependent strategy lead to different actions, the difference in expected reward is small, and it becomes even smaller with more decks. Using a composition-dependent strategy rather than basic strategy in a single deck game reduces the house edge by 4 in 10,000, which falls to 3 in 100,000 for a six-deck game. Advantage play Blackjack has been a high-profile target for advantage players since the 1960s. Advantage play is the attempt to win more using skills such as memory, computation, and observation. These techniques, while generally legal, can be powerful enough to give the player a long-term edge in the game, making them an undesirable customer for the casino and potentially leading to ejection or blacklisting if they are detected. The main techniques of advantage play in blackjack are as follows: Card counting During the course of a blackjack shoe, the dealer exposes the dealt cards. Careful accounting of the exposed cards allows a player to make inferences about the cards which remain to be dealt. These inferences can be used in the following ways:Players can make larger bets when they have an advantage. For example, the players can increase the starting bet if there are many aces and tens left in the deck, in the hope of hitting a blackjack.Players can deviate from basic strategy according to the composition of their undealt cards. For example, with many tens left in the deck, players might double down in more situations since there is a better chance of getting a good hand.A card counting system assigns a point score to each rank of card (e.g., 1 point for 2–6, 0 points for 7–9 and −1 point for 10–A). When a card is exposed, a counter adds the score of that card to a running total, the 'count'. A card counter uses this count to make betting and playing decisions according to a table which they have learned. The count starts at 0 for a freshly shuffled deck for "balanced" counting systems. Unbalanced counts are often started at a value which depends on the number of decks used in the game.Blackjack's house edge is usually between 0.5%–1% when players use basic strategy. Card counting can give the player an edge of up to 2% over the house.Card counting is most rewarding near the end of a complete shoe when as few as possible cards remain. Single-deck games are therefore particularly susceptible to card counting. As a result, casinos are more likely to insist that players do not reveal their cards to one another in single-deck games. In games with more decks of cards, casinos limit penetration by ending the shoe and reshuffling when one or more decks remain undealt. Casinos also sometimes use a shuffling machine to reintroduce the exhausted cards every time a deck has been played.Card counting is legal and is not considered cheating as long as the counter isn't using an external device, but if a casino realizes a player is counting, the casino might inform them that they are no longer welcome to play blackjack. Sometimes a casino might ban a card counter from the property.The use of external devices to help counting cards is illegal in all US states that license blackjack card games. Shuffle tracking Techniques other than card counting can swing the advantage of casino blackjack toward the player. All such techniques are based on the value of the cards to the player and the casino as originally conceived by Edward O. Thorp. One technique, mainly applicable in multi-deck games, involves tracking groups of cards (also known as slugs, clumps, or packs) during the play of the shoe, following them through the shuffle, and then playing and betting accordingly when those cards come into play from the new shoe. Shuffle tracking requires excellent eyesight and powers of visual estimation but is more difficult to detect since the player's actions are largely unrelated to the composition of the cards in the shoe.Arnold Snyder's articles in Blackjack Forum magazine brought shuffle tracking to the general public. His book, The Shuffle Tracker's Cookbook, mathematically analyzed the player edge available from shuffle tracking based on the actual size of the tracked slug. Jerry L. Patterson also developed and published a shuffle-tracking method for tracking favorable clumps of cards and cutting them into play and tracking unfavorable clumps of cards and cutting them out of play. Identifying concealed cards The player can also gain an advantage by identifying cards from distinctive wear markings on their backs, or by hole carding (observing during the dealing process the front of a card dealt face down). These methods are generally legal although their status in particular jurisdictions may vary. Side bets Many blackjack tables offer a side bet on various outcomes including:Player hand and dealer's up card sum to 19, 20, or 21 ("Lucky Lucky")Player initial hand is a pair ("Perfect pairs")Player initial hand is suited, suited and connected, or a suited K-Q ("Royal match")Player initial hand plus dealer's card makes a flush, straight, or three-of-a-kind poker hand ("21+3")Player initial hand totals 20 ("Lucky Ladies")Dealer upcard is in between the value of the players two cards ("In Bet")First card drawn to the dealer will result in a dealer bust ("Bust It!")One or both of the players cards is the same as the dealers card ("Match the Dealer")Player allowed to make optional second hand, and effectively receive the hand of 10,8, or 18 without drawings cards ("Instant 18")The side wager is typically placed in a designated area next to the box for the main wager. A player wishing to wager on a side bet is usually required to place a wager on blackjack. Some games require that the blackjack wager should equal or exceed any side bet wager. A non-controlling player of a blackjack hand is usually permitted to place a side bet regardless of whether the controlling player does so.The house edge for side bets is generally far higher than for the blackjack game itself. Nonetheless side bets can be susceptible to card counting. A side count, designed specifically for a particular side bet, can improve the player edge. Most side games do not offer a sufficient win rate to justify the effort of advantage play; exceptions include "Lucky ladies" and "Over/Under".In team play it is common for team members to be dedicated toward counting only a sidebet using a specialized count. Blackjack tournaments Blackjack can be played in tournament form. Players start with an equal numbers of chips; the goal is to finish among the top chip-holders. Depending on the number of competitors, tournaments may be held over several rounds, with one or two players qualifying from each table after a set number of deals to meet the qualifiers from the other tables in the next round. Another tournament format, Elimination Blackjack, drops the lowest-stacked player from the table at pre-determined points in the tournament. Good strategy for blackjack tournaments can differ from non-tournament strategy because of the added dimension of choosing the amount to be wagered. As in poker tournaments, players pay the casino an initial entry fee to participate in a tournament, and re-buys are sometimes permitted. Video blackjack Some casinos, as well as general betting outlets, provide blackjack among a selection of casino-style games at electronic consoles. Video blackjack game rules are generally more favorable to the house; e.g., paying out only even money for winning blackjacks. Video and online blackjack games deal each coup from a fresh shoe, rendering card counting much less effective. Variants of the game Blackjack is a member of a large family of traditional card games played recreationally all around the world. Most of these games have not been adapted for casino play. Furthermore, the casino game development industry is very active in producing blackjack variants, most of which are ultimately not adopted for widespread use in casinos. The following are the prominent twenty-one themed comparing card games which have been adapted or invented for use in casinos and have become established in the gambling industry.Spanish 21 provides players with many liberal blackjack rules, such as doubling down any number of cards (with the option to rescue, or surrender only one wager to the house), payout bonuses for five or more card 21s, 6–7–8 21s, 7–7–7 21s, late surrender, and player blackjacks always winning and player 21s always winning, at the cost of having no 10 cards in the deck (though there are jacks, queens, and kings). An unlicensed version of Spanish 21 played without a hole card is found in Australian casinos under the name "Pontoon" (presumably borrowed from the British recreational blackjack-like game "Pontoon" which has substantially different rules).21st-Century Blackjack (also known as "Vegas Style" Blackjack) is found in California card rooms. In this form of the game, a player bust does not always result in an automatic loss; depending on the casino, the player can still push if the dealer busts as well, although the dealer typically has to bust with a higher total.Double Exposure Blackjack deals the first two cards of the dealer's hand face up. Blackjacks pay even money, and players lose on ties; also, they can neither buy insurance nor can they surrender their hand (as both dealer's cards are exposed at the outset).Double Attack Blackjack has very liberal blackjack rules and the option of increasing one's wager after seeing the dealer's up card. This game is dealt from a Spanish shoe, and blackjacks only pay even money.Blackjack Switch is played over two hands whose second cards the player is allowed to interchange. For example, if the player is dealt 10–6 and 5–10, then the player can switch two cards to make hands of 10–10 and 6–5. Natural blackjacks are paid 1:1 instead of the standard 3:2, and a dealer 22 is a push.Multiple Action Blackjack involves a player placing between 2 or 3 bets on a single hand. The dealer then gets a hand for each bet the player places on a hand. This essentially doubles the number of hands a single dealer can play per hour. Splitting and doubling are still allowed, but often limited due to limited space on the felt for additional chips. Strategy for this game is the same as strategy for conventional blackjack regardless of how many places are bet.Super Fun 21 allows a player to split a hand up to four times. If the player has six cards totaling 20, he automatically wins. Wins are paid 1:1.Examples of the many local traditional and recreational blackjack-like games include French/German Blackjack, called Vingt-et-un (French: Twenty-one) or "Siebzehn und Vier" (German: Seventeen and Four). The French/German game does not allow splitting. An ace can only count as eleven, but two aces count as a blackjack. It is mostly played in private circles and barracks. A British variation is called "Pontoon", the name being probably a corruption of "Vingt-et-un". TV show variations Blackjack is also featured in various television shows. Here are a few shows inspired by the game.Gambit was a game show with Wink Martindale where couples answer questions to collect cards that could add up to 21.Catch 21 is a game show with Alfonso Ribeiro from The Fresh Prince of Bel-Air. On this show, three players answer questions to earn cards in order to win cash and ties are not allowed.From 1980 to 2006, there was a blackjack-based pricing game on The Price is Right, called Hit Me. In this game, six grocery products were used, and five of those products' prices were multiplied by various numbers from 2 to 10, with the one remaining product having the exact price. In order to win a large prize, the contestant had to achieve a score of 21 (which was usually done by picking a product whose price was multiplied by ten and the one that was correctly priced), or beat the house with any score that did not exceed 21. Blackjack Hall of Fame In 2002, professional gamblers around the world were invited to nominate great blackjack players for admission into the Blackjack Hall of Fame. Seven members were inducted in 2002, with new people inducted every year after. The Hall of Fame is at the Barona Casino in San Diego. Members include Edward O. Thorp, author of the 1960s book Beat the Dealer which proved that the game could be beaten with a combination of basic strategy and card counting; Ken Uston, who popularized the concept of team play; Arnold Snyder, author and editor of the Blackjack Forum trade journal; Stanford Wong, author and popularizer of the "Wonging" technique of only playing at a positive count, and several others. Blackjack in the arts Novels have been written around blackjack and the possibility of winning games via some kind of method. Among these were The Blackjack Hijack (Charles Einstein, 1976), later produced as the TV movie Nowhere to Run, Bringing Down the House (Ben Mezrich), also filmed as 21, and a 2008 remake. An almost identical theme was shown in the 2004 Canadian film The Last Casino.Movies titled "21" or "Twenty One" depicting the blackjack game as a central theme have been produced and released in 1918 (starring Bryant Washburn) and in 1923 (starring Richard Barthelmess). In The Hangover, an American comedy, four friends try to count cards to win back enough money to secure the release of their friend from the clutches of a notorious criminal they stole from the previous night while blacked out. A central part of the plot of Rain Man is that Raymond (Dustin Hoffman), an autistic savant, is able to win at blackjack by counting cards. See also Glossary of blackjack termsMIT Blackjack Team Blackjack literature Beat the Dealer : A Winning Strategy for the Game of Twenty-One, Edward O. Thorp, 1966, ISBN 978-0-394-70310-7Blackbelt in Blackjack, Arnold Snyder, 1998 (1980), ISBN 978-0-910575-05-8Blackjack and the Law, I. Nelson Rose and Robert A. Loeb, 1998, ISBN 0-910575-08-8Blackjack: A Winner's Handbook, Jerry L. Patterson, 2001, (1978), ISBN 978-0-399-52683-1Encyclopedia of Casino Twenty-One, Michael Dalton, 2016, (1993), ISBN 1-879712-02-4Ken Uston on Blackjack, Ken Uston, 1986, ISBN 978-0-8184-0411-5Knock-Out Blackjack, Olaf Vancura and Ken Fuchs, 1998, ISBN 978-0-929712-31-4Luck, Logic, and White Lies: The Mathematics of Games, Jörg Bewersdorff, 2004, ISBN 978-1-56881-210-6, 121–134, supplement: Blackjack calculator (JavaScript)Million Dollar Blackjack, Ken Uston, 1994 (1981), ISBN 978-0-89746-068-2Playing Blackjack as a Business, Lawrence Revere, 1998 (1971), ISBN 978-0-8184-0064-3Professional Blackjack, Stanford Wong, 1994 (1975), ISBN 978-0-935926-21-7The Theory of Blackjack, Peter Griffin, 1996 (1979), ISBN 978-0-929712-12-3The Theory of Gambling and Statistical Logic, Richard A. Epstein, 1977, ISBN 978-0-12-240761-1, 215–251The World's Greatest Blackjack Book, Lance Humble and Carl Cooper, 1980, ISBN 978-0-385-15382-9The Blackjack Life, Nathaniel Tilton, 2012, ISBN 978-1935396338BlackjackinColorRegulation in the United KingdomThe Gaming Clubs (Bankers' Games) No. 2899 Regulation 7 1994The Gaming Clubs (Bankers' Games) (Amendment) No. 597 Regulation 3 2000The Gaming Clubs (Bankers' Games)(Amendment) No. 1130 Regulation 2 2002 References  External links Search for Blackjack at DMOZ
A bank is a financial institution that accepts deposits from the public and creates credit. Lending activities can be performed either directly or indirectly through capital markets. Due to their importance in the financial stability of a country, banks are highly regulated in most countries. Most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, known as the Basel Accords.Banking in its modern sense evolved in the 14th century in the prosperous cities of Renaissance Italy but in many ways was a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties — notably, the Medicis, the Fuggers, the Welsers, the Berenbergs and the Rothschilds — have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena, while the oldest existing merchant bank is Berenberg Bank. HistoryEdit Banking began with the first prototype banks of merchants of the ancient world, which made grain loans to farmers and traders who carried goods between cities. This began around 2000 BC in Assyria and Babylonia. Later, in ancient Greece and during the Roman Empire, lenders based in temples made loans and added two important innovations: they accepted deposits and changed money. Archaeology from this period in ancient China and India also shows evidence of money lending activity.The origins of modern banking can be traced to medieval and early Renaissance Italy, to the rich cities in the centre and north like Florence, Lucca, Siena, Venice and Genoa. The Bardi and Peruzzi families dominated banking in 14th-century Florence, establishing branches in many other parts of Europe. One of the most famous Italian banks was the Medici Bank, set up by Giovanni di Bicci de' Medici in 1397. The earliest known state deposit bank, Banco di San Giorgio (Bank of St. George), was founded in 1407 at Genoa, Italy.Modern banking practices, including fractional reserve banking and the issue of banknotes, emerged in the 17th and 18th centuries. Merchants started to store their gold with the goldsmiths of London, who possessed private vaults, and charged a fee for that service. In exchange for each deposit of precious metal, the goldsmiths issued receipts certifying the quantity and purity of the metal they held as a bailee; these receipts could not be assigned, only the original depositor could collect the stored goods.Gradually the goldsmiths began to lend the money out on behalf of the depositor, which led to the development of modern banking practices; promissory notes (which evolved into banknotes) were issued for money deposited as a loan to the goldsmith. The goldsmith paid interest on these deposits. Since the promissory notes were payable on demand, and the advances (loans) to the goldsmith's customers were repayable over a longer time period, this was an early form of fractional reserve banking. The promissory notes developed into an assignable instrument which could circulate as a safe and convenient form of money backed by the goldsmith's promise to pay, allowing goldsmiths to advance loans with little risk of default. Thus, the goldsmiths of London became the forerunners of banking by creating new money based on credit.The Bank of England was the first to begin the permanent issue of banknotes, in 1695. The Royal Bank of Scotland established the first overdraft facility in 1728. By the beginning of the 19th century a bankers' clearing house was established in London to allow multiple banks to clear transactions. The Rothschilds pioneered international finance on a large scale, financing the purchase of the Suez canal for the British government. EtymologyEdit The word bank was borrowed in Middle English from Middle French banque, from Old Italian banca, meaning "table", from Old High German banc, bank "bench, counter". Benches were used as makeshift desks or exchange counters during the Renaissance by Jewish Florentine bankers, who used to make their transactions atop desks covered by green tablecloths. DefinitionEdit The definition of a bank varies from country to country. See the relevant country pages under for more information.Under English common law, a banker is defined as a person who carries on the business of banking, which is specified as:conducting current accounts for his customers,paying cheques drawn on him/her, andcollecting cheques for his/her customers.In most common law jurisdictions there is a Bills of Exchange Act that codifies the law in relation to negotiable instruments, including cheques, and this Act contains a statutory definition of the term banker: banker includes a body of persons, whether incorporated or not, who carry on the business of banking' (Section 2, Interpretation). Although this definition seems circular, it is actually functional, because it ensures that the legal basis for bank transactions such as cheques does not depend on how the bank is structured or regulated.The business of banking is in many English common law countries not defined by statute but by common law, the definition above. In other English common law jurisdictions there are statutory definitions of the business of banking or banking business. When looking at these definitions it is important to keep in mind that they are defining the business of banking for the purposes of the legislation, and not necessarily in general. In particular, most of the definitions are from legislation that has the purpose of regulating and supervising banks rather than regulating the actual business of banking. However, in many cases the statutory definition closely mirrors the common law one. Examples of statutory definitions:"banking business" means the business of receiving money on current or deposit account, paying and collecting cheques drawn by or paid in by customers, the making of advances to customers, and includes such other business as the Authority may prescribe for the purposes of this Act; (Banking Act (Singapore), Section 2, Interpretation)."banking business" means the business of either or both of the following:receiving from the general public money on current, deposit, savings or other similar account repayable on demand or within less than [3 months] ... or with a period of call or notice of less than that period;paying or collecting cheques drawn by or paid in by customers.Since the advent of EFTPOS (Electronic Funds Transfer at Point Of Sale), direct credit, direct debit and internet banking, the cheque has lost its primacy in most banking systems as a payment instrument. This has led legal theorists to suggest that the cheque based definition should be broadened to include financial institutions that conduct current accounts for customers and enable customers to pay and be paid by third parties, even if they do not pay and collect cheques . BankEdit  Standard businessEdit Banks act as payment agents by conducting checking or current accounts for customers, paying cheques drawn by customers on the bank, and collecting cheques deposited to customers' current accounts. Banks also enable customer payments via other payment methods such as Automated Clearing House (ACH), Wire transfers or telegraphic transfer, EFTPOS, and automated teller machines (ATMs).Banks borrow money by accepting funds deposited on current accounts, by accepting term deposits, and by issuing debt securities such as banknotes and bonds. Banks lend money by making advances to customers on current accounts, by making installment loans, and by investing in marketable debt securities and other forms of money lending.Banks provide different payment services, and a bank account is considered indispensable by most businesses and individuals. Non-banks that provide payment services such as remittance companies are normally not considered as an adequate substitute for a bank account.Banks can create new money when they make a loan. New loans throughout the banking system generate new deposits elsewhere in the system. The money supply is usually increased by the act of lending, and reduced when loans are repaid faster than new ones are generated. In the United Kingdom between 1997 and 2007, there was an increase in the money supply, largely caused by much more bank lending, which served to push up property prices and increase private debt. The amount of money in the economy as measured by M4 in the UK went from £750 billion to £1700 billion between 1997 and 2007, much of the increase caused by bank lending. If all the banks increase their lending together, then they can expect new deposits to return to them and the amount of money in the economy will increase. Excessive or risky lending can cause borrowers to default, the banks then become more cautious, so there is less lending and therefore less money so that the economy can go from boom to bust as happened in the UK and many other Western economies after 2007. Range of activitiesEdit Activities undertaken by banks include personal banking, corporate banking, investment banking, private banking, transaction banking, insurance, consumer finance, foreign exchange trading, commodity trading, trading in equities, futures and options trading and money market trading. ChannelsEdit Banks offer many different channels to access their banking and other services:Automated teller machinesA branch in a retail locationCall centreMail: most banks accept cheque deposits via mail and use mail to communicate to their customers, e.g. by sending out statementsMobile banking is a method of using one's mobile phone to conduct banking transactionsOnline banking is a term used for performing multiple transactions, payments etc. over the InternetRelationship managers, mostly for private banking or business banking, often visiting customers at their homes or businessesTelephone banking is a service which allows its customers to conduct transactions over the telephone with automated attendant, or when requested, with telephone operatorVideo banking is a term used for performing banking transactions or professional banking consultations via a remote video and audio connection. Video banking can be performed via purpose built banking transaction machines (similar to an Automated teller machine), or via a video conference enabled bank branch clarificationDSA is a Direct Selling Agent, who works for the bank based on a contract. Its main job is to increase the customer base for the bank. Business modelsEdit A bank can generate revenue in a variety of different ways including interest, transaction fees and financial advice. Traditionally, the most significant method is via charging interest on the capital it lends out to customers. The bank profits from the difference between the level of interest it pays for deposits and other sources of funds, and the level of interest it charges in its lending activities.This difference is referred to as the spread between highly the cost of funds and the loan interest rate. Historically, profitability from lending activities has been cyclical and dependent on the needs and strengths of loan customers and the stage of the economic cycle. Fees and financial advice constitute a more stable revenue stream and banks have therefore placed more emphasis on these revenue lines to smooth their financial performance.In the past 20 years, American banks have taken many measures to ensure that they remain profitable while responding to increasingly changing market conditions.First, this includes the Gramm–Leach–Bliley Act, which allows banks again to merge with investment and insurance houses. Merging banking, investment, and insurance functions allows traditional banks to respond to increasing consumer demands for "one-stop shopping" by enabling cross-selling of products (which, the banks hope, will also increase profitability).Second, they have expanded the use of risk-based pricing from business lending to consumer lending, which means charging higher interest rates to those customers that are considered to be a higher credit risk and thus increased chance of default on loans. This helps to offset the losses from bad loans, lowers the price of loans to those who have better credit histories, and offers credit products to high risk customers who would otherwise be denied credit.Third, they have sought to increase the methods of payment processing available to the general public and business clients. These products include debit cards, prepaid cards, smart cards, and credit cards. They make it easier for consumers to conveniently make transactions and smooth their consumption over time (in some countries with underdeveloped financial systems, it is still common to deal strictly in cash, including carrying suitcases filled with cash to purchase a home).However, with the convenience of easy credit, there is also increased risk that consumers will mismanage their financial resources and accumulate excessive debt. Banks make money from card products through interest charges and fees charged to cardholders, and transaction fees to retailers who accept the bank's credit and/or debit cards for payments.This helps in making a profit and facilitates economic development as a whole. ProductsEdit  Retail bankingEdit Savings accountRecurring deposit accountFixed deposit accountMoney market accountCertificate of deposit (CD)Individual retirement account (IRA)Credit cardDebit cardMortgageMutual fundPersonal loanTime depositsATM cardCurrent accountsCheque booksAutomated Teller Machine (ATM) Business (or commercial/investment) bankingEdit Business loanCapital raising (equity / debt / hybrids)Revolving creditRisk management (foreign exchange (FX)), interest rates, commodities, derivatives)Term loanCash management services (lock box, remote deposit capture, merchant processing)Credit services Capital and riskEdit Banks face a number of risks in order to conduct their business, and how well these risks are managed and understood is a key driver behind profitability, and how much capital a bank is required to hold. Bank capital consists principally of equity, retained earnings and subordinated debt.After the 2007-2009 financial crisis, regulators force banks to issue Contingent convertible bonds (CoCos).These are hybrid capital securities that absorb losses in accordance with their contractual terms when the capital of the issuing bank falls below a certain level. Then debt is reduced and bank capitalization gets a boost. Owing to their capacity to absorb losses, CoCos have the potential to satisfy regulatory capital requirement.Some of the main risks faced by banks include:Credit risk: risk of loss arising from a borrower who does not make payments as promised.Liquidity risk: risk that a given security or asset cannot be traded quickly enough in the market to prevent a loss (or make the required profit).Market risk: risk that the value of a portfolio, either an investment portfolio or a trading portfolio, will decrease due to the change in value of the market risk factors.Operational risk: risk arising from execution of a company's business functions.Reputational risk: a type of risk related to the trustworthiness of business.Macroeconomic risk: risks related to the aggregate economy the bank is operating in.The capital requirement is a bank regulation, which sets a framework within which a bank or depository institution must manage its balance sheet. The categorization of assets and capital is highly standardized so that it can be risk weighted. Banks in the economyEdit  Economic functionsEdit The economic functions of banks include:Issue of money, in the form of banknotes and current accounts subject to cheque or payment at the customer's order. These claims on banks can act as money because they are negotiable or repayable on demand, and hence valued at par. They are effectively transferable by mere delivery, in the case of banknotes, or by drawing a cheque that the payee may bank or cash.Netting and settlement of payments – banks act as both collection and paying agents for customers, participating in interbank clearing and settlement systems to collect, present, be presented with, and pay payment instruments. This enables banks to economize on reserves held for settlement of payments, since inward and outward payments offset each other. It also enables the offsetting of payment flows between geographical areas, reducing the cost of settlement between them.Credit intermediation – banks borrow and lend back-to-back on their own account as middle men.Credit quality improvement – banks lend money to ordinary commercial and personal borrowers (ordinary credit quality), but are high quality borrowers. The improvement comes from diversification of the bank's assets and capital which provides a buffer to absorb losses without defaulting on its obligations. However, banknotes and deposits are generally unsecured; if the bank gets into difficulty and pledges assets as security, to raise the funding it needs to continue to operate, this puts the note holders and depositors in an economically subordinated position.Asset liability mismatch/Maturity transformation – banks borrow more on demand debt and short term debt, but provide more long term loans. In other words, they borrow short and lend long. With a stronger credit quality than most other borrowers, banks can do this by aggregating issues (e.g. accepting deposits and issuing banknotes) and redemptions (e.g. withdrawals and redemption of banknotes), maintaining reserves of cash, investing in marketable securities that can be readily converted to cash if needed, and raising replacement funding as needed from various sources (e.g. wholesale cash markets and securities markets).Money creation/destruction – whenever a bank gives out a loan in a fractional-reserve banking system, a new sum of money is created and conversely, whenever the principal on that loan is repaid money is destroyed. Bank crisisEdit Banks are susceptible to many forms of risk which have triggered occasional systemic crises. These include liquidity risk (where many depositors may request withdrawals in excess of available funds), credit risk (the chance that those who owe money to the bank will not repay it), and interest rate risk (the possibility that the bank will become unprofitable, if rising interest rates force it to pay relatively more on its deposits than it receives on its loans).Banking crises have developed many times throughout history, when one or more risks have emerged for a banking sector as a whole. Prominent examples include the bank run that occurred during the Great Depression, the U.S. Savings and Loan crisis in the 1980s and early 1990s, the Japanese banking crisis during the 1990s, and the sub-prime mortgage crisis in the 2000s. Size of global banking industryEdit Assets of the largest 1,000 banks in the world grew by 6.8% in the 2008/2009 financial year to a record US$96.4 trillion while profits declined by 85% to US$115 billion. Growth in assets in adverse market conditions was largely a result of recapitalization. EU banks held the largest share of the total, 56% in 2008/2009, down from 61% in the previous year. Asian banks' share increased from 12% to 14% during the year, while the share of US banks increased from 11% to 13%. Fee revenue generated by global investment banking totalled US$66.3 billion in 2009, up 12% on the previous year.The United States has the most banks in the world in terms of institutions (5,330 as of 2015) and possibly branches (81,607 as of 2015). This is an indicator of the geography and regulatory structure of the USA, resulting in a large number of small to medium-sized institutions in its banking system. As of Nov 2009, China's top 4 banks have in excess of 67,000 branches (ICBC:18000+, BOC:12000+, CCB:13000+, ABC:24000+) with an additional 140 smaller banks with an undetermined number of branches. Japan had 129 banks and 12,000 branches. In 2004, Germany, France, and Italy each had more than 30,000 branches—more than double the 15,000 branches in the UK. RegulationEdit Currently commercial banks are regulated in most jurisdictions by government entities and require a special bank license to operate.Usually the definition of the business of banking for the purposes of regulation is extended to include acceptance of deposits, even if they are not repayable to the customer's order—although money lending, by itself, is generally not included in the definition.Unlike most other regulated industries, the regulator is typically also a participant in the market, being either a publicly or privately governed central bank. Central banks also typically have a monopoly on the business of issuing banknotes. However, in some countries this is not the case. In the UK, for example, the Financial Services Authority licenses banks, and some commercial banks (such as the Bank of Scotland) issue their own banknotes in addition to those issued by the Bank of England, the UK government's central bank.Banking law is based on a contractual analysis of the relationship between the bank (defined above) and the customer—defined as any entity for which the bank agrees to conduct an account.The law implies rights and obligations into this relationship as follows:The bank account balance is the financial position between the bank and the customer: when the account is in credit, the bank owes the balance to the customer; when the account is overdrawn, the customer owes the balance to the bank.The bank agrees to pay the customer's checks up to the amount standing to the credit of the customer's account, plus any agreed overdraft limit.The bank may not pay from the customer's account without a mandate from the customer, e.g. a cheque drawn by the customer.The bank agrees to promptly collect the cheques deposited to the customer's account as the customer's agent, and to credit the proceeds to the customer's account.The bank has a right to combine the customer's accounts, since each account is just an aspect of the same credit relationship.The bank has a lien on cheques deposited to the customer's account, to the extent that the customer is indebted to the bank.The bank must not disclose details of transactions through the customer's account—unless the customer consents, there is a public duty to disclose, the bank's interests require it, or the law demands it.The bank must not close a customer's account without reasonable notice, since cheques are outstanding in the ordinary course of business for several days.These implied contractual terms may be modified by express agreement between the customer and the bank. The statutes and regulations in force within a particular jurisdiction may also modify the above terms and/or create new rights, obligations or limitations relevant to the bank-customer relationship.Some types of financial institution, such as building societies and credit unions, may be partly or wholly exempt from bank license requirements, and therefore regulated under separate rules.The requirements for the issue of a bank license vary between jurisdictions but typically include:Minimum capitalMinimum capital ratio'Fit and Proper' requirements for the bank's controllers, owners, directors, or senior officersApproval of the bank's business plan as being sufficiently prudent and plausible. Types of banksEdit Banks' activities can be divided into:retail banking, dealing directly with individuals and small businesses;business banking, providing services to mid-market business;corporate banking, directed at large business entities;private banking, providing wealth management services to high-net-worth individuals and families;investment banking, relating to activities on the financial markets.Most banks are profit-making, private enterprises. However, some are owned by government, or are non-profit organizations. Types of banksEdit Commercial banks: the term used for a normal bank to distinguish it from an investment bank. After the Great Depression, the U.S. Congress required that banks only engage in banking activities, whereas investment banks were limited to capital market activities. Since the two no longer have to be under separate ownership, some use the term "commercial bank" to refer to a bank or a division of a bank that mostly deals with deposits and loans from corporations or large businesses.Community banks: locally operated financial institutions that empower employees to make local decisions to serve their customers and the partners.Community development banks: regulated banks that provide financial services and credit to under-served markets or populations.Land development banks: The special banks providing long-term loans are called land development banks (LDB). The history of LDB is quite old. The first LDB was started at Jhang in Punjab in 1920. The main objective of the LDBs are to promote the development of land, agriculture and increase the agricultural production. The LDBs provide long-term finance to members directly through their branches.Credit unions or co-operative banks: not-for-profit cooperatives owned by the depositors and often offering rates more favourable than for-profit banks. Typically, membership is restricted to employees of a particular company, residents of a defined area, members of a certain union or religious organizations, and their immediate families.Postal savings banks: savings banks associated with national postal systems.Private banks: banks that manage the assets of high-net-worth individuals. Historically a minimum of USD 1 million was required to open an account, however, over the last years many private banks have lowered their entry hurdles to USD 350,000 for private investors.Offshore banks: banks located in jurisdictions with low taxation and regulation. Many offshore banks are essentially private banks.Savings bank: in Europe, savings banks took their roots in the 19th or sometimes even in the 18th century. Their original objective was to provide easily accessible savings products to all strata of the population. In some countries, savings banks were created on public initiative; in others, socially committed individuals created foundations to put in place the necessary infrastructure. Nowadays, European savings banks have kept their focus on retail banking: payments, savings products, credits and insurances for individuals or small and medium-sized enterprises. Apart from this retail focus, they also differ from commercial banks by their broadly decentralized distribution network, providing local and regional outreach—and by their socially responsible approach to business and society.Building societies and Landesbanks: institutions that conduct retail banking.Ethical banks: banks that prioritize the transparency of all operations and make only what they consider to be socially responsible investments.A direct or internet-only bank is a banking operation without any physical bank branches, conceived and implemented wholly with networked computers. Types of investment banksEdit Investment banks "underwrite" (guarantee the sale of) stock and bond issues, trade for their own accounts, make markets, provide investment management, and advise corporations on capital market activities such as mergers and acquisitions.Merchant banks were traditionally banks which engaged in trade finance. The modern definition, however, refers to banks which provide capital to firms in the form of shares rather than loans. Unlike venture caps, they tend not to invest in new companies. Both combinedEdit Universal banks, more commonly known as financial services companies, engage in several of these activities. These big banks are very diversified groups that, among other services, also distribute insurance— hence the term bancassurance, a portmanteau word combining "banque or bank" and "assurance", signifying that both banking and insurance are provided by the same corporate entity. Other types of banksEdit Central banks are normally government-owned and charged with quasi-regulatory responsibilities, such as supervising commercial banks, or controlling the cash interest rate. They generally provide liquidity to the banking system and act as the lender of last resort in event of a crisis.Islamic banks adhere to the concepts of Islamic law. This form of banking revolves around several well-established principles based on Islamic canons. All banking activities must avoid interest, a concept that is forbidden in Islam. Instead, the bank earns profit (markup) and fees on the financing facilities that it extends to customers. Challenges within the banking industryEdit  United StatesEdit The United States banking industry is one of the most heavily regulated and guarded in the world, with multiple specialized and focused regulators. All banks with FDIC-insured deposits have the Federal Deposit Insurance Corporation (FDIC) as a regulator. However, for soundness examinations (i.e., whether a bank is operating in a sound manner), the Federal Reserve is the primary federal regulator for Fed-member state banks; the Office of the Comptroller of the Currency (OCC) is the primary federal regulator for national banks; and the Office of Thrift Supervision, or OTS, is the primary federal regulator for thrifts. State non-member banks are examined by the state agencies as well as the FDIC. National banks have one primary regulator—the OCC.Each regulatory agency has their own set of rules and regulations to which banks and thrifts must adhere. The Federal Financial Institutions Examination Council (FFIEC) was established in 1979 as a formal inter-agency body empowered to prescribe uniform principles, standards, and report forms for the federal examination of financial institutions. Although the FFIEC has resulted in a greater degree of regulatory consistency between the agencies, the rules and regulations are constantly changing.In addition to changing regulations, changes in the industry have led to consolidations within the Federal Reserve, FDIC, OTS, and OCC. Offices have been closed, supervisory regions have been merged, staff levels have been reduced and budgets have been cut. The remaining regulators face an increased burden with increased workload and more banks per regulator. While banks struggle to keep up with the changes in the regulatory environment, regulators struggle to manage their workload and effectively regulate their banks. The impact of these changes is that banks are receiving less hands-on assessment by the regulators, less time spent with each institution, and the potential for more problems slipping through the cracks, potentially resulting in an overall increase in bank failures across the United States.The changing economic environment has a significant impact on banks and thrifts as they struggle to effectively manage their interest rate spread in the face of low rates on loans, rate competition for deposits and the general market changes, industry trends and economic fluctuations. It has been a challenge for banks to effectively set their growth strategies with the recent economic market. A rising interest rate environment may seem to help financial institutions, but the effect of the changes on consumers and businesses is not predictable and the challenge remains for banks to grow and effectively manage the spread to generate a return to their shareholders.The management of the banks’ asset portfolios also remains a challenge in today’s economic environment. Loans are a bank’s primary asset category and when loan quality becomes suspect, the foundation of a bank is shaken to the core. While always an issue for banks, declining asset quality has become a big problem for financial institutions.There are several reasons for this, one of which is the lax attitude some banks have adopted because of the years of “good times.” The potential for this is exacerbated by the reduction in the regulatory oversight of banks and in some cases depth of management. Problems are more likely to go undetected, resulting in a significant impact on the bank when they are discovered. In addition, banks, like any business, struggle to cut costs and have consequently eliminated certain expenses, such as adequate employee training programs.Banks also face a host of other challenges such as ageing ownership groups. Across the country, many banks’ management teams and board of directors are ageing. Banks also face ongoing pressure by shareholders, both public and private, to achieve earnings and growth projections. Regulators place added pressure on banks to manage the various categories of risk. Banking is also an extremely competitive industry. Competing in the financial services industry has become tougher with the entrance of such players as insurance agencies, credit unions, cheque cashing services, credit card companies, etc.As a reaction, banks have developed their activities in financial instruments, through financial market operations such as brokerage and have become big players in such activities. Loan activities of banksEdit To be able to provide home buyers and builders with the funds needed, banks must compete for deposits. The phenomenon of disintermediation had to dollars moving from savings accounts and into direct market instruments such as U.S. Department of Treasury obligations, agency securities, and corporate debt. One of the greatest factors in recent years in the movement of deposits was the tremendous growth of money market funds whose higher interest rates attracted consumer deposits.To compete for deposits, US savings institutions offer many different types of plans:Passbook or ordinary deposit accounts — permit any amount to be added to or withdrawn from the account at any time.NOW and Super NOW accounts — function like checking accounts but earn interest. A minimum balance may be required on Super NOW accounts.Money market accounts — carry a monthly limit of preauthorized transfers to other accounts or persons and may require a minimum or average balance.Certificate accounts — subject to loss of some or all interest on withdrawals before maturity.Notice accounts — the equivalent of certificate accounts with an indefinite term. Savers agree to notify the institution a specified time before withdrawal.Individual retirement accounts (IRAs) and Keogh plans — a form of retirement savings in which the funds deposited and interest earned are exempt from income tax until after withdrawal.Checking accounts — offered by some institutions under definite restrictions.All withdrawals and deposits are completely the sole decision and responsibility of the account owner unless the parent or guardian is required to do otherwise for legal reasons.Club accounts and other savings accounts — designed to help people save regularly to meet certain goals. Accounting for bank accountsEdit Bank statements are accounting records produced by banks under the various accounting standards of the world. Under GAAP there are two kinds of accounts: debit and credit. Credit accounts are Revenue, Equity and Liabilities. Debit Accounts are Assets and Expenses. The bank credits a credit account to increase its balance, and debits a credit account to decrease its balance.The customer debits his or her savings/bank (asset) account in his ledger when making a deposit (and the account is normally in debit), while the customer credits a credit card (liability) account in his ledger every time he spends money (and the account is normally in credit). When the customer reads his bank statement, the statement will show a credit to the account for deposits, and debits for withdrawals of funds. The customer with a positive balance will see this balance reflected as a credit balance on the bank statement. If the customer is overdrawn, he will have a negative balance, reflected as a debit balance on the bank statement. Brokered depositsEdit One source of deposits for banks is brokers who deposit large sums of money on behalf of investors through trust corporations. This money will generally go to the banks which offer the most favourable terms, often better than those offered local depositors. It is possible for a bank to engage in business with no local deposits at all, all funds being brokered deposits. Accepting a significant quantity of such deposits, or "hot money" as it is sometimes called, puts a bank in a difficult and sometimes risky position, as the funds must be lent or invested in a way that yields a return sufficient to pay the high interest being paid on the brokered deposits. This may result in risky decisions and even in eventual failure of the bank. Banks which failed during 2008 and 2009 in the United States during the global financial crisis had, on average, four times more brokered deposits as a percent of their deposits than the average bank. Such deposits, combined with risky real estate investments, factored into the savings and loan crisis of the 1980s. Regulation of brokered deposits is opposed by banks on the grounds that the practice can be a source of external funding to growing communities with insufficient local deposits. There are different types of accounts: saving, recurring and current accounts. Globalization in the banking industryEdit In modern time there has been huge reductions to the barriers of global competition in the banking industry. Increases in telecommunications and other financial technologies, such as Bloomberg, have allowed banks to extend their reach all over the world, since they no longer have to be near customers to manage both their finances and their risk. The growth in cross-border activities has also increased the demand for banks that can provide various services across borders to different nationalities. However, despite these reductions in barriers and growth in cross-border activities, the banking industry is nowhere near as globalized as some other industries. In the USA, for instance, very few banks even worry about the Riegle–Neal Act, which promotes more efficient interstate banking. In the vast majority of nations around globe the market share for foreign owned banks is currently less than a tenth of all market shares for banks in a particular nation. One reason the banking industry has not been fully globalized is that it is more convenient to have local banks provide loans to small business and individuals. On the other hand, for large corporations, it is not as important in what nation the bank is in, since the corporation's financial information is available around the globe. See alsoEdit  ReferencesEdit  External linksEdit Guardian Datablog – World's Biggest BanksBanking, Banks, and Credit Unions from UCB Libraries GovPubsA Guide to the National Banking System (PDF). Office of the Comptroller of the Currency (OCC), Washington, D.C. Provides an overview of the national banking system of the USA, its regulation, and the OCC.
Discounts and allowances are reductions to a basic price of goods or services.They can occur anywhere in the distribution channel, modifying either the manufacturer's list price (determined by the manufacturer and often printed on the package), the retail price (set by the retailer and often attached to the product with a sticker), or the list price (which is quoted to a potential buyer, usually in written form).There are many purposes for discounting, including to increase short-term sales, to move out-of-date stock, to reward valuable customers, to encourage distribution channel members to perform a function, or to otherwise reward behaviors that benefit the discount issuer. Some discounts and allowances are forms of sales promotion. Types The most common types of discounts and allowances are listed below. Dealing with payment  Prompt payment discount Trade Discounts are deductions in price given by the wholesaler or manufacturer to the retailer at the list price or catalogue price. Cash Discounts are reductions in price given by the creditor to the debitor to motivate the debitor to make payment with in specified time . These discounts are intended to speed payment and thereby provide liquidity to the firm. They are sometimes used as a promotional device. we also explain that discount is relaxation in price. Examples 2/10 net 30 - this means the buyer must pay within 30 days of the invoice date, but will receive a 2% discount if they pay within 10 days of the invoice date.3/7 EOM - this means the buyer will receive a cash discount of 3% if the bill is paid within 7 days after the end of the month indicated on the invoice date. If an invoice is received on or before the 25th day of the month, payment is due on the 7th day of the next calendar month. If a proper invoice is received after the 25th day of the month, payment is due on the 7th day of the second calendar month.3/7 EOM net 30 - this means the buyer must pay within 30 days of the invoice date, but will receive a 3% discount if they pay within 7 days after the end of the month indicated on the invoice date. If an invoice is received on or before the 25th day of the month, payment is due on the 7th day of the next calendar month. If a proper invoice is received after the 25th day of the month, payment is due on the 7th day of the second calendar month.2/15 net 40 ROG - this means the buyer must pay within 40 days of receipt of goods, but will receive a 2% discount if paid in 15 days of the invoice date. (ROG is short for "Receipt of goods.") Preferred payment method discount Some retailers (particularly small retailers with low margins) offer discounts to customers paying with cash, to avoid paying fees on credit card transactions. Partial payment discount Similar to the Trade discount, this is used when the seller wishes to improve cash flow or liquidity, but finds that the buyer typically is unable to meet the desired discount deadline. A partial discount for whatever payment the buyer makes helps the seller's cash flow partially. Sliding scale A discount offered based on one's ability to pay. More common with non-profit organizations than with for-profit retail. Forward dating This is where the purchaser doesn’t pay for the goods until well after they arrive. The date on the invoice is moved forward - example: purchase goods in November for sale during the December holiday season, but the payment date on the invoice is January 27. Seasonal discount These are price reductions given when an order is placed in a slack period (example: purchasing skis in April in the northern hemisphere, or in September in the southern hemisphere). On a shorter time scale, a happy hour may fall in this category. Generally, this discount is referred to as "X-Dating" or "Ex-Dating". An example of X-Dating would be:3/7 net 30 extra 10 - this means the buyer must pay within 30 days of the invoice date, but will receive a 3% discount if they pay within 7 days after the end of the month indicated on the invoice date plus an extra 10 days. Dealing with trade  Bargaining Bargaining is where the seller and the buyer negotiate a price below the original asking price. Trade discount Trade discounts, also called functional discounts, are payments to distribution channel members for performing some function. Examples of these functions are warehousing and shelf stocking. Trade discounts are often combined to include a series of functions, for example 20/12/5 could indicate a 20% discount for warehousing the product, an additional 12% discount for shipping the product, and an additional 5% discount for keeping the shelves stocked. Trade discounts are most frequent in industries where retailers hold the majority of the power in the distribution channel (referred to as channel captains).Trade discounts are given to try to increase the volume of sales being made by the supplier.The discount described as trade rate discount is sometimes called "trade discount". Trade discount is the discount allowed on retail price of a product or something. for e.g. Retail price of a cream is 25 and trade discount is 2% on 25. Trade rate discount A trade rate discount, sometimes also called "trade discount", is offered by a seller to a buyer for purposes of trade or reselling, rather than to an end user. For example, a pharmacist might offer a discount for over-the-counter drugs to physicians who are purchasing them for dispensing to the physicians' own patients. A seller supplying both trade or resellers, and the general public will have a general list price for anybody, and will offer a trade discount to bona-fide trade customers. Trade-in credit Trade-in credit, also called trade-up credit, is a discount or credit granted for the return of something. The returned item may have little monetary value, as an old version of newer item being bought, or may be worth reselling as second-hand. The idea from a seller's viewpoint is to offer some discount but have the buyer showing some "counter action" to earn this special discount. Sellers like this as the discount granted is not just "given for free" and makes future price/value negotiations easier. Buyers have the advantage of getting some value for something no longer used. Examples can be found in many industries. Dealing with quantity These are price reductions given for large purchases. The rationale behind them is to obtain economies of scale and pass some (or all) of these savings on to the customer. In some industries, buyer groups and co-ops have formed to take advantage of these discounts. Generally there are two types: Cumulative quantity discount Cumulative quantity discounts, also called accumulation discounts, are price reductions based on the quantity purchased over a set period of time. The expectation is that they will impose an implied switching cost and thereby bond the purchaser to the seller. Non-cumulative quantity discount These are price reductions based on the quantity of a single order. The expectation is that they will encourage larger orders, thus reducing billing, order filling, shipping, and sales personnel expenses. Dependence of price on quantity An extreme form of quantity discount occurs when, within a quantity range, the price does not depend on quantity:if one wants less than the minimum amount one has to be pay for the minimum amount anywayif one wants an amount between two of the fixed amounts on offer, one has to pay for the higher amountThese also apply in the case of a service with "quantity" referring to time. For example, an entrance ticket for a zoo is usually for a day; if one stays shorter, the price is the same. It is a kind of pass for unlimited use of a service during a day, where one can distinguish whether or not, when leaving and returning, one has to pay again. Similarly a pass can be for another period. In the case of long periods, it is obvious that one can leave and return without paying again.If one has to buy more than one wants, we can distinguish between the surplus just not being used, or the surplus being a nuisance, e.g. because of having to carry a large container. Dealing with customer characteristics The following discounts have to do with specific characteristics of the customer. Disability discount A discount offered to customers with what is considered to be a disability. Educational or student discount These are price reductions given to members of educational institutions, usually students but possibly also to educators and to other institution staff. The provider's purpose is to build brand awareness early in a buyer's life, or build product familiarity so that after graduation the holder is likely to buy the same product, for own use or for an employer, at its normal price. Providers also offer student discounts as means of offering a product within the budget of a student, which would otherwise be too expensive, thus gaining extra sales. Educational discounts may be given by merchants directly, or via a student discount program. Employee discount A discount offered by a company to employees who buy its products.In 2005, the American automakers ran an "employee discount" for all customers promotional campaign in order to entice buyers, with some success. Military discount A discount offered to customers who are or were members of a military service. A discount is the value that deduct from a service or from goods. Types of military discounts include discounts for active duty military, veterans, retired military personnel, and military spouses or dependents. In the United States, military discounts frequently require proof of ID to show eligibility such as a DD Form 214, DD Form 215, or DD Form 217 from any branch of the Armed Forces, TRICARE Cards, Veterans Affairs Cards Uniformed Services Privilege and Identification Cards (USPIC) or other official documentation. Eligibility for military discounts can also be verified online or via mobile by verification companies like SheerID. Age-related discounts  Toddler discount, child discount, kid discount A discount, or free service, offered to children younger than a certain age, commonly for admission to entertainments and attractions, restaurants, and hotels. There may be a requirement that the child be accompanied by an adult paying full price. Small children often travel free on public transport, and older ones may pay a substantially discounted price; proof of age may be required. Young person's discount Discounts are sometimes offered to young people below a certain age who are neither children nor in education. Senior discount  A discount offered to customers who are above a certain relatively advanced age, typically a round number such as 50, 55, 60, 65, 70, and 75; the exact age varies in different cases. The rationale for a senior discount offered by companies is that the customer is assumed to be retired and living on a limited income, and unlikely to be willing to pay full price; sales at reduced price are better than no sales. Non-commercial organizations may offer concessionary prices as a matter of social policy. Free or reduced-rate travel is often available to older people (see, for example, Freedom Pass). In United States most grocery stores offer senior discounts, starting for those age 50 or older, but most discounts are offered for those over 60. Special prices offered to friends of the seller A discounted price offered to friends of the salesperson, an attitude which is parodied in the stereotype of a salesman saying "It costs [such-and such], but for you..." In Australia, New Zealand, and the UK, discounts to friends are known as "mates' rates." In French this discount is known as prix d'ami. In Spain this is known as "precio de amigo" in Spanish, or "preu d'amic" in Catalan. In German the term "Freundschaftspreis" is commonly used. Special prices offered to local residents Discounts are common in tourist destinations. In Hawaii, for example, many tourist attractions, hotels, and restaurants charge a deeply discounted price to someone who shows proof that they live in Hawaii; this is known as a "Kama'aina discount," after the Hawaiian word for an old-timer or native. It is also known outside of Hawaii but in the Hawaiian islands as a resident discount. Discount card Sometimes a document, typically a plastic card similar to a payment card, is issued as proof of eligibility for discounts. In other cases, existing documents proving status (as student, disabled, resident, etc.) are accepted. Documentation may not be required, for example, for people who are obviously young or old enough to qualify for age-related discounts. In some cases, the card may be issued to anyone who asks. Coupons A discount, either of a certain specified amount or a percentage to the holder of a voucher, usually with certain terms. Commonly, the terms involve the terms of other discounts on this page, such as being valid only if a certain quantity is bought or only if the customer is older than a specified age. Coupons are often printed in newspapers, brochures, and magazines, or can be downloaded and printed from Worldwide Web pages that can be accessed via the Internet. Rebates A refund of part of sometimes the full price of the product following purchase, though some rebates are offered at the time of purchase. A particular case is the promise of a refund in full if applied for in a restricted date range some years in the future; the hope is that the promise will lure customers and increase sales, but that the majority will fail to meet the conditions for a valid claim. Other Promotional allowances - These are price reductions given to the buyer for performing some promotional activity. These include an allowance for creating and maintaining an in-store display or a co-op advertising allowance.Brokerage allowance - From the point of view of the manufacturer, any brokerage fee paid is similar to a promotional allowance. It is usually based on a percentage of the sales generated by the broker. See also Net 30Multi-use tickets for public transportTicket systems References  Further reading Shell, Ellen Ruppel, Cheap: The High Cost of Discount Culture, New York : Penguin Press, 2009. ISBN 978-1-59420-215-5
The IBM Personal Computer, commonly known as the IBM PC, is the original version and progenitor of the IBM PC compatible hardware platform. It is IBM model number 5150, and was introduced on August 12, 1981. It was created by a team of engineers and designers under the direction of Don Estridge of the IBM Entry Systems Division in Boca Raton, Florida.The generic term "personal computer" was in use before 1981, applied as early as 1972 to the Xerox PARC's Alto, but because of the success of the IBM Personal Computer, the term "PC" came to mean more specifically a desktop microcomputer compatible with IBM's PC products. Within a short time of the introduction, third-party suppliers of peripheral devices, expansion cards, and software proliferated; the influence of the IBM PC on the personal computer market was substantial in standardizing a platform for personal computers. "IBM compatible" became an important criterion for sales growth; only the Apple Macintosh family kept significant market share without compatibility with the IBM personal computer. History  Rumors International Business Machines (IBM), one of the world's largest companies, had a 62% share of the mainframe computer market in 1981. Its share of the overall computer market, however, had declined from 60% in 1970 to 32% in 1980. Perhaps distracted by a long-running antitrust lawsuit, the "Colossus of Armonk" completely missed the fast-growing minicomputer market during the 1970s, and was behind rivals such as Wang, Hewlett-Packard (HP), and Control Data in other areas.In 1979 BusinessWeek asked, "Is IBM just another stodgy, mature company?" By 1981 its stock price had declined by 22%. IBM's earnings for the first half the year grew by 5.3%—one third of the inflation rate—while those of minicomputer maker Digital Equipment Corporation (DEC) grew by more than 35%. The company began selling minicomputers, but in January 1982 the United States Department of Justice ended the antitrust suit because, The New York Times reported, the government "recognized what computer experts and securities analysts had long since concluded: I.B.M. no longer dominates the computer business".IBM wished to avoid the same outcome with the new personal computer industry, dominated by the Commodore PET, Atari 8-bit family, Apple II, Tandy Corporation's TRS-80, and various CP/M machines. With $150 million in sales by 1979 and projected annual growth of more than 40% in the early 1980s, the microcomputer market was large enough for IBM's attention. Other large technology companies such as HP, Texas Instruments, and Data General had entered it, and some large IBM customers were buying Apples, so the company saw introducing its own personal computer as both an experiment in a new market and a defense against rivals, large and small.In 1980 and 1981 rumors spread of an IBM personal computer, perhaps a miniaturized version of the IBM System/370, while Matsushita acknowledged that it had discussed with IBM the possibility of manufacturing a personal computer for the American company. The Japanese project, codenamed "Go", ended before the 1981 release of the American-designed IBM PC codenamed "Chess", but two simultaneous projects further confused rumors about the forthcoming product. Too late? Data General and Texas Instruments' small computers were not very successful, but observers expected AT&T to soon enter the computer industry, and other large companies such as Exxon, Montgomery Ward, Pentel, and Sony were designing their own microcomputers. Whether IBM had waited too long to enter an industry in which Apple and others were already successful was unclear.An observer stated that "IBM bringing out a personal computer would be like teaching an elephant to tap dance." Successful microcomputer company Vector Graphic's fiscal 1980 revenue was $12 million. A single IBM computer in the early 1960s cost as much as $9 million, occupied one quarter acre of air-conditioned space, and had a staff of 60 people; in 1980 its least-expensive computer, the 5120, still cost about $13,500. The company only sold through its internal sales force, had no experience with resellers or retail stores, and did not introduce the first product designed to work with non-IBM equipment until 1980.Another observer claimed that IBM made decisions so slowly that, when tested, "what they found is that it would take at least nine months to ship an empty box". As with other large computer companies, its new products typically required about four to five years for development. IBM had to learn how to quickly develop, mass-produce, and market new computers. While the company traditionally let others pioneer a new market—IBM released its first commercial computer a year after Remington Rand's UNIVAC in 1951, but within five years had 85% of the market—the personal-computer development and pricing cycles were much faster than for mainframes, with products designed in a few months and obsolete quickly.Many in the microcomputer industry resented IBM's power and wealth, and disliked the perception that an industry founded by startups needed a latecomer so staid that it had a strict dress code and employee songbook. The potential importance to microcomputers of a company so prestigious, that a popular saying in American companies stated "No one ever got fired for buying IBM", was nonetheless clear. InfoWorld, which described itself as "The Newsweekly for Microcomputer Users", stated that "for my grandmother, and for millions of people like her, IBM and computer are synonymous". Byte ("The Small Systems Journal") stated in an editorial just before the announcement of the IBM PC:Rumors abound about personal computers to come from giants such as Digital Equipment Corporation and the General Electric Company. But there is no contest. IBM's new personal computer ... is far and away the media star, not because of its features, but because it exists at all. When the number eight company in the Fortune 500 enters the field, that is news ... The influence of a personal computer made by a company whose name has literally come to mean "computer" to most of the world is hard to contemplate.The editorial acknowledged that "some factions in our industry have looked upon IBM as the 'enemy'", but concluded with optimism: "I want to see personal computing take a giant step." Predecessors Desktop sized programmable calculators by Hewlett Packard had evolved into the HP 9830 BASIC language computer by 1972. In 1972–1973 a team led by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT, and full-function keyboard. SCAMP emulated an IBM 1130 minicomputer to run APL\1130. In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because it was the first to emulate APL\1130 performance on a portable, single-user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer". The prototype is in the Smithsonian Institution. A non-working industrial design model was also created in 1973 illustrating how the SCAMP engineering prototype could be transformed into a usable product design for the marketplace. IBM executive Bill Lowe used the engineering prototype and design model in his early efforts to demonstrate the viability of creating a single-user computer.Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer in 1975. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton. The 5100 was a complete computer system programmable in BASIC or APL, with a small built-in CRT monitor, keyboard, and tape drive for data storage. It was also very expensive, up to US$20,000; the computer was designed for professional and scientific customers, not business users or hobbyists. BYTE in 1975 announced the 5100 with the headline "Welcome, IBM, to personal computing", but PC Magazine in 1984 described 5100s as "little mainframes" and stated that "as personal computers, these machines were dismal failures ... the antithesis of user-friendly", with no IBM support for third-party software. Despite news reports that it was the first IBM product without a model number, when the PC was introduced in 1981 it was designated as the IBM 5150, putting it in the "5100" series though its architecture was not directly descended from the IBM 5100. Later models followed in the trend: For example, the IBM Portable Personal Computer, PC/XT, and PC AT are IBM machine types 5155, 5160, and 5170, respectively.Following SCAMP, the IBM Boca Raton Laboratory created several single-user computer design concepts to support Lowe's ongoing effort to convince IBM there was a strategic opportunity in the personal computer business. A selection of these early IBM design concepts created in the infancy of personal computing is highlighted in the book ‘’DELETE: A Design History of Computer Vapourware.‘’ One such concept in 1977, code-named Aquarius, was a working prototype utilizing advanced bubble memory cartridges. While this design was more powerful and smaller than Apple II launched the same year, the advanced bubble technology was deemed unstable and not ready for mass production. Project Chess Some employees opposed IBM entering the market. One said, "Why on earth would you care about the personal computer? It has nothing at all to do with office automation." "Besides", he added, "all it can do is cause embarrassment for IBM". The company had determined from studying the market for years, and building the prototypes during the 1970s, that IBM was unable to internally build a personal computer profitably.IBM President John Opel was not among those skeptical of personal computers. He and CEO Frank Cary had created more than one dozen semi-autonomous "Independent Business Units" (IBU) to encourage innovation; Fortune called them "How to start your own company without leaving IBM". After Lowe became the first head of the Entry Level Systems IBU in Boca Raton his team researched the market. Computer dealers were very interested in selling an IBM product, but told Lowe that the company could not design, sell, or service it as IBM had previously done. An IBM microcomputer, they said, must be composed of standard parts that store employees could repair. While dealers disliked Apple's business practices, including a shortage of the Apple II while the company focused on the more sophisticated Apple III, they saw no alternative because they doubted that IBM's traditional sales methods and bureaucracy would change.Atari in 1980 proposed that it act as original equipment manufacturer for an IBM microcomputer. Aware that the company needed to enter the market quickly—even the schools in Broward County, near Boca Raton, purchased Apples—in July 1980 Lowe met with Opel, Cary, and others on the important Corporate Management Committee. Lowe demonstrated the proposal with an industrial design model based on the Atari 800 platform, and suggested acquiring Atari "because we can't do this within the culture of IBM".Cary agreed about the culture, observing that IBM would need "four years and three hundred people" to develop its own personal computer; Lowe, however, promised one in a year if done without traditional IBM methods. Instead of acquiring Atari, the committee allowed him to form an independent group of employees—"the Dirty Dozen", led by engineer Bill Sydnes—which, Lowe promised, could design a prototype in 30 days. The crude prototype barely worked when he demonstrated it in August, but Lowe presented a detailed business plan that proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM practice.The committee agreed that Lowe's approach was the most likely to succeed. With Opel's strong support, in October it approved turning the group into another IBU codenamed "Project Chess" to develop "Acorn", with unusually large funding to help achieve the goal of introducing the product within one year of the August demonstration. After Lowe's promotion Don Estridge became the head of Chess, and by January 1981 the team made its first demonstration of the computer within IBM. Other key members included Sydnes, Lewis Eggebrecht, David Bradley, Mark Dean, and David O'Connor. Many were already hobbyists who owned their own computers including Estridge, who had an Apple II. After the team received permission to expand to 150 by the end of 1980, it received more than 500 calls in one day from IBM employees interested in joining the IBU. Open standards IBM normally was vertically integrated, internally developing all hardware and software and discouraging customers from purchasing third-party products compatible with IBM products. For the PC the company avoided doing so as much as possible; choosing, for example, to license Microsoft BASIC despite having a BASIC of its own for mainframes. Although the company denied doing so, many observers concluded that IBM intentionally emulated Apple when designing the PC. The many Apple II owners on the team influenced its decision to design the computer with an open architecture and publish technical information so others could create software and expansion slot peripherals.Although the company knew that it could not avoid competition from third-party software on proprietary hardware—Digital Research released CP/M-86 for the IBM Displaywriter, for example—it considered using the IBM 801 RISC processor and its operating system, developed at the Thomas J. Watson Research Center in Yorktown Heights, New York. The 801 processor was more than an order of magnitude more powerful than the Intel 8088, and the operating system more advanced than the PC DOS 1.0 operating system from Microsoft. Ruling out an in-house solution made the team’s job much easier and may have avoided a delay in the schedule, but the ultimate consequences of this decision for IBM were far-reaching.IBM had recently developed the Datamaster business microcomputer, which used a processor and other chips from Intel; familiarity with them and the immediate availability of the 8088 was a reason for choosing it for the PC. The 62-pin expansion bus slots were designed to be similar to the Datamaster slots. Differences from the Datamaster included avoiding an all-in-one design while limiting the computer's size so that it would still fit on a standard desktop with the keyboard (also similar to the Datamaster's), and 5.25" disk drives instead of 8". Delays due to in-house development of the Datamaster software was a reason why IBM chose Microsoft BASIC—already available for the 8088—and published available technical information to encourage third-party developers. IBM chose the 8088 over the similar but superior 8086 because Intel offered a better price on the former and could provide more units, and the 8088's 8-bit bus reduced the cost of the rest of the computer.The design for the computer was essentially complete by April 1981, when the manufacturing team took over the project. IBM could not only use its own hardware and make a profit with "Acorn". To save time and money, the IBU built the machine with commercial off-the-shelf parts from original equipment manufacturers whenever possible, with assembly occurring in Boca Raton. The IBU would decide whether it would be more economical to "Make or Buy" each manufacturing step. Various IBM divisions for the first time competed with outsiders to build parts of the new computer; a North Carolina IBM factory built the keyboard, the Endicott, New York factory had to lower its bid for printed circuit boards, and a Taiwanese company built the monitor. The IBU chose an existing monitor from IBM Japan and an Epson printer. Because of the off-the-shelf parts only the system unit and keyboard has unique IBM industrial design elements, the IBM copyright appears in only the ROM BIOS and on the company logo, and the company reportedly received no patents on the PC.Because the product would carry the IBM logo, the only corporate division the IBU could not bypass was the Quality Assurance Unit. Another aspect of IBM that did not change was its emphasis on secrecy. Those working on the project were under strict confidentiality agreements. When an individual mentioned in public on a Saturday that his company was working on software for a new IBM computer, IBM security appeared at the company on Monday to investigate the leak. Developers received prototype computers in boxes lined with lead to block X-rays and sealed with solder, and had to keep them in locked, windowless rooms; to develop software Microsoft emulated the PC on a DEC minicomputer and used the prototype for debugging. After the PC's debut, IBM Boca Raton employees continued to decline to discuss their jobs in public. One writer compared the "silence" after asking one about his role at the company to "hit[ting] the wall at the Boston Marathon: the conversation is over". Debut IBM is proud to announce a product you may have a personal interest in. It's a tool that could soon be on your desk, in your home or in your child's schoolroom. It can make a surprising difference in the way you work, learn or otherwise approach the complexities (and some of the simple pleasures) of living.It's the computer we're making for you.After developing it in 12 months—faster than any other hardware product in company history—IBM announced the Personal Computer on 12 August 1981. Pricing started at US$1,565 (equivalent to $4,123 in 2016) for a configuration with 16K RAM, Color Graphics Adapter, and no disk drives. The company intentionally set prices for it and other configurations that were comparable to those of Apple and other rivals; one analyst stated that IBM "has taken the gloves off", while the company said "we suggest [the PC's price] invites comparison". Microsoft, Personal Software, and Peachtree Software were among the developers of nine launch titles, including EasyWriter and VisiCalc. In addition to the existing corporate sales force IBM opened its own Product Center retail stores. After studying Apple's successful distribution network, the company for the first time sold through others, ComputerLand and Sears Roebuck. Because retail stores receive revenue from repairing computers and providing warranty service, IBM broke a 70-year tradition by permitting and training non-IBM service personnel to fix the PC.BYTE described IBM as having "the strongest marketing organization in the world", but the PC's marketing also differed from that of previous products. The company was aware of its strong corporate reputation among potential customers; an early advertisement began "Presenting the IBM of Personal Computers". The advertisements emphasized the novelty of an individual owning an IBM computer, describing "a product you may have a personal interest in" and asking readers to think of "'My own IBM computer. Imagine that' ... it's yours. For your business, your project, your department, your class, your family and, indeed, for yourself." The Little Tramp After considering Alan Alda, Beverly Sills, Kermit the Frog, and Billy Martin as celebrity endorsers IBM chose Charlie Chaplin's The Little Tramp character—played by Billy Scudder—for a series of advertisements based on Chaplin's films. The very popular and award-winning $36-million marketing campaign made the star of Modern Times—a film that expresses Chaplin's opposition to big business, mechanization, and technological efficiency—the (as Creative Computing described him) "warm cuddly" mascot of one of the world's largest companies.Chaplin and his character became so widely associated with IBM—Time stated that "The Tramp ... has given [it] a human face"—that others used his bowler hat and cane to represent or satirize the company. Although the Chaplin estate sued those like Otrona who used the trademark without permission, PC Magazine's April 1983 issue had 12 advertisements that referred to the Little Tramp. Third-party products "We encourage third-part suppliers [for the PC] ... we are delighted to have them", IBM stated. It did not sell internally developed PC software until April 1984, instead relying on already established software companies. The company contacted Microsoft even before the official approval of Chess, and it and others received cooperation that was, one writer said, "unheard of" for IBM. Such openness surprised observers; BYTE called it "striking" and "startling", and one developer reported that "it's a very different IBM." Another said "They were very open and helpful about giving us all the technical information we needed. The feeling was so radically different—it's like stepping out into a warm breeze." He concluded, "After years of hassling—fighting the Not-Invented-Here attitude—we're the gods."Most other personal-computer companies that did not disclose technical details; Texas Instruments, for example, intentionally made developing third-party TI 99/4A software difficult, even requiring a lockout chip in cartridges. IBM itself kept its mainframe technology so secret that rivals were indicted for industrial espionage. For the PC, however, IBM immediately released detailed information. The US$36 IBM PC Technical Reference Manual included complete circuit schematics, commented ROM BIOS source code, and other engineering and programming information for all of IBM's PC-related hardware, plus instructions on designing third-party peripherals. It was so comprehensive that one reviewer suggested that the manual could serve as a university textbook, and so clear that a developer claimed that he could design an expansion card without seeing the physical computer.IBM marketed the technical manual in full-page color print advertisements, stating that "our software story is still being written. Maybe by you". Sydnes stated that "The definition of a personal computer is third-party hardware and software". Estridge said that IBM did not keep software development proprietary because it would have to "out-VisiCalc VisiCorp and out-Peachtree Peachtree—and you just can't do that", and unlike IBM's own version "Microsoft BASIC had hundreds of thousands of users around the world. How are you going to argue with that?"Another advertisement told developers that the company would consider publishing software for "Education. Entertainment. Personal finance. Data management. Self-improvement. Games. Communications. And yes, business." Estridge explicitly invited small, "cottage" amateur and professional developers to create products "with", he said, "our logo and our support". IBM sold the PC at a large discount to employees, encouraged them to write software, and distributed a catalog of inexpensive software written by individuals that might not otherwise appear in public. Reaction BYTE was correct in predicting that an IBM personal computer would receive much public attention. Its rapid development amazed observers, as did the willingness of the Colossus of Armonk to sell as a launch title Microsoft Adventure (a video game that, its press release stated, brought "players into a fantasy world of caves and treasures"); the company even offered an optional joystick port. Future Computing estimated that "IBM's Billion Dollar Baby" would have $2.3 billion in hardware sales by 1986. David Bunnell, an editor at Osborne/McGraw-Hill, recalled thatNone of my associates wanted to talk about the Apple II or the Osborne I computer anymore, nor did they want to fantasize about writing the next super-selling program ... All they wanted to talk about was the IBM Personal Computer—what it was, its potential and limitations, and most of all, the impact IBM would have on the business of personal computing.Within seven weeks Bunnell helped found PC Magazine, the first periodical for the new computer.Competitors were more skeptical. Adam Osborne said "when you buy a computer from IBM, you buy a la carte. By the time you have a computer that does anything, it will cost more than an Apple. I don't think Apple has anything to worry about." Apple's Mike Markkula agreed that IBM's product was more expensive than the Apple II, and claimed that the Apple III "offers better performance". He denied that the IBM PC offered more memory, stating that his company could offer more than 128K "but frankly we don't know what anyone would do with that memory". Jon Shirley of Tandy admitted that IBM had a "legendary service reputation" but claimed that its thousands of Radio Shack stores "can provide better service", while predicting the IBM PC's "major market will be IBM addicts"; another executive claimed that Tandy could undersell a $3,000 IBM computer by $1,000. Many criticized the PC's design as not innovative and outdated, and believed that its alleged weaknesses, such as the use of single-sided, single-density disks with less storage than the computer's RAM, existed because the company was uncertain about the market and was experimenting before releasing a better computer. (Estridge later boasted, "Many ... said that there was nothing technologically new in this machine. That was the best news we could have had; we actually had done what we had set out to do.")Rivals such as Apple, Tandy, and Commodore—together with more than 50% of the personal-computer market—had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Radio Shack had 14 million customers and 8,000 stores—more than McDonald's—that only sold its broad range of computers and accessories. Apple had five times as many dealers in the US as IBM, an established international distribution network, and an installed base of more than 250,000 customers. Hundreds of independent developers produced software and peripherals for both companies' computers; at least ten Apple databases and ten word processors were available, while the PC had no databases and one word processor. The computer had very limited graphics capability, and customers who wanted both color and high-quality text had to purchase two graphics cards and two monitors.Steve Jobs at Apple ordered a team to examine an IBM PC. After finding it unimpressive—Chris Espinosa called the computer "a half-assed, hackneyed attempt"—the company confidently purchased a full-page advertisement in The Wall Street Journal with the headline "Welcome, IBM. Seriously". Microsoft head Bill Gates was at Apple headquarters the day of IBM's announcement and later said "They didn't seem to care. It took them a full year to realize what had happened". Success The IBM PC was immediately successful. BYTE reported a rumor that more than 40,000 were ordered on the day of the announcement; John Dvorak recalled that one dealer that day praised the computer as an "incredible winner, and IBM knows how to treat us — none of the Apple arrogance". One dealer received 22 $1,000 deposits from customers although he could not promise a delivery date. The company could have sold its entire projected first-year production to employees, and IBM customers that were reluctant to purchase Apples were glad to buy microcomputers from its traditional supplier. By October some referred to the computer simply as the "PC".BYTE estimated that 90% of the 40,000 first-day orders were from software developers. By COMDEX in November Tecmar developed 20 products including memory expansion and expansion chassis, surprising even IBM. Jerry Pournelle reported after attending the West Coast Computer Faire in early 1982 that because IBM "encourages amateurs" with "documents that tell all", "an explosion of [third-party] hardware and software" was visible at the convention. Many manufacturers of professional business application software, who had been planning/developing versions for the Apple II, promptly switched their efforts over to the IBM PC when it was announced. Often, these products needed the capacity and speed of a hard-disk. Although IBM did not offer a hard-disk option for almost two years following introduction of its PC, business sales were nonetheless catalyzed by the simultaneous availability of hard-disk subsystems, like those of Tallgrass Technologies which sold in Computerland stores alongside the IBM 5150 at the introduction in 1981.Although IBM sold fewer than 100,000 PCs in its first year, PC World counted 753 software packages for the PC—more than four times the number available for the Apple Macintosh one year after its 1984 release—including 422 applications and almost 200 utilities and languages. InfoWorld reported that "most of the major software houses have been frantically adapting their programs to run on the PC", with new PC-specific developers composing "an entire subindustry that has formed around the PC's open system", which Dvorak described as a "de facto standard microcomputer". The magazine estimated that "hundreds of tiny garage-shop operations" were in "bloodthirsty" competition to sell peripherals, with 30 to 40 companies in a price war for memory-expansion cards, for example. PC Magazine renamed its planned "1001 Products to Use with Your IBM PC" special issue after the number of product listings it received exceeded the figure. Tecmar and other companies that benefited from IBM's openness rapidly grew in size and importance, as did PC Magazine; within two years it expanded from 96 bimonthly to 800 monthly pages, including almost 500 pages of advertisements.By the end of 1982 IBM was selling one PC every minute of the business day. It estimated that 50 to 70% of PCs sold in retail stores went to the home, and the publicity from selling a popular product to consumers caused IBM to, a spokesman said, "enter the world" by familiarizing them with the Colossus of Armonk. Although the PC only provided two to three percent of sales the company found that it had underestimated demand by as much as 800%. Because its prices were based on forecasts of much lower volume—250,000 over five years, which would have made the PC a very successful IBM product—the PC became very profitable; at times the company sold almost that many computers per month. Estridge claimed in 1983 that from October 1982 to March 1983 customer demand quadrupled. He stated that the company had increased production three times in one year, and warned of a component shortage if demand continued to increase.By mid-1983 Yankee Group estimated that ten new IBM PC-related products appeared every day. In August 1983 the Chess IBU, with 4,000 employees, became the Entry Systems Division, which observers believed indicated that the PC was significantly important to IBM overall, and no longer an experiment. The PC surpassed the Apple II as the best-selling personal computer with more than 750,000 sold by the end of the year, while DEC only sold 69,000 microcomputers in the first nine months of the year despite offering three models for different markets. Retailers also benefited, with 65% of BusinessLand's revenue coming from the PC. Demand still so exceeded supply two years after its debut that, despite IBM shipping 40,000 PCs a month, dealers reportedly received 60% or less of their desired quantity. Pournelle received the PC he paid for in early July 1983 on 1 November, and IBM Boca Raton employees and neighbors had to wait five weeks to buy the computers assembled there. Domination Yankee Group also stated that the PC had by 1983 "destroyed the market for some older machines" from companies like Vector Graphic, North Star, and Cromemco. inCider wrote "This may be an Apple magazine, but let's not kid ourselves, IBM has devoured competitors like a cloud of locusts". By February 1984 BYTE reported on "the phenomenal market acceptance of the IBM PC", and by fall concluded that the company "has given the field its third major standard, after the Apple II and CP/M".By then Apple was less welcoming of the rival that inCider stated had a "godlike" reputation. Its focus on the III had delayed improvements to the II, and the sophisticated Lisa was unsuccessful in part because, unlike the II and the PC, Apple discouraged third-party developers. The head of a retail chain said "It appears that IBM had a better understanding of why the Apple II was successful than had Apple." Jobs, after trying to recruit Estridge to become Apple's president, admitted that in two years IBM had joined Apple as "the industry's two strongest competitors". He warned in a speech before previewing the forthcoming "1984" Super Bowl commercial: "It appears IBM wants it all ... Will Big Blue dominate the entire computer industry? The entire information age? Was George Orwell right about 1984?"IBM had $4 billion in annual PC revenue by 1984, more than twice that of Apple and as much as the sales of Apple, Commodore, HP, and Sperry combined, and 6% of total revenue. A Fortune survey found that 56% of American companies with personal computers used IBM PCs, compared to Apple's 16%. A 1983 study of corporate customers similarly found that two thirds of large customers standardizing on one computer chose the PC, compared to 9% for Apple. IBM's own documentation described the PC as inferior to competitors' less-expensive products, but the company generally did not compete on price; rather, the study found that they preferred "IBM's hegemony" because of its support. Most companies with mainframes used their PCs with the larger computers, which likely benefited IBM's mainframe sales and discouraged their purchasing non-IBM hardware.In 1984 IBM introduced the PC/AT, unlike its predecessor the most sophisticated personal computer from any major company. By 1985 the PC family had more than doubled Future Computing's 1986 revenue estimate, with more than 12,000 applications and 4,500 dealers and distributors worldwide. In his obituary that year, The New York Times wrote that Estridge had led the "extraordinarily successful entry of the International Business Machines Corporation into the personal computer field". The Entry Systems Division had 10,000 employees and by itself would have been the world's third-largest computer company behind IBM and DEC, with more revenue than IBM's minicomputer business despite its much later start. IBM was the only major company with significant minicomputer and microcomputer businesses, in part because rivals like DEC and Wang did not adjust to the retail market.Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. Other manufacturers soon reverse engineered the BIOS to produce their own non-infringing functional copies. Columbia Data Products introduced the first IBM-PC compatible computer in June 1982. In November 1982, Compaq Computer Corporation announced the Compaq Portable, the first portable IBM PC compatible. The first models were shipped in January 1983. IBM PC as standard The success of the IBM computer led other companies to develop IBM Compatibles, which in turn led to branding like diskettes being advertised as "IBM format". An IBM PC clone could be built with off-the-shelf parts, but the BIOS required some reverse engineering. Companies like Compaq, Phoenix Software Associates, American Megatrends, Award, and others achieved fully functional versions of the BIOS, allowing companies like Dell, Gateway and HP to manufacture PCs that worked like IBM's product. The IBM PC became the industry standard. Third-party distribution Because IBM had no retail experience, the retail chains ComputerLand and Sears Roebuck provided important knowledge of the marketplace. They became the main outlets for the new product. More than 190 Computerland stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product. This guaranteed IBM widespread distribution across the U.S.Targeting the new PC at the home market, Sears Roebuck sales failed to live up to expectations. This unfavorable outcome revealed that the strategy of targeting the office market was the key to higher sales. Models All IBM personal computers are software backwards-compatible with each other in general, but not every program will work in every machine. Some programs are time sensitive to a particular speed class. Older programs will not take advantage of newer higher-resolution and higher-color display standards, while some newer programs require newer display adapters. (Note that as the display adapter was an adapter card in all of these IBM models, newer display hardware could easily be, and often was, retrofitted to older models.) A few programs, typically very early ones, are written for and require a specific version of the IBM PC BIOS ROM. Most notably, BASICA which was dependent on the BIOS ROM had a sister program called GW-BASIC which supported more functions, was 100% backwards compatible and could run independently from the BIOS ROM. Original PC The CGA video card, with a suitable modulator, could use an NTSC television set or an RGBi monitor for display; IBM's RGBi monitor was their display model 5153. The other option that was offered by IBM was an MDA and their monochrome display model 5151. It was possible to install both an MDA and a CGA card and use both monitors concurrently if supported by the application program. For example, AutoCAD, Lotus 1-2-3 and others allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Some model 5150 PCs with CGA monitors and a printer port also included the MDA adapter by default, because IBM provided the MDA port and printer port on the same adapter card; it was in fact an MDA/printer port combo card.Although cassette tape was originally envisioned by IBM as a low-budget storage alternative, the most commonly used medium was the floppy disk. The 5150 was available with one or two 5-1/4" floppy drives - with two drives the program disc(s) would be in drive A, while drive B would hold the disc(s) for working files; with one drive the user had to swap program and file discs into the single drive. For models without any drives or storage medium, IBM intended users to connect their own cassette recorder via the 5150's cassette socket. The cassette tape socket was physically the same DIN plug as the keyboard socket and next to it, but electrically completely different.A hard disk could not be installed into the 5150's system unit without changing to a higher-rated power supply (although later drives with lower power consumption have been known to work with the standard 63.5 Watt unit). The "IBM 5161 Expansion Chassis" came with its own power supply and one 10 MB hard disk and allowed the installation of a second hard disk. The system unit had five expansion slots, and the expansion unit had eight; however, one of the system unit's slots and one of the expansion unit's slots had to be occupied by the Extender Card and Receiver Card, respectively, which were needed to connect the expansion unit to the system unit and make the expansion unit's other slots available, for a total of 11 slots. A working configuration required that some of the slots be occupied by display, disk, and I/O adapters, as none of these were built into the 5150's motherboard; the only motherboard external connectors were the keyboard and cassette ports.The simple PC speaker sound hardware was also on board.The original PC's maximum memory using IBM parts was 256 kB, achievable through the installation of 64 kB on the motherboard and three 64 kB expansion cards. The processor was an Intel 8088 running at 4.77 MHz, 4/3 the standard NTSC color burst frequency of 315/88  3.57954 MHz. (In early units, the Intel 8088 used was a 1978 version, later were 1978/81/2 versions of the Intel chip; second-sourced AMDs were used after 1983). Some owners replaced the 8088 with an NEC V20 for a slight increase in processing speed and support for real mode 80186 instructions. The V20 gained its speed increase through the use of a hardware multiplier which the 8088 lacked. An Intel 8087 co-processor could also be added for hardware floating-point arithmetic.IBM sold the first IBM PCs in configurations with 16 or 64 kB of RAM preinstalled using either nine or thirty-six 16-kilobit DRAM chips. (The ninth bit was used for parity checking of memory.) After the IBM XT shipped, the IBM PC motherboard was redesigned with the same RAM configuration as the IBM XT. (64 kB in one bank, expandable to 256kB by populating the other 3 banks.)Although the TV-compatible video board, cassette port and Federal Communications Commission Class B certification were all aimed at making it a home computer, the original PC proved too expensive for the home market. At introduction, a PC with 64 kB of RAM and a single 5.25-inch floppy drive and monitor sold for US $3,005 (equivalent to $7,916 in 2016), while the cheapest configuration (US $1,565) that had no floppy drives, only 16 kB RAM, and no monitor (again, under the expectation that users would connect their existing TV sets and cassette recorders) proved too unattractive and low-spec, even for its time (cf. footnotes to the above IBM PC range table). While the 5150 did not become a top selling home computer, its floppy-based configuration became an unexpectedly large success with businesses. XT The "IBM Personal Computer XT", IBM model 5160, was introduced two years after the PC and featured a 10 megabyte hard drive. It had eight expansion slots but the same processor and clock speed as the PC. The XT had no cassette jack, but still had the Cassette Basic interpreter in ROMs.The XT could take 256 kB of memory on the main board (using 64 kbit DRAM); later models were expandable to 640 kB. The remaining 384 kilobytes of the 8088 address space were used for the BIOS ROM, adapter ROM and RAM space, including video RAM space. It was usually sold with a Monochrome Display Adapter (MDA) video card or a CGA video card.The eight expansion slots were the same as the model 5150 but were spaced closer together. Although rare, a card designed for the 5150 could be wide enough to obstruct the adjacent slot in an XT. Because of the spacing, an XT motherboard would not fit into a case designed for the PC motherboard, but the slots and peripheral cards were compatible. The XT expansion bus (later called "8 bit Industry Standard Architecture" (ISA) by competitors) was retained in the IBM AT, which added connectors for some slots to allow 16-bit transfers; 8 bit cards could be used in an AT. XT/370 The "IBM Personal Computer XT/370" was an XT with three custom 8-bit cards: the processor card (370PC-P) contained a modified Motorola 68000 chip, microcoded to execute System/370 instructions, a second 68000 to handle bus arbitration and memory transfers, and a modified 8087 to emulate the S/370 floating point instructions. The second card (370PC-M) connected to the first and contained 512 kB of memory. The third card (PC3277-EM), was a 3270 terminal emulator necessary to install the system software for the VM/PC software to run the processors.The computer booted into DOS, then ran the VM/PC Control Program. PCjr The "IBM PCjr" was IBM's first attempt to enter the market for relatively inexpensive educational and home-use personal computers. The PCjr, IBM model number 4860, retained the IBM PC's 8088 CPU and BIOS interface for compatibility, but its cost and differences in the PCjr's architecture, as well as other design and implementation decisions, eventually led to the PCjr, and the related IBM JX, being commercial failures. Portable The "IBM Portable Personal Computer" 5155 model 68 was an early portable computer developed by IBM after the success of Compaq's suitcase-size portable machine (the Compaq Portable). It was released in February, 1984, and was eventually replaced by the IBM Convertible.The Portable was an XT motherboard, transplanted into a Compaq-style luggable case. The system featured 256 kilobytes of memory (expandable to 512 kB), an added CGA card connected to an internal monochrome (amber) composite monitor, and one or two half-height 5.25" 360K floppy disk drives. Unlike the Compaq Portable, which used a dual-mode monitor and special display card, IBM used a stock CGA board and a composite monitor, which had lower resolution. It could however, display color if connected to an external monitor or television. AT The "IBM Personal Computer/AT" (model 5170), announced August 15, 1984, used an Intel 80286 processor, originally running at 6 MHz. It had a 16-bit ISA bus and 20 MB hard drive. A faster model, running at 8 MHz and sporting a 30-megabyte hard disk was introduced in 1986.The AT was designed to support multitasking; the new SysRq (System request key), little noted and often overlooked, is part of this design, as is the 80286 itself, the first Intel 16-bit processor with multitasking features (i.e. the 80286 protected mode). IBM made some attempt at marketing the AT as a multi-user machine, but it sold mainly as a faster PC for power users. For the most part, IBM PC/ATs were used as more powerful DOS (single-tasking) personal computers, in the literal sense of the PC name.Early PC/ATs were plagued with reliability problems, in part because of some software and hardware incompatibilities, but mostly related to the internal 20 MB hard disk, and High Density Floppy Disk Drive.While some people blamed IBM's hard disk controller card and others blamed the hard disk manufacturer Computer Memories Inc. (CMI), the IBM controller card worked fine with other drives, including CMI's 33-MB model. The problems introduced doubt about the computer and, for a while, even about the 286 architecture in general, but after IBM replaced the 20 MB CMI drives, the PC/AT proved reliable and became a lasting industry standard.IBM AT's Drive parameter table listed the CMI-33 as having 615 cylinders instead of the 640 the drive was designed with, as to make the size an even 30 MB. Those who re-used the drives mostly found that the 616th cylinder was bad due to it being used as a landing area. AT/370 The "IBM Personal Computer AT/370" was an AT with two custom 16-bit cards, running almost exactly the same setup as the XT/370. Convertible The IBM PC Convertible, released April 3, 1986, was IBM's first laptop computer and was also the first IBM computer to utilize the 3.5" floppy disk which went on to become the standard. Like modern laptops, it featured power management and the ability to run from batteries. It was the follow-up to the IBM Portable and was model number 5140. The concept and the design of the body was made by the German industrial designer Richard Sapper.It utilized an Intel 80c88 CPU (a CMOS version of the Intel 8088) running at 4.77 MHz, 256 kB of RAM (expandable to 640 kB), dual 720 kB 3.5" floppy drives, and a monochrome CGA-compatible LCD screen at a price of $2,000. It weighed 13 pounds (5.8 kg) and featured a built-in carrying handle.The PC Convertible had expansion capabilities through a proprietary ISA bus-based port on the rear of the machine. Extension modules, including a small printer and a video output module, could be snapped into place. The machine could also take an internal modem, but there was no room for an internal hard disk. Next-generation IBM PS/2 The IBM PS/2 line was introduced in 1987. The Model 30 at the bottom end of the lineup was very similar to earlier models; it used an 8086 processor and an ISA bus. The Model 30 was not "IBM compatible" in that it did not have standard 5.25-inch drive bays; it came with a 3.5-inch floppy drive and optionally a 3.5-inch-sized hard disk. Most models in the PS/2 line further departed from "IBM compatible" by replacing the ISA bus completely with Micro Channel Architecture. Technology  Electronics The main circuit board in an PC is called the motherboard (IBM terminology calls it a planar). This mainly carries the CPU and RAM, and it has a bus with slots for expansion cards. On the motherboard are also the ROM subsystem, DMA and IRQ controllers, coprocessor socket, sound (PC speaker, tone generation) circuitry, and keyboard interface. The original PC also has a cassette interface.The bus used in the original PC became very popular, and it was subsequently named ISA. While it was popular, it was more commonly known as the PC-bus or XT-bus; the term ISA arose later when industry leaders chose to continue manufacturing machines based on the IBM PC AT architecture rather than license the PS/2 architecture and its MCA bus from IBM. The XT-bus was then retroactively named 8-bit ISA or XT ISA, while the unqualified term ISA usually refers to the 16-bit AT-bus (as better defined in the ISA specifications.) The AT-bus is an extension of the PC-/XT-bus and is in use to this day in computers for industrial use, where its relatively low speed, 5 volt signals, and relatively simple, straightforward design (all by year 2011 standards) give it technical advantages (e.g. noise immunity for reliability).A monitor and any floppy or hard disk drives are connected to the motherboard through cables connected to graphics adapter and disk controller cards, respectively, installed in expansion slots. Each expansion slot on the motherboard has a corresponding opening in the back of the computer case through which the card can expose connectors; a blank metal cover plate covers this case opening (to prevent dust and debris intrusion and control airflow) when no expansion card is installed. Memory expansion beyond the amount installable on the motherboard was also done with boards installed in expansion slots, and I/O devices such as parallel, serial, or network ports were likewise installed as individual expansion boards. For this reason, it was easy to fill the five expansion slots of the PC, or even the eight slots of the XT, even without installing any special hardware. Companies like Quadram and AST addressed this with their popular multi-I/O cards which combine several peripherals on one adapter card that uses only one slot; Quadram offered the QuadBoard and AST the SixPak.Intel 8086 and 8088-based PCs require expanded memory (EMS) boards to work with more than 640 kB of memory. (Though the 8088 can address one megabyte of memory, the last 384 kB of that is used or reserved for the BIOS ROM, BASIC ROM, extension ROMs installed on adapter cards, and memory address space used by devices including display adapter RAM and even the 64 kB EMS page frame itself.) The original IBM PC AT used an Intel 80286 processor which can access up to 16 MB of memory (though standard DOS applications cannot use more than one megabyte without using additional APIs.) Intel 80286-based computers running under OS/2 can work with the maximum memory. Peripheral integrated circuits The set of peripheral chips selected for the original IBM PC defined the functionality of an IBM compatible. These became the de facto base for later application specific integrated circuits (ASICs) used in compatible products.The original system chips were one Intel 8259 programmable interrupt controller (PIC) (at I/O address 0x20), one Intel 8237 direct memory access (DMA) controller (at I/O address 0x00), and an Intel 8253 programmable interval timer (PIT) (at I/O address 0x40). The PIT provides the 18.2 Hz clock ticks, dynamic memory refresh timing, and can be used for speaker output; one DMA channel is used to perform the memory refresh.The mathematics coprocessor was the Intel 8087 using I/O address 0xF0. This was an option for users who needed extensive floating-point arithmetic, such as users of computer-aided drafting.The IBM PC AT added a second, slave 8259 PIC (at I/O address 0xA0), a second 8237 DMA controller for 16-bit DMA (at I/O address 0xC0), a DMA address register (implemented with a 74LS612 IC) (at I/O address 0x80), and a Motorola MC146818 real-time clock (RTC) with nonvolatile memory (NVRAM) used for system configuration (replacing the DIP switches and jumpers used for this purpose in PC and PC/XT models (at I/O address 0x70). On expansion cards, the Intel 8255 programmable peripheral interface (PPI) (at I/O addresses 0x378 is used for parallel I/O controls the printer, and the 8250 universal asynchronous receiver/transmitter (UART) (at I/O address 0x3F8 or 0x3E8) controls the serial communication at the (pseudo-) RS-232 port. Joystick port IBM offered a Game Control Adapter for the PC, which supported analog joysticks similar to those on the Apple II. Although analog controls proved inferior for arcade-style games, they were an asset in certain other genres such as flight simulators. The joystick port on the IBM PC supported two controllers, but required a Y-splitter cable to connect both at once. It remained the standard joystick interface on IBM compatibles until being replaced by USB during the 2000s. Keyboard The keyboard that came with the IBM 5150 was an extremely reliable and high-quality electronic keyboard originally developed in North Carolina for the Datamaster. Each key was rated to be reliable to over 100 million keystrokes. For the IBM PC, a separate keyboard housing was designed with a novel usability feature that allowed users to adjust the keyboard angle for personal comfort. Compared with the keyboards of other small computers at the time, the IBM PC keyboard was far superior and played a significant role in establishing a high-quality impression. For example, the industrial design of the adjustable keyboard, together with the system unit, was recognized with a major design award. Byte magazine in the fall of 1981 went so far as to state that the keyboard was 50% of the reason to buy an IBM PC. The importance of the keyboard was definitely established when the 1983 IBM PCjr flopped, in very large part for having a much different and mediocre Chiclet keyboard that made a poor impression on customers. Oddly enough, the same thing almost happened to the original IBM PC when in early 1981 management seriously considered substituting a cheaper and lower quality keyboard. This mistake was narrowly avoided on the advice of one of the original development engineers.However, the original 1981 IBM PC 83-key keyboard was criticized by typists for its non-standard placement of the Return and left ⇧ Shift keys, and because it did not have separate cursor and numeric pads that were popular on the pre-PC DEC VT100 series video terminals. In 1982, Key Tronic introduced the now standard 101-key PC keyboard. In 1984, IBM corrected the Return and left ⇧ Shift keys on its AT keyboard, but shortened the Backspace key, making it harder to reach. In 1986, IBM changed to the 101 key enhanced keyboard, which added the separate cursor and numeric key pads, relocated all the function keys and the Ctrl keys, and the Esc key was also relocated to the opposite side of the keyboard.Another feature of the original keyboard is the relatively loud "click" sound each key made when pressed. Since typewriter users were accustomed to keeping their eyes on the hardcopy they were typing from and had come to rely on the mechanical sound that was made as each character was typed onto the paper to ensure that they had pressed the key hard enough (and only once), the PC keyboard used a keyswitch that produced a click and tactile bump intended to provide that same reassurance.The IBM PC keyboard is very robust and flexible. The low-level interface for each key is the same: each key sends a signal when it is pressed and another signal when it is released. An integrated microcontroller in the keyboard scans the keyboard and encodes a "scan code" and "release code" for each key as it is pressed and released separately. Any key can be used as a shift key, and a large number of keys can be held down simultaneously and separately sensed. The controller in the keyboard handles typematic operation, issuing periodic repeat scan codes for a depressed key and then a single release code when the key is finally released.An "IBM PC compatible" may have a keyboard that does not recognize every key combination a true IBM PC does, such as shifted cursor keys. In addition, the "compatible" vendors sometimes used proprietary keyboard interfaces, preventing the keyboard from being replaced.Although the PC/XT and AT used the same style of keyboard connector, the low-level protocol for reading the keyboard was different between these two series. The AT keyboard uses a bidirectional interface which allows the computer to send commands to the keyboard. An AT keyboard could not be used in an XT, nor the reverse. Third-party keyboard manufacturers provided a switch on some of their keyboards to select either the AT-style or XT-style protocol for the keyboard. Character set The original IBM PC used the 7-bit ASCII alphabet as its basis, but extended it to 8 bits with nonstandard character codes. This character set was not suitable for some international applications, and soon a veritable cottage industry emerged providing variants of the original character set in various national variants. In IBM tradition, these variants were called code pages. These codings are now obsolete, having been replaced by more systematic and standardized forms of character coding, such as ISO 8859-1, Windows-1251 and Unicode. The original character set is known as code page 437. Storage media  Cassette tape IBM equipped the model 5150 with a cassette port for connecting a cassette drive and assumed that home users would purchase the low-end model and save files to cassette tapes as was typical of home computers of the time. However, adoption of the floppy- and monitor-less configuration was low; few (if any) IBM PCs left the factory without a floppy disk drive installed. Also, DOS was not available on cassette tape, only on floppy disks (hence "Disk Operating System"). 5150s with just external cassette recorders for storage could only use the built-in ROM BASIC as their operating system. As DOS saw increasing adoption, the incompatibility of DOS programs with PCs that used only cassettes for storage made this configuration even less attractive. The ROM BIOS supported cassette operations.The IBM PC cassette interface encodes data using frequency modulation with a variable data rate. Either a one or a zero is represented by a single cycle of a square wave, but the square wave frequencies differ by a factor of two, with ones having the lower frequency. Therefore, the bit periods for zeros and ones also differ by a factor of two, with the unusual effect that a data stream with more zeros than ones will use less tape (and time) than an equal-length (in bits) data stream containing more ones than zeros, or equal numbers of each.IBM also had an exclusive license agreement with Microsoft to include BASIC in the ROM of the PC; clone manufacturers could not have ROM BASIC on their machines, but it also became a problem as the XT, AT, and PS/2 eliminated the cassette port and IBM was still required to install the (now useless) BASIC with them. The agreement finally expired in 1991 when Microsoft replaced BASICA/GW-BASIC with QBASIC. The main core BASIC resided in ROM and "linked" up with the RAM-resident BASIC.COM/BASICA.COM included with PC-DOS (they provided disk support and other extended features not present in ROM BASIC). Because BASIC was over 50 kB in size, this served a useful function during the first three years of the PC when machines only had 64 -128 kB of memory, but became less important by 1985. For comparison, clone makers such as Compaq were forced to include a version of BASIC that resided entirely in RAM. Floppy diskettes Most or all 5150 PCs had one or two 5.25-inch floppy disk drives. These were either single-sided double-density (SSDD) or double-sided double-density (DSDD) drives. The IBM PC never used single density floppy drives. The drives and disks were commonly referred to by capacity, such as "160KB floppy disk" or "360KB floppy drive". DSDD drives were backwards compatible; they could read and write SSDD floppies. The same type of physical diskette media could be used for both drives, but a disk formatted for double-sided use could not be read on a single-sided drive.The disks were Modified Frequency Modulation (MFM) coded in 512-byte sectors, and were soft-sectored. They contained 40 tracks per side at the 48 track per inch (TPI) density, and initially were formatted to contain eight sectors per track. This meant that SSDD disks initially had a formatted capacity of 160 kB, while DSDD disks had a capacity of 320 kB. However, the DOS operating system was later updated to allow formatting the disks with nine sectors per track. This yielded a formatted capacity of 180 kB with SSDD disks/drives, and 360 kB with DSDD disks/drives. The unformatted capacity of the floppy disks was advertised as "250KB" for SSDD and "500KB" for DSDD ("KB" ambiguously referring to either 1000 or 1024 bytes; essentially the same for rounded-off values), however these "raw" 250/500 kB were not the same thing as the usable formatted capacity; under DOS, the maximum capacity for SSDD and DSDD disks was 180 kB and 360 kB, respectively. Regardless of type, the file system of all floppy disks (under DOS) was FAT12.The earliest IBM PCs had only single-sided floppy drives until double-sided drives became available in the spring of 1982. After the upgraded 64k-256k motherboard PCs arrived in early 1983, single-sided drives and the cassette model were discontinued.IBM's original floppy disk controller card also included an external 37-pin D-shell connector. This allowed users to connect additional external floppy drives by third party vendors, but IBM did not offer their own external floppies until 1986.The industry-standard way of setting floppy drive numbers was via setting jumper switches on the drive unit, however IBM chose to instead use a method known as the "cable twist" which had a floppy data cable with a bend in the middle of it that served as a switch for the drive motor control. This eliminated the need for users to adjust jumpers while installing a floppy drive. Fixed disks The 5150 could not itself power hard drives without retrofitting a stronger power supply, but IBM later offered the 5161 Expansion Unit, which not only provided more expansion slots, but also included a 10 MB (later 20 MB) hard drive powered by the 5161's own separate 130-watt power supply. The IBM 5161 Expansion Unit was released in early 1983.During the first year of the IBM PC, it was commonplace for users to install third-party Winchester hard disks which generally connected to the floppy controller and required a patched version of PC-DOS which treated them as a giant floppy disk (there was no subdirectory support).IBM began offering hard disks with the XT, however the original PC was never sold with them. Nonetheless, many users installed hard disks and upgraded power supplies in them.After floppy disks became obsolete in the early 2000s, the letters A and B became unused. But for 25 years, virtually all DOS-based PC software assumed the program installation drive was C, so the primary HDD continues to be "the C drive" even today. Other operating system families (e.g. Unix) are not bound to these designations. OS support Which operating system IBM customers would choose was at first unclear. Although the company expected that most would use PC DOS IBM supported using CP/M-86—which became available six months after DOS—or UCSD p-System as operating systems. IBM promised that it would not favor one operating system over the others; the CP/M-86 support surprised Gates, who claimed that IBM was "blackmailed into it". IBM was correct, nonetheless, in its expectation; one survey found that 96.3% of PCs were ordered with the $40 DOS compared to 3.4% for the $240 CP/M-86.The IBM PC's ROM BASIC and BIOS supported cassette tape storage. PC DOS itself did not support cassette tape storage. PC DOS version 1.00 supported only 160 kB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160 kB SSDD and 320 kB DSDD floppies. Support for the slightly larger nine sector per track 180 kB and 360 kB formats arrived 10 months later in March 1983. BIOS The BIOS (Basic Input/Output System) provided the core ROM code for the PC. It contained a library of functions that software could call for basic tasks such as video output, keyboard input, and disk access in addition to interrupt handling, loading the operating system on boot-up, and testing memory and other system components.The original IBM PC BIOS was 8k in size and occupied four 2k ROM chips on the motherboard, with a fifth and sixth empty slot left for any extra ROMs the user wished to install. IBM offered three different BIOS revisions during the PC's lifespan. The initial BIOS was dated April 1981 and came on the earliest models with single-sided floppy drives and PC DOS 1.00. The second version was dated October 1981 and arrived on the "Revision B" models sold with double-sided drives and PC DOS 1.10. It corrected some bugs, but was otherwise unchanged. Finally, the third BIOS version was dated October 1982 and found on all IBM PCs with the newer 64k-256k motherboard. This revision was more-or-less identical to the XT's BIOS. It added support for detecting ROMs on expansion cards as well as the ability to use 640k of memory (the earlier BIOS revisions had a limit of 544k). Unlike the XT, the original PC remained functionally unchanged from 1983 until its discontinuation in early 1987 and did not get support for 101-key keyboards or 3.5" floppy drives, nor was it ever offered with half-height floppies. Video output IBM initially offered two video adapters for the PC, the Color/Graphics Adapter and the Monochrome Display and Printer Adapter. CGA was intended to be a typical home computer display; it had NTSC output and could be connected to a composite monitor or a TV set with an RF modulator in addition to RGB for digital RGBI-type monitors, although IBM did not offer their own RGB monitor until 1983. Supported graphics modes were 40 or 80x25 color text with 8x8 character resolution, 320x200 bitmap graphics with two fixed 4-color palettes, or 640x200 monochrome graphics.The MDA card and its companion 5151 monitor supported only 80x25 text with a 9x14 character resolution (total pixel resolution was 720x350). It was mainly intended for the business market and so also included a printer port.During 1982, the first third-party video card for the PC appeared when Hercules Computer Technologies released a clone of the MDA that could use bitmap graphics. Although not supported by the BIOS, the Hercules Graphics Adapter became extremely popular for business use due to allowing sharp, high resolution graphics plus text and itself was widely cloned by other manufacturers.In 1985, after the launch of the IBM AT, the new Enhanced Graphics Adapter became available which could support 320x200 or 640x200 in 16 colors in addition to high-resolution 640x350 16 color graphics.IBM also offered a video board for the PC, XT, and AT known as the Professional Graphics Adapter during 1984-86, mainly intended for CAD design. It was extremely expensive, required a special monitor, and was rarely ordered by customers.VGA graphics cards could also be installed in IBM PCs and XTs, although they were introduced after the computer's discontinuation. Serial port addresses and interrupts The serial port is an 8250 or a derivative (such as the 16450 or 16550), mapped to eight consecutive IO addresses and one interrupt request line.Only COM1: and COM2: addresses were defined by the original PC. Attempts to share IRQ3 and IRQ4 to use additional ports require special measures in hardware and software, since shared IRQs were not defined in the original PC design. The most typical devices plugged into the serial port were modems and mice. Plotters and serial printers were also among the more commonly used serial peripherals, and there were numerous other more unusual uses such as operating cash registers, factory equipment, and connecting terminals. Printer port IBM made a deal with Japan-based Epson to produce printers for the PC and all IBM-branded printers were manufactured by that company (Epson of course also sold printers with their own name). There was a considerable amount of controversy when IBM included a printer port on the PC that did not follow the industry-standard Centronics design, and it was rumored that this had been done to prevent customers from using non-Epson/IBM printers with their machines (plugging a Centronics printer into an IBM PC could damage the printer, the parallel port, or both). Although third-party cards were available with Centronics ports on them, PC clones quickly copied the IBM printer port and by the late 80s, it had largely displaced the Centronics standard. Reception BYTE wrote in October 1981 that the IBM PC's "hardware is impressive, but even more striking are two decisions made by IBM: to use outside suppliers already established in the microcomputer industry, and to provide information and assistance to independent, small-scale software writers and manufacturers of peripheral devices". It praised the "smart" hardware design and stated that its price was not much higher than the 8-bit machines from Apple and others. The reviewer admitted that the computer "came as a shock. I expected that the giant would stumble by overestimating or underestimating the capabilities the public wants and stubbornly insisting on incompatibility with the rest of the microcomputer world. But IBM didn't stumble at all; instead, the giant jumped leagues in front of the competition ... the only disappointment about the IBM Personal Computer is its dull name".In a more detailed review in January 1982, BYTE called the IBM PC "a synthesis of the best the microcomputer industry has offered to date ... as well designed on the inside as it is on the outside". The magazine praised the keyboard as "bar none, the best ... on any microcomputer", describing the unusual Shift key locations as "minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard". The review also complimented IBM's manuals, which it predicted "will set the standard for all microcomputer documentation in the future. Not only are they well packaged, well organized, and easy to understand, but they are also complete". Observing that detailed technical information was available "much earlier ... than it has been for other machines", the magazine predicted that "given a reasonable period of time, plenty of hardware and software will probably be developed for" the computer. The review stated that although the IBM PC cost more than comparably configured Apple II and TRS-80 computers, and the insufficient number of slots for all desirable expansion cards was its most serious weakness, "you get a lot more for your money" and concluded, "In two years or so, I think [it] will be one of the most popular and best-supported ... IBM should be proud of the people who designed it".In a special 1984 issue dedicated to the IBM PC, BYTE concluded that the PC had succeeded both because of its features like an 80-column screen, open architecture, and high-quality keyboard, and "the failure of other major companies to provide these same fundamental features earlier. In retrospect, it seems IBM stepped into a void that remained, paradoxically, at the center of a crowded market". Longevity Many IBM PCs have remained in service long after their technology became largely obsolete. In June 2006, IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, used to process data as it is returned from the ascending radiosonde, attached to a weather balloon, although they have been slowly phased out. Factors that have contributed to the 5150 PC's longevity are its flexible modular design, its open technical standard (making information needed to adapt, modify, and repair it readily available), use of few special nonstandard parts, and rugged high-standard IBM manufacturing, which provided for exceptional long-term reliability and durability.Some of the mechanical aspects of the slot specifications are still used in current PCs. A few systems still come with PS/2 style keyboard and mouse connectors. Collectability The IBM model 5150 Personal Computer has become a collectable among vintage computer collectors, due to the system being the first true “PC” as we know them today. Today these systems can fetch anywhere from $100 to $4500, depending on cosmetic and operational condition. The IBM model 5150 has proven to be reliable; despite their age of 30 years or more, some still function as they did when new. See also  Notes  References Cited referencesGeneral references Further reading "Birth of the IBM PC", IBM Corporation History Archives website External links IBM SCAMPIBM 5150 information at www.minuszerodegrees.netIBM PC 5150 System Disks and ROMsIBM PC from IT DictionaryIBM PC history and technical informationWhat a legacy! The IBM PC's 25 year legacyCNN.com - IBM PC turns 25IBM-5150 and collection of old digital and analog computers at oldcomputermuseum.comIBM PC images and informationA brochure from November, 1982 advertising the IBM PCA Picture of the XT/370 cards, showing the dual 68000 processorsThe History Of The IBM Personal Computer
Microsoft Corporation /ˈmaɪkrəˌsɒft, -roʊ-, -ˌsɔːft/ (known professionally as Microsoft, formerly stylized as Microsoft and abbreviated as MS) is an American multinational technology company headquartered in Redmond, Washington, that develops, manufactures, licenses, supports and sells computer software, consumer electronics and personal computers and services. Its best known software products are the Microsoft Windows line of operating systems, Microsoft Office office suite, and Internet Explorer and Edge web browsers. Its flagship hardware products are the Xbox video game consoles and the Microsoft Surface tablet lineup. As of 2016, it is the world's largest software maker by revenue, and one of the world's most valuable companies.Microsoft was founded by Paul Allen and Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Microsoft Windows. The company's 1986 initial public offering (IPO), and subsequent rise in its share price, created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made a number of corporate acquisitions. In May 2011, Microsoft acquired Skype Technologies for $8.5 billion, and in December 2016 bought LinkedIn for $26.2 billion.As of 2015, Microsoft is market-dominant in the IBM PC-compatible operating system market and the office software suite market, although it has lost the majority of the overall operating system market to Android. The company also produces a wide range of other software for desktops and servers, and is active in areas including Internet search (with Bing), the video game industry (with the Xbox, Xbox 360 and Xbox One consoles), the digital services market (through MSN), and mobile phones (via the operating systems of Nokia's former phones and Windows Phone OS). In June 2012, Microsoft entered the personal computer production market for the first time, with the launch of the Microsoft Surface, a line of tablet computers. With the acquisition of Nokia's devices and services division to form Microsoft Mobile, the company re-entered the smartphone hardware market, after its previous attempt, Microsoft Kin, which resulted from their acquisition of Danger Inc.The word "Microsoft" is a portmanteau of "microcomputer" and "software". History  1972–1984: Founding and company beginnings Paul Allen and Bill Gates, childhood friends with a passion for computer programming, sought to make a successful business utilizing their shared skills. In 1972 they founded their first company, named Traf-O-Data, which offered a rudimentary computer that tracked and analyzed automobile traffic data. Allen went on to pursue a degree in computer science at Washington State University, later dropping out of school to work at Honeywell. Gates began studies at Harvard. The January 1975 issue of Popular Electronics featured Micro Instrumentation and Telemetry Systems's (MITS) Altair 8800 microcomputer. Allen suggested that they could program a BASIC interpreter for the device; after a call from Gates claiming to have a working interpreter, MITS requested a demonstration. Since they didn't actually have one, Allen worked on a simulator for the Altair while Gates developed the interpreter. Although they developed the interpreter on a simulator and not the actual device, the interpreter worked flawlessly when they demonstrated the interpreter to MITS in Albuquerque, New Mexico in March 1975; MITS agreed to distribute it, marketing it as Altair BASIC. They officially established Microsoft on April 4, 1975, with Gates as the CEO. Allen came up with the original name of "Micro-Soft," as recounted in a 1995 Fortune magazine article. In August 1977 the company formed an agreement with ASCII Magazine in Japan, resulting in its first international office, "ASCII Microsoft". The company moved to a new home in Bellevue, Washington in January 1979.Microsoft entered the OS business in 1980 with its own version of Unix, called Xenix. However, it was MS-DOS that solidified the company's dominance. After negotiations with Digital Research failed, IBM awarded a contract to Microsoft in November 1980 to provide a version of the CP/M OS, which was set to be used in the upcoming IBM Personal Computer (IBM PC). For this deal, Microsoft purchased a CP/M clone called 86-DOS from Seattle Computer Products, branding it as MS-DOS, which IBM rebranded to PC DOS. Following the release of the IBM PC in August 1981, Microsoft retained ownership of MS-DOS. Since IBM copyrighted the IBM PC BIOS, other companies had to reverse engineer it in order for non-IBM hardware to run as IBM PC compatibles, but no such restriction applied to the operating systems. Due to various factors, such as MS-DOS's available software selection, Microsoft eventually became the leading PC operating systems vendor. The company expanded into new markets with the release of the Microsoft Mouse in 1983, as well as with a publishing division named Microsoft Press. Paul Allen resigned from Microsoft in 1983 after developing Hodgkin's disease. 1984–1994: Windows and Office While jointly developing a new OS with IBM in 1984, OS/2, Microsoft released Microsoft Windows, a graphical extension for MS-DOS, on November 20, 1985. Microsoft moved its headquarters to Redmond on February 26, 1986, and on March 13 the company went public; the ensuing rise in the stock would make an estimated four billionaires and 12,000 millionaires from Microsoft employees. Due to the partnership with IBM, in 1990 the Federal Trade Commission set its eye on Microsoft for possible collusion; it marked the beginning of over a decade of legal clashes with the U.S. government. Microsoft released its version of OS/2 to original equipment manufacturers (OEMs) on April 2, 1987; meanwhile, the company was at work on a 32-bit OS, Microsoft Windows NT, using ideas from OS/2; it shipped on July 21, 1993, with a new modular kernel and the Win32 application programming interface (API), making porting from 16-bit (MS-DOS-based) Windows easier. Once Microsoft informed IBM of NT, the OS/2 partnership deteriorated.In 1990, Microsoft introduced its office suite, Microsoft Office. The software bundled separate office productivity applications, such as Microsoft Word and Microsoft Excel. On May 22 Microsoft launched Windows 3.0 with a streamlined user interface graphics and improved protected mode capability for the Intel 386 processor. Both Office and Windows became dominant in their respective areas. Novell, a Word competitor from 1984–1986, filed a lawsuit years later claiming that Microsoft left part of its APIs undocumented in order to gain a competitive advantage.On July 27, 1994, the U.S. Department of Justice, Antitrust Division filed a Competitive Impact Statement that said, in part: "Beginning in 1988, and continuing until July 15, 1994, Microsoft induced many OEMs to execute anti-competitive "per processor" licenses. Under a per processor license, an OEM pays Microsoft a royalty for each computer it sells containing a particular microprocessor, whether the OEM sells the computer with a Microsoft operating system or a non-Microsoft operating system. In effect, the royalty payment to Microsoft when no Microsoft product is being used acts as a penalty, or tax, on the OEM's use of a competing PC operating system. Since 1988, Microsoft's use of per processor licenses has increased." 1995–2007: Internet and the 32-bit era Following Bill Gates's internal "Internet Tidal Wave memo" on May 26, 1995, Microsoft began to redefine its offerings and expand its product line into computer networking and the World Wide Web. The company released Windows 95 on August 24, 1995, featuring pre-emptive multitasking, a completely new user interface with a novel start button, and 32-bit compatibility; similar to NT, it provided the Win32 API. Windows 95 came bundled with the online service MSN (which was at first intended to be a competitor to the Internet), and for OEMs Internet Explorer, a web browser. Internet Explorer was not bundled with the retail Windows 95 boxes because the boxes were printed before the team finished the web browser, and instead was included in the Windows 95 Plus! pack. Branching out into new markets in 1996, Microsoft and NBC Universal created a new 24/7 cable news station, MSNBC. Microsoft created Windows CE 1.0, a new OS designed for devices with low memory and other constraints, such as personal digital assistants. In October 1997, the Justice Department filed a motion in the Federal District Court, stating that Microsoft violated an agreement signed in 1994 and asked the court to stop the bundling of Internet Explorer with Windows.Bill Gates handed over the CEO position on January 13, 2000, to Steve Ballmer, an old college friend of Gates and employee of the company since 1980, creating a new position for himself as Chief Software Architect. Various companies including Microsoft formed the Trusted Computing Platform Alliance in October 1999 to, among other things, increase security and protect intellectual property through identifying changes in hardware and software. Critics decry the alliance as a way to enforce indiscriminate restrictions over how consumers use software, and over how computers behave, a form of digital rights management; for example the scenario where a computer is not only secured for its owner, but also secured against its owner as well. On April 3, 2000, a judgment was handed down in the case of United States v. Microsoft, calling the company an "abusive monopoly"; it settled with the U.S. Department of Justice in 2004. On October 25, 2001, Microsoft released Windows XP, unifying the mainstream and NT lines under the NT codebase. The company released the Xbox later that year, entering the game console market dominated by Sony and Nintendo. In March 2004 the European Union brought antitrust legal action against the company, citing it abused its dominance with the Windows OS, resulting in a judgment of €497 million ($613 million) and to produce new versions of Windows XP without Windows Media Player, Windows XP Home Edition N and Windows XP Professional N. 2007–2011: Windows Vista, mobile, and Windows 7 Released in January 2007, the next version of Windows, Windows Vista, focused on features, security and a redesigned user interface dubbed Aero. Microsoft Office 2007, released at the same time, featured a "Ribbon" user interface which was a significant departure from its predecessors. Relatively strong sales of both titles helped to produce a record profit in 2007. The European Union imposed another fine of €899 million ($1.4 billion) for Microsoft's lack of compliance with the March 2004 judgment on February 27, 2008, saying that the company charged rivals unreasonable prices for key information about its workgroup and backoffice servers. Microsoft stated that it was in compliance and that "these fines are about the past issues that have been resolved". 2007 also saw the creation of a multi-core unit at Microsoft, as they followed in the steps of server companies such as Sun and IBM.Gates retired from his role as Chief Software Architect on June 27, 2008, a decision announced in June 2006, while retaining other positions related to the company in addition to being an advisor for the company on key projects. Azure Services Platform, the company's entry into the cloud computing market for Windows, launched on October 27, 2008. On February 12, 2009, Microsoft announced its intent to open a chain of Microsoft-branded retail stores, and on October 22, 2009, the first retail Microsoft Store opened in Scottsdale, Arizona; the same day the first store opened, Windows 7 was officially released to the public. Windows 7's focus was on refining Vista with ease of use features and performance enhancements, rather than a large reworking of Windows.As the smartphone industry boomed beginning in 2007, Microsoft struggled to keep up with its rivals Apple and Google in providing a modern smartphone operating system. As a result, in 2010, Microsoft revamped their aging flagship mobile operating system, Windows Mobile, replacing it with the new Windows Phone OS; along with a new strategy in the smartphone industry that had Microsoft working more closely with smartphone manufacturers, such as Nokia, and to provide a consistent user experience across all smartphones using Microsoft's Windows Phone OS. It used a new user interface design language, codenamed "Metro", which prominently used simple shapes, typography and iconography, and the concept of minimalism. Microsoft is a founding member of the Open Networking Foundation started on March 23, 2011. Other founding companies include Google, HP Networking, Yahoo, Verizon, Deutsche Telekom and 17 other companies. The nonprofit organization is focused on providing support for a new cloud computing initiative called Software-Defined Networking. The initiative is meant to speed innovation through simple software changes in telecommunications networks, wireless networks, data centers and other networking areas. 2011–2014: Rebranding, Windows 8, Surface and Nokia devices Following the release of Windows Phone, Microsoft underwent a gradual rebranding of its product range throughout 2011 and 2012—the corporation's logos, products, services and websites adopted the principles and concepts of the Metro design language. Microsoft previewed Windows 8, an operating system designed to power both personal computers and tablet computers, in Taipei in June 2011. A developer preview was released on September 13, and was replaced by a consumer preview on February 29, 2012. On May 31, 2012, the preview version was released. On June 18, 2012, Microsoft unveiled the Surface, the first computer in the company's history to have its hardware made by Microsoft. On June 25, Microsoft paid US$1.2 billion to buy the social network Yammer. On July 31, 2012, Microsoft launched the Outlook.com webmail service to compete with Gmail. On September 4, 2012, Microsoft released Windows Server 2012.In July 2012, Microsoft sold its 50% stake in MSNBC.com, which it had run as a joint venture with NBC since 1996. On October 1, Microsoft announced its intention to launch a news operation, part of a new-look MSN, at the time of the Windows 8 launch that was later in the month. On October 26, 2012, Microsoft launched Windows 8 and the Microsoft Surface. Three days later, Windows Phone 8 was launched. To cope with the potential for an increase in demand for products and services, Microsoft opened a number of "holiday stores" across the U.S. to complement the increasing number of "bricks-and-mortar" Microsoft Stores that opened in 2012. On March 29, 2013, Microsoft launched a Patent Tracker.The Kinect, a motion-sensing input device made by Microsoft and designed as a video game controller, which was first introduced in November 2010, was upgraded for the 2013 release of the eighth-generation Xbox One video game console. Kinect's capabilities were revealed in May 2013. The new Kinect uses an ultra-wide 1080p camera, it can function in the dark due to an infrared sensor, it employs higher-end processing power and new software, it can distinguish between fine movements (such as a thumb movements), and the device can determine a user's heart rate by looking at his/her face. Microsoft filed a patent application in 2011 that suggests that the corporation may use the Kinect camera system to monitor the behavior of television viewers as part of a plan to make the viewing experience more interactive. On July 19, 2013, Microsoft stocks suffered its biggest one-day percentage sell-off since the year 2000 after its fourth-quarter report raised concerns among the investors on the poor showings of both Windows 8 and the Surface tablet; with more than 11 percentage points declining Microsoft suffered a loss of more than US$32 billion. For the 2010 fiscal year, Microsoft had five product divisions: Windows Division, Server and Tools, Online Services Division, Microsoft Business Division and Entertainment and Devices Division.On September 3, 2013, Microsoft agreed to buy Nokia's mobile unit for $7 billion. Also in 2013, Amy Hood became the CFO of Microsoft. The Alliance for Affordable Internet (A4AI) was launched in October 2013 and Microsoft was part of the coalition of public and private organizations that also included Facebook, Intel and Google. Led by World Wide Web inventor Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease Internet access prices so that they fall below the UN Broadband Commission's worldwide target of 5% of monthly income. In line with the maturing PC business, in July 2013, Microsoft announced that it would reorganize the business into four new business divisions by function: Operating System, Apps, Cloud and Devices. All previous divisions will be diluted into new divisions without any workforce cut. 2014–present: Windows 10, Minecraft, and HoloLens  Corporate affairs On February 4, 2014, Steve Ballmer stepped down as CEO of Microsoft and was succeeded by Satya Nadella, who previously led Microsoft's Cloud and Enterprise division. On the same day, John W. Thompson took on the role of chairman, with Bill Gates stepping down from the position, while continuing to participate as a technology advisor.On April 25, 2014, Microsoft acquired Nokia Devices and Services for $7.2 billion. The new subsidiary was renamed Microsoft Mobile Oy. In May 2016, the company announced it will lay off 1,850 workers, taking an impairment and restructuring charge of $950 million. During the previous summer of 2015 the company wrote down $7.6 billion related to its mobile-phone business and fired 7,800 employees from those operations.On September 15, 2014, Microsoft acquired the video game development company Mojang, best known for its wildly popular flagship game Minecraft, for $2.5 billion. Products On January 21, 2015, Microsoft announced the release of their first Interactive whiteboard, Microsoft Surface Hub (part of the Surface family.) On July 29, 2015, Microsoft released the next version of the Windows operating system, Windows 10. Its server sibling, Windows Server 2016, was released in September 2016.In Q1 2015, Microsoft was the third largest maker of mobile phones selling 33 million units (7.2% of all), while a large majority (at least 75%) of them do not run any version of Windows Phone – those other phones are not categorized as smartphones by Gartner – in the same time frame 8 million Windows smartphones (2.5% of all smartphones) were made by all manufacturers (but mostly by Microsoft). Microsoft's share of the U.S. smartphone market in January 2016 was 2.7%.On March 1, 2016, Microsoft announced the merger of its PC and Xbox divisions, with Phil Spencer announcing that Universal Windows Applications would be the focus for Microsoft's gaming in the future.On January 24, 2017, Microsoft showcased Intune for Education at the BETT 2017 education technology conference in London. Intune for Education is a new cloud-based application and device management service for the education sector. Microsoft will launch a preview of Intune for Education "in the coming weeks", with general availability scheduled for spring 2017, priced at $30 per device, or through volume licensing agreements. Services In June 2016, Microsoft announced a project named, Microsoft Azure Information Protection. It aims to help enterprises protect their data as it moves between servers and devices.In November 2016, Microsoft has joined the Linux Foundation as a Platinum member during Microsoft’s Connect(); developer event in New York. Price of each platinum membership is $500,000 USD per year. Businesses  Windows Division, Server and Tools, Online Services Division The company's Client division produces the flagship Windows OS line such as Windows 8; it also produces the Windows Live family of products and services. Server and Tools produces the server versions of Windows, such as Windows Server 2008 R2 as well as a set of development tools called Microsoft Visual Studio, Microsoft Silverlight, a web application framework, and System Center Configuration Manager, a collection of tools providing remote-control abilities, patch management, software distribution and a hardware/software inventory. Other server products include: Microsoft SQL Server, a relational database management system, Microsoft Exchange Server, for certain business-oriented e-mail and scheduling features, Small Business Server, for messaging and other small business-oriented features; and Microsoft BizTalk Server, for business process management.Microsoft provides IT consulting ("Microsoft Consulting Services") and produces a set of certification programs handled by the Server and Tools division designed to recognize individuals who have a minimal set of proficiencies in a specific role; this includes developers ("Microsoft Certified Solution Developer"), system/network analysts ("Microsoft Certified Systems Engineer"), trainers ("Microsoft Certified Trainers") and administrators ("Microsoft Certified Systems Administrator" and "Microsoft Certified Database Administrator"). Microsoft Press, which publishes books, is also managed by the division. The Online Services Business division handles the online service MSN and the search engine Bing. Business Division The Microsoft Business Division produces Microsoft Office including Microsoft Office 2016, the company's line of office software. The software product includes Word (a word processor), Access (a relational database program), Excel (a spreadsheet program), Outlook (Groupware, frequently used with Exchange Server), PowerPoint (presentation software), Publisher (desktop publishing software) and Sharepoint. A number of other products were added later with the release of Office 2003 including Visio, Project, MapPoint, InfoPath and OneNote. The division also develops enterprise resource planning (ERP) software for companies under the Microsoft Dynamics brand. These include: Microsoft Dynamics AX, Microsoft Dynamics NAV, Microsoft Dynamics GP, and Microsoft Dynamics SL. They are targeted at varying company types and countries, and limited to organizations with under 7,500 employees. Also included under the Dynamics brand is the customer relationship management software Microsoft Dynamics CRM, part of the Azure Services Platform. Entertainment and Devices Division The Entertainment and Devices Division produces the Windows CE OS for embedded systems and Windows Phone for smartphones. Microsoft initially entered the mobile market through Windows CE for handheld devices, eventually developing into the Windows Mobile OS and now, Windows Phone. Windows CE is designed for devices where the OS may not directly be visible to the end user, in particular, appliances and cars. The division also produces computer games, via its in-house game publisher Microsoft Studios, that run on Windows PCs and other systems including titles such as Age of Empires, Halo and the Microsoft Flight Simulator series, and houses the Macintosh Business Unit which produces macOS software including Microsoft Office 2011 for Mac. Microsoft's Entertainment and Devices Division designs, markets, and manufactures consumer electronics including the Xbox 360 game console, the handheld Zune media player, and the television-based Internet appliance MSN TV. Microsoft also markets personal computer hardware including mice, keyboards, and various game controllers such as joysticks and gamepads. Future Decoded Future Decoded is a business networking event held every year by Microsoft that allows business partners of the company to share their views on what the future holds for business, society, leadership and technology. Corporate affairs  Board of Directors The company is run by a board of directors made up of mostly company outsiders, as is customary for publicly traded companies. Members of the board of directors as of January 2016 are John W. Thompson, Bill Gates, Teri L. List-Stoll, Mason Morfit, Satya Nadella, Charles Noski, Helmut Panke, Sandi Peterson, Charles W. Scharf, John W. Stanton, and Padmasree Warrior. Board members are elected every year at the annual shareholders' meeting using a majority vote system. There are five committees within the board which oversee more specific matters. These committees include the Audit Committee, which handles accounting issues with the company including auditing and reporting; the Compensation Committee, which approves compensation for the CEO and other employees of the company; the Finance Committee, which handles financial matters such as proposing mergers and acquisitions; the Governance and Nominating Committee, which handles various corporate matters including nomination of the board; and the Antitrust Compliance Committee, which attempts to prevent company practices from violating antitrust laws. Financial When Microsoft went public and launched its initial public offering (IPO) in 1986, the opening stock price was $21; after the trading day, the price closed at $27.75. As of July 2010, with the company's nine stock splits, any IPO shares would be multiplied by 288; if one were to buy the IPO today given the splits and other factors, it would cost about 9 cents. The stock price peaked in 1999 at around $119 ($60.928 adjusting for splits). The company began to offer a dividend on January 16, 2003, starting at eight cents per share for the fiscal year followed by a dividend of sixteen cents per share the subsequent year, switching from yearly to quarterly dividends in 2005 with eight cents a share per quarter and a special one-time payout of three dollars per share for the second quarter of the fiscal year. Though the company had subsequent increases in dividend payouts, the price of Microsoft's stock remained steady for years.Standard and Poor's and Moody's have both given a AAA rating to Microsoft, whose assets were valued at $41 billion as compared to only $8.5 billion in unsecured debt. Consequently, in February 2011 Microsoft released a corporate bond amounting to $2.25 billion with relatively low borrowing rates compared to government bonds. For the first time in 20 years Apple Inc. surpassed Microsoft in Q1 2011 quarterly profits and revenues due to a slowdown in PC sales and continuing huge losses in Microsoft's Online Services Division (which contains its search engine Bing). Microsoft profits were $5.2 billion, while Apple Inc. profits were $6 billion, on revenues of $14.5 billion and $24.7 billion respectively. Microsoft's Online Services Division has been continuously loss-making since 2006 and in Q1 2011 it lost $726 million. This follows a loss of $2.5 billion for the year 2010.On July 20, 2012, Microsoft posted its first quarterly loss ever, despite earning record revenues for the quarter and fiscal year, with a net loss of $492 million due to a writedown related to the advertising company aQuantive, which had been acquired for $6.2 billion back in 2007. As of January 2014, Microsoft's market capitalization stood at $314B, making it the 8th largest company in the world by market capitalization. On November 14, 2014, Microsoft overtook Exxon Mobil to become the 2nd most valuable company by market capitalization, behind only Apple Inc. Its total market value was over $410B — with the stock price hitting $50.04 a share, the highest since early 2000. In 2015, Reuters reported that Microsoft Corp had earnings abroad of $76.4 billion which were untaxed by the IRS. Under U.S. law corporations don't pay income tax on overseas profits until the profits are brought into the United States. Marketing In 2004, Microsoft commissioned research firms to do independent studies comparing the total cost of ownership (TCO) of Windows Server 2003 to Linux; the firms concluded that companies found Windows easier to administrate than Linux, thus those using Windows would administrate faster resulting in lower costs for their company (i.e. lower TCO). This spurred a wave of related studies; a study by the Yankee Group concluded that upgrading from one version of Windows Server to another costs a fraction of the switching costs from Windows Server to Linux, although companies surveyed noted the increased security and reliability of Linux servers and concern about being locked into using Microsoft products. Another study, released by the Open Source Development Labs, claimed that the Microsoft studies were "simply outdated and one-sided" and their survey concluded that the TCO of Linux was lower due to Linux administrators managing more servers on average and other reasons.As part of the "Get the Facts" campaign, Microsoft highlighted the .NET trading platform that it had developed in partnership with Accenture for the London Stock Exchange, claiming that it provided "five nines" reliability. After suffering extended downtime and unreliability the LSE announced in 2009 that it was planning to drop its Microsoft solution and switch to a Linux-based one in 2010.In 2012, Microsoft hired a political pollster named Mark Penn, whom the New York Times called "famous for bulldozing" his political opponents as Executive Vice-President, Advertising and Strategy. Penn created a series of negative ads targeting one of Microsoft's chief competitors, Google. The ads, called "Scroogled", attempt to make the case that Google is "screwing" consumers with search results rigged to favor Google's paid advertisers, that Gmail violates the privacy of its users to place ad results related to the content of their emails and shopping results which favor Google products. Tech publications like Tech Crunch have been highly critical of the ad campaign, while Google employees have embraced it. Layoffs In July 2014, Microsoft announced plans to lay off 18,000 employees. Microsoft employed 127,104 people as of June 5, 2014, making this about a 14 percent reduction of its workforce as the biggest Microsoft lay off ever. This included 12,500 professional and factory personnel. Previously, Microsoft has laid off 5,800 jobs in 2009 in line with US financial crisis. In September 2014, Microsoft laid off 2,100 people, including 747 people in the Seattle-Redmond area, where the company is headquartered. The firings came as a second wave of the layoffs that were previously announced. This brings the total number to over 15,000 out of the 18,000 expected cuts. In October 2014, Microsoft revealed that it was almost done with the elimination of 18,000 employees which was its largest ever layoff sweep. In July 2015, Microsoft announced another 7,800 job cuts in the next several months. In May 2016, Microsoft announced another 1,850 job cuts mostly in (Nokia) mobile phone division. As a result, the company will record an impairment and restructuring charge of approximately $950 million, of which approximately $200 million will relate to severance payments. United States government Microsoft provides information about reported bugs in their software to intelligence agencies of the United States government, prior to the public release of the fix. A Microsoft spokesperson has stated that the corporation runs several programs that facilitate the sharing of such information with the U.S. government. Following media reports about PRISM, NSA's massive electronic surveillance program, in May 2013, several technology companies were identified as participants, including Microsoft. According to leaks of said program, Microsoft joined the PRISM program in 2007. However, in June 2013, an official statement from Microsoft flatly denied their participation in the program:We provide customer data only when we receive a legally binding order or subpoena to do so, and never on a voluntary basis. In addition we only ever comply with orders for requests about specific accounts or identifiers. If the government has a broader voluntary national security program to gather customer data, we don't participate in it.During the first six months in 2013, Microsoft had received requests that affected between 15,000 and 15,999 accounts. In December 2013, the company made statement to further emphasize the fact that they take their customers' privacy and data protection very seriously, even saying that "government snooping potentially now constitutes an "advanced persistent threat," alongside sophisticated malware and cyber attacks". The statement also marked the beginning of three-part program to enhance Microsoft's encryption and transparency efforts. On July 1, 2014, as part of this program they opened the first (of many) Microsoft Transparency Center, that provides "participating governments with the ability to review source code for our key products, assure themselves of their software integrity, and confirm there are no "back doors." Microsoft has also argued that the United States Congress should enact strong privacy regulations to protect consumer data. In 2016, the company sued the U.S., arguing that secrecy orders were preventing the company from disclosing warrants to customers in violation of the company's and customers' rights. Corporate identity  Corporate culture Technical reference for developers and articles for various Microsoft magazines such as Microsoft Systems Journal (MSJ) are available through the Microsoft Developer Network (MSDN). MSDN also offers subscriptions for companies and individuals, and the more expensive subscriptions usually offer access to pre-release beta versions of Microsoft software. In April 2004 Microsoft launched a community site for developers and users, titled Channel 9, that provides a wiki and an Internet forum. Another community site that provides daily videocasts and other services, On10.net, launched on March 3, 2006. Free technical support is traditionally provided through online Usenet newsgroups, and CompuServe in the past, monitored by Microsoft employees; there can be several newsgroups for a single product. Helpful people can be elected by peers or Microsoft employees for Microsoft Most Valuable Professional (MVP) status, which entitles them to a sort of special social status and possibilities for awards and other benefits.Noted for its internal lexicon, the expression "eating our own dog food" is used to describe the policy of using pre-release and beta versions of products inside Microsoft in an effort to test them in "real-world" situations. This is usually shortened to just "dog food" and is used as noun, verb, and adjective. Another bit of jargon, FYIFV or FYIV ("Fuck You, I'm [Fully] Vested"), is used by an employee to indicate they are financially independent and can avoid work anytime they wish. The company is also known for its hiring process, mimicked in other organizations and dubbed the "Microsoft interview", which is notorious for off-the-wall questions such as "Why is a manhole cover round?".Microsoft is an outspoken opponent of the cap on H1B visas, which allow companies in the U.S. to employ certain foreign workers. Bill Gates claims the cap on H1B visas makes it difficult to hire employees for the company, stating "I'd certainly get rid of the H1B cap" in 2005. Critics of H1B visas argue that relaxing the limits would result in increased unemployment for U.S. citizens due to H1B workers working for lower salaries. The Human Rights Campaign Corporate Equality Index, a report of how progressive the organization deems company policies towards LGBT (lesbian, gay, bisexual and transsexual) employees, rated Microsoft as 87% from 2002 to 2004 and as 100% from 2005 to 2010 after they allowed gender expression. Environment In 2011, Greenpeace released a report rating the top ten big brands in cloud computing on their sources of electricity for their data centers. At the time, data centers consumed up to 2% of all global electricity and this amount was projected to increase. Phil Radford of Greenpeace said "we are concerned that this new explosion in electricity use could lock us into old, polluting energy sources instead of the clean energy available today," and called on "Amazon, Microsoft and other leaders of the information-technology industry must embrace clean energy to power their cloud-based data centers." In 2013, Microsoft agreed to buy power generated by a Texas wind project to power one of its data centers. Microsoft is ranked on the 17th place in Greenpeace's Guide to Greener Electronics (16th Edition) that ranks 18 electronics manufacturers according to their policies on toxic chemicals, recycling and climate change. Microsoft's timeline for phasing out brominated flame retardant (BFRs) and phthalates in all products is 2012 but its commitment to phasing out PVC is not clear. As of January 2011, it has no products that are completely free from PVC and BFRs.Microsoft's main U.S. campus received a silver certification from the Leadership in Energy and Environmental Design (LEED) program in 2008, and it installed over 2,000 solar panels on top of its buildings in its Silicon Valley campus, generating approximately 15 percent of the total energy needed by the facilities in April 2005. Microsoft makes use of alternative forms of transit. It created one of the world's largest private bus systems, the "Connector", to transport people from outside the company; for on-campus transportation, the "Shuttle Connect" uses a large fleet of hybrid cars to save fuel. The company also subsidises regional public transport, provided by Sound Transit and King County Metro, as an incentive. In February 2010 however, Microsoft took a stance against adding additional public transport and high-occupancy vehicle (HOV) lanes to the State Route 520 and its floating bridge connecting Redmond to Seattle; the company did not want to delay the construction any further. Microsoft was ranked number 1 in the list of the World's Best Multinational Workplaces by the Great Place to Work Institute in 2011. Headquarters The corporate headquarters, informally known as the Microsoft Redmond campus, is located at One Microsoft Way in Redmond, Washington. Microsoft initially moved onto the grounds of the campus on February 26, 1986, weeks before the company went public on March 13. The headquarters has since experienced multiple expansions since its establishment. It is estimated to encompass over 8 million ft2 (750,000 m2) of office space and 30,000–40,000 employees. Additional offices are located in Bellevue and Issaquah (90,000 employees worldwide). The company is planning to upgrade its Mountain View, California campus on a grand scale. The company has occupied this campus since 1981. The company is planning to buy the 32-acre campus. The plans submitted involve expanding the campus by 25%. It is expected that it will take three years to complete the expansion. If approved, construction will start in early 2017. Microsoft operates an East Coast headquarters in Charlotte, North Carolina. Flagship stores On October 26, 2015, the company opened its flagship retail location on Fifth Avenue in New York City. The location features a five-story glass storefront and is 22,270 square feet. As per company executives, Microsoft had been on the lookout for a flagship location since 2009. The company's retail locations are part of a greater strategy to help build a connection with its consumers. The opening of the store coincided with the launch of the Surface Book and Surface Pro 4. Notably, the second floor has a large area designated for consumers to play Xbox games. The third floor has been named the "Dell Experience at the Microsoft Store," which showcases various Dell products. The fourth floor is for employees and administrative operations. The fifth floor has been designed as a pseudo-conference center, as it will hold events and meetings. On November 12, 2015, Microsoft opened a second flagship store, located in Sydney's Pitt Street Mall. The two-storey, 6000 sq ft location features Microsoft's flagship products including the Surface line and Xbox One, there is also an Answer Desk on site for customers to get product support. Logo Microsoft adopted the so-called "Pac-Man Logo", designed by Scott Baker, in 1987. Baker stated "The new logo, in Helvetica italic typeface, has a slash between the o and s to emphasize the "soft" part of the name and convey motion and speed." Dave Norris ran an internal joke campaign to save the old logo, which was green, in all uppercase, and featured a fanciful letter O, nicknamed the blibbet, but it was discarded. Microsoft's logo with the tagline "Your potential. Our passion." –  below the main corporate name –  is based on a slogan Microsoft used in 2008. In 2002, the company started using the logo in the United States and eventually started a television campaign with the slogan, changed from the previous tagline of "Where do you want to go today?" During the private MGX (Microsoft Global Exchange) conference in 2010, Microsoft unveiled the company's next tagline, "Be What's Next." They also had a slogan/tagline "Making it all make sense."On August 23, 2012, Microsoft unveiled a new corporate logo at the opening of its 23rd Microsoft store in Boston, indicating the company's shift of focus from the classic style to the tile-centric modern interface, which it uses/will use on the Windows Phone platform, Xbox 360, Windows 8 and the upcoming Office Suites. The new logo also includes four squares with the colors of the then-current Windows logo which have been used to represent Microsoft's four major products: Windows (blue), Office (red), Xbox (green) and Bing (yellow). The logo resembles the opening of one of the commercials for Windows 95.1975–1980: First Microsoft logo, in 19751980–1982: Second Microsoft logo, in 19801982–1987: Third Microsoft logo, in 19821987–2012: Microsoft "Pac-Man" logo, designed by Scott Baker and used from 1987 to 20122012–present: Fifth Microsoft logo, introduced on August 23, 2012 Sponsorship The company was the official jersey sponsor of Finland's national basketball team at the 2015 EuroBasket.The company sponsored the Lotus F1 Team as Microsoft Dynamics. See also Bill GatesPaul AllenList of mergers and acquisitions by Microsoft References  External links Official websiteMicrosoft companies grouped at OpenCorporates
IBM PC compatible computers are those similar to the original IBM PC, XT, and AT, able to run the same software and support the same expansion cards as those. Such computers used to be referred to as PC clones, or IBM clones. They duplicate almost exactly all the significant features of the PC architecture, facilitated by IBM's choice of commodity hardware components and various manufacturers' ability to reverse engineer the BIOS firmware using a "clean room design" technique. Columbia Data Products built the first clone of the IBM personal computer by a clean room implementation of its BIOS.Early IBM PC compatibles used the same computer bus as the original PC and AT models. The IBM AT compatible bus was later named the Industry Standard Architecture bus by manufacturers of compatible computers. The term "IBM PC compatible" is now a historical description only, since IBM has ended its personal computer sales.Descendants of the IBM PC compatibles comprise the majority of personal computers on the market presently with the dominant operating system being Microsoft Windows, although interoperability with the bus structure and peripherals of the original PC architecture may be limited or non-existent. Only the Macintosh (classic Mac OS and macOS) kept significant market share without compatibility with the IBM PC, so consumers are typically identified as being a "PC or Mac" user (similar to the divide in mobile devices between Android and iOS). Origins IBM decided in 1980 to market a low-cost single-user computer as quickly as possible in response to Apple Computer's success in the burgeoning microcomputer market. On 12 August 1981, the first IBM PC went on sale. There were three operating systems (OS) available for it. The least expensive and most popular was PC DOS made by Microsoft. In a crucial concession, IBM's agreement allowed Microsoft to sell its own version, MS-DOS, for non-IBM computers. The only component of the original PC architecture exclusive to IBM was the BIOS (Basic Input/Output System).IBM at first asked developers to avoid writing software that addressed the computer's hardware directly, and to instead make standard calls to BIOS functions that carried out hardware-dependent operations. This software would run on any machine using MS-DOS or PC-DOS. Software that directly addressed the hardware instead of making standard calls was faster, however; this was particularly relevant to games. Software addressing IBM PC hardware in this way would not run on MS-DOS machines with different hardware. The IBM PC was sold in high enough volumes to justify writing software specifically for it, and this encouraged other manufacturers to produce machines which could use the same programs, expansion cards, and peripherals as the PC. The 808x computer marketplace rapidly excluded all machines which were not hardware- and software-compatible with the PC. The 640 KB barrier on "conventional" system memory available to MS-DOS is a legacy of that period; other non-clone machines, while subject to a limit, could exceed 640 kB.Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. By June 1983 PC Magazine defined "PC 'clone'" as "a computer [that can] accommodate the user who takes a disk home from an IBM PC, walks across the room, and plugs it into the 'foreign' machine". Because of a shortage of IBM PCs that year, many customers purchased clones instead. Columbia Data Products produced the first computer more or less compatible to the IBM PC standard during June 1982, soon followed by Eagle Computer. Compaq announced its first IBM PC compatible in November 1982, the Compaq Portable. The Compaq was the first sewing machine-sized portable computer that was essentially 100% PC-compatible. The company could not copy the BIOS directly as a result of the court decision in Apple v. Franklin, but it could reverse-engineer the IBM BIOS and then write its own BIOS using clean room design. Compatibility issues  Non-compatible MS-DOS computers At the same time, many manufacturers such as Tandy/RadioShack, Xerox, Hewlett-Packard, Digital Equipment Corporation, Sanyo, Texas Instruments, Tulip, Wang and Olivetti introduced personal computers that supported MS DOS, but were not completely software- or hardware-compatible with the IBM PC.Like IBM, Microsoft's intention was that application writers would write to the application programming interfaces in MS-DOS or the firmware BIOS, and that this would form what would now be termed a hardware abstraction layer. Each computer would have its own Original Equipment Manufacturer (OEM) version of MS-DOS, customized to its hardware. Any software written for MS-DOS would operate on any MS-DOS computer, despite variations in hardware design.This expectation seemed reasonable in the computer marketplace of the time. Until then Microsoft was based primarily on computer languages such as BASIC. The established small system operating software was CP/M from Digital Research which was in use both at the hobbyist level and by the more professional of those using microcomputers. To achieve such widespread use, and thus make the product viable economically, the OS had to operate across a range of machines from different vendors that had widely varying hardware. Those customers who needed other applications than the starter programs could reasonably expect publishers to offer their products for a variety of computers, on suitable media for each.Microsoft's competing OS was intended initially to operate on a similar varied spectrum of hardware, although all based on the 8086 processor. Thus, MS-DOS was for several years sold only as an OEM product. There was no Microsoft-branded MS-DOS: MS-DOS could not be purchased directly from Microsoft, and each OEM release was packaged with the trade dress of the given PC vendor. Malfunctions were to be reported to the OEM, not to Microsoft. However, as machines that were compatible with IBM hardware—thus supporting direct calls to the hardware—became widespread, it soon became clear that the OEM versions of MS-DOS were virtually identical, except perhaps for the provision of a few utility programs.MS-DOS provided adequate functionality for character-oriented applications such as those that could have been implemented on a text-only terminal. Had the bulk of commercially important software been of this nature, low-level hardware compatibility might not have mattered. However, in order to provide maximum performance and leverage hardware features (or work around hardware bugs), PC applications quickly developed beyond the simple terminal applications that MS-DOS supported directly. Spreadsheets, WYSIWYG word processors, presentation software and remote communication software established new markets that exploited the PC's strengths, but required capabilities beyond what MS-DOS provided. Thus, from very early in the development of the MS-DOS software environment, many significant commercial software products were written directly to the hardware, for a variety of reasons:MS-DOS itself did not provide any way to position the text cursor other than to advance it after displaying each letter (teletype mode). While the BIOS video interface routines were adequate for rudimentary output, they were necessarily less efficient than direct hardware addressing, as they added extra processing; they did not have "string" output, but only character-by-character teletype output, and they inserted delays to prevent CGA hardware "snow" (a display artifact of CGA cards produced when writing directly to screen memory)——an especially bad artifact since they were called by IRQs, thus making multitasking very difficult. A program that wrote directly to video memory could achieve output rates 5 to 20 times faster than making system calls. Turbo Pascal used this technique from its earliest versions.Graphics capability was not taken seriously in the original IBM design brief; graphics were considered only from the perspective of generating static business graphics such as charts and graphs. MS-DOS did not have an API for graphics, and the BIOS only included the rudimentary graphics functions such as changing screen modes and plotting single points. To make a BIOS call for every point drawn or modified increased overhead considerably, making the BIOS interface notoriously slow. Because of this, line-drawing, arc-drawing, and blitting had to be performed by the application to achieve acceptable speed, which was usually done by bypassing the BIOS and accessing video memory directly. Software written to address IBM PC hardware directly would run on any IBM clone, but would have to be rewritten especially for each non-PC-compatible MS-DOS machine.Video games, even early ones, mostly required a true graphics mode. They also performed any machine-dependent trick the programmers could think of in order to gain speed. Though initially the major market for the PC was for business applications, games capability became an important factor motivating PC purchases as prices decreased. The availability and quality of games could mean the difference between the purchase of a PC compatible or a different platform with the ability to exchange data like the Amiga.Communications software directly accessed the UART serial port chip, because the MS-DOS API and the BIOS did not provide full support and was too slow to keep up with hardware which could transfer data at 19,200 bit/s.Even for standard business applications, speed of execution was a significant competitive advantage. Integrated software Context MBA preceded Lotus 1-2-3 to market and included more functions. Context MBA was written in UCSD p-System, making it very portable but too slow to be truly usable on a PC. 1-2-3 was written in x86 assembly language and performed some machine-dependent tricks. It was so much faster that it quickly surpassed Context MBA's sales.Disk copy-protection schemes, in common use at the time, worked by reading nonstandard data patterns on the diskette to verify originality. These patterns were impossible to detect using standard DOS or BIOS calls, so direct access to the disk controller hardware was necessary for the protection to work.Some software was designed to run only on a true IBM PC, and checked for an actual IBM BIOS. Difficulty of "Operationally Compatible" In May 1983, Future Computing defined four levels of compatibility:Operationally Compatible. Can run "the top selling" IBM PC software, use PC expansion boards, and read and write PC disks. Has "complementary features" like portability or lower price that distinguish computer from the PC, which is sold in the same store. Examples: (Best) Columbia Data Products, Compaq; (Better) Corona; (Good) Eagle.Functionally Compatible. Runs own version of popular PC software. Cannot use PC expansion boards but can read and write PC disks. Cannot become Operationally Compatible. Example: TI Professional.Data Compatible. May not run top PC software. Can read and/or write PC disks. Can become Functionally Compatible. Examples: NCR Decision Mate, Olivetti M20, Wang PC, Zenith Z-100.Incompatible. Cannot read PC disks. Can become Data Compatible. Examples: Altos 586, DEC Rainbow 100, Grid Compass, Victor 9000.During development, Compaq engineers found that Microsoft Flight Simulator would not run because of what subLOGIC's Bruce Artwick described as "a bug in one of Intel's chips", forcing them to make their new computer bug compatible with the IBM PC. At first, few clones other than Compaq's offered truly full compatibility; Columbia University reported in January 1984, for example, that Kermit ran without modification on Compaq and Columbia Data Products clones, but not on those from Eagle or Seequa. Other MS-DOS computers also required custom code.When PC Magazine requested samples from computer manufacturers that claimed to produce PC compatibles for an April 1984 review, 14 of 31 declined. Corona Data Systems specified that "Our systems run all software that conforms to IBM PC programming standards. And the most popular software does." When a BYTE journalist asked to test Peachtext at the Spring 1983 COMDEX, Corona representatives "hemmed and hawed a bit, but they finally led me ... off in the corner where no one would see it should it fail". The magazine reported that "Their hesitancy was unnecessary. The disk booted up without a problem".Creative Computing in 1985 stated, "we reiterate our standard line regarding the IBM PC compatibles: try the package you want to use before you buy the computer." Companies modified their computers' BIOS to work with newly discovered incompatible applications, and reviewers and users developed stress tests to measure compatibility; by 1984 the ability to operate Lotus 1-2-3 and Flight Simulator became the standard.IBM believed that some companies such as Eagle, Corona, and Handwell infringed on its copyright, and after Apple Computer, Inc. v. Franklin Computer Corp. successfully forced the clone makers to stop using the BIOS. The Phoenix BIOS in 1984, however, and similar products such as AMI BIOS, permitted computer makers to legally build essentially 100%-compatible clones without having to reverse-engineer the PC BIOS themselves. A September 1985 InfoWorld chart listed seven compatibles with 256 KB RAM, two disk drives, and monochrome monitors for $1,495 to $2,320, while the equivalent IBM PC cost $2,820. The decreasing influence of IBM In February 1984 Byte wrote that "IBM's burgeoning influence in the PC community is stifling innovation because so many other companies are mimicking Big Blue", but as the market grew IBM's influence diminished. In November 1985 PC Magazine stated "Now that it has created the [PC] market, the market doesn't necessarily need IBM for the machines. It may depend on IBM to set standards and to develop higher-performance machines, but IBM had better conform to existing standards so as to not hurt users". In January 1987 Bruce Webster wrote in Byte of rumors that IBM would introduce proprietary personal computers with a proprietary operating system: "Who cares? If IBM does it, they will most likely just isolate themselves from the largest marketplace, in which they really can't compete anymore anyway". The magazine predicted that in 1987 the market "will complete its transition from an IBM standard to an Intel/MS-DOS/expansion bus standard ... Folks aren't so much concerned about IBM compatibility as they are about Lotus 1-2-3 compatibility".After IBM announced the OS/2-oriented PS/2 line in early 1987, sales of existing DOS-compatible PC compatibles rose, in part because the proprietary operating system was not available. In 1988 Gartner Group estimated that the public purchased 1.5 clones for every IBM PC. By 1989 Compaq was so influential that industry executives spoke of "Compaq compatible", with observers stating that customers saw the company as IBM's equal.After 1987, IBM PC compatibles dominated both the home and business markets of commodity computers, with other notable alternative architectures being used in niche markets, like the Macintosh computers offered by Apple Inc. and used mainly for desktop publishing at the time, the aging 8-bit Commodore 64 which was selling for $150 by this time and became the world's best-selling computer, the 32-bit Commodore Amiga line used for television and video production and the 32-bit Atari ST used by the music industry. However, IBM itself lost the main role in the market for IBM PC compatibles by 1990. A few events in retrospect are important:The 1982 introduction of the Compaq Portable, the first 100% IBM PC compatible computer, providing portability unavailable from IBM at the time.An Independent Business Unit (IBU) within IBM developed the IBM PC and XT. IBUs did not share in corporate R&D expense. After the IBU became the Entry Systems Division it lost this benefit, greatly decreasing margins.The availability by 1986 of sub-$1000 "Turbo XT" PC XT compatibles, including early offerings from Dell Computer, reducing demand for IBM's models. It was possible to buy two of these "generic" systems for less than the cost of one IBM-branded PC AT, and many companies did just that.Compaq beating IBM to market during 1986 with Compaq Deskpro 386, the first 80386-based PC.IBM's 1987 introduction of the incompatible and proprietary MicroChannel Architecture (MCA) computer bus, for its Personal System/2 (PS/2) line.The 1988 introduction by the "Gang of Nine" companies of a rival bus, Extended Industry Standard Architecture, intended to compete with, rather than copy, MCA.The duelling expanded memory (EMS) and extended memory (XMS) standards of the late 1980s, both developed without input from IBM.Despite popularity of its ThinkPad set of laptop PC's, IBM finally relinquished its role as a PC manufacturer during April 2005, when it sold its consumer PC division to Lenovo for US$1.75 billion.As of October 2007, Hewlett-Packard and Dell have the largest shares of the PC market in North America. They are also successful overseas, with Acer, Lenovo, and Toshiba also notable. Worldwide, a huge number of PCs are "white box" systems assembled by myriad local systems builders. Despite advances of computer technology, all current IBM PC compatibles remain very much compatible with the original IBM PC computers, although most of the components implement the compatibility in special backward compatibility modes used only during a system boot. It is often more practical to run old software on a modern system using an emulator rather than relying on these features. Expandability One of the strengths of the PC compatible design is its modular hardware design. End-users could readily upgrade peripherals and, to some degree, processor and memory without modifying the computer's motherboard or replacing the whole computer, as was the case with many of the microcomputers of the time. However, as processor speed and memory width increased, the limits of the original XT/AT bus design were soon reached, particularly when driving graphics video cards. IBM did introduce an upgraded bus in the IBM PS/2 computer that overcame many of the technical limits of the XT/AT bus, but this was rarely used as the basis for IBM compatible computers since it required licence payments to IBM both for the PS/2 bus and any prior AT-bus designs produced by the company seeking a license. This was unpopular with hardware manufacturers and several competing bus standards were developed by consortiums, with more agreeable license terms. Various attempts to standardize the interfaces were made, but in practice, many of these attempts were either flawed or ignored. Even so, there were many expansion options, and despite the confusion of its users, the PC compatible design advanced much faster than other competing designs of the time, even if only because of its market dominance. "IBM PC compatible" becomes "Wintel" During the 1990s, IBM's influence on PC architecture started to decline. An IBM-brand PC became the exception rather than the rule. Instead of placing importance on compatibility with the IBM PC, vendors began to emphasize compatibility with Windows. In 1993, a version of Windows NT was released that could operate on processors other than the x86 set. While it required that applications be recompiled, which most developers did not do, its hardware independence was used for Silicon Graphics (SGI) x86 workstations–thanks to NT's Hardware abstraction layer (HAL), they could operate NT (and its vast application library).No mass-market personal computer hardware vendor dared to be incompatible with the latest version of Windows, and Microsoft's annual WinHEC conferences provided a setting in which Microsoft could lobby for—and in some cases dictate—the pace and direction of the hardware of the PC industry. Microsoft and Intel had become so important to the ongoing development of PC hardware that industry writers began using the word Wintel to refer to the combined hardware-software system.This terminology itself is becoming a misnomer, as Intel has lost absolute control over the direction of x86 hardware development with AMD's AMD64. Also, non-Windows operating systems like macOS and Linux have established a presence on the x86 architecture. Design limitations and more compatibility issues Although the IBM PC was designed for expandability, the designers could not anticipate the hardware developments of the 1980s, nor the size of the industry they would engender. To make things worse, IBM's choice of the Intel 8088 for the CPU introduced several limitations for developing software for the PC compatible platform. For example, the 8088 processor only had a 20-bit memory addressing space. To expand PCs beyond one megabyte, Lotus, Intel, and Microsoft jointly created expanded memory (EMS), a bank-switching scheme to allow more memory provided by add-in hardware, and accessed by a set of four 16-Kilobyte "windows" inside the 20-bit addressing. Later, Intel CPUs had larger address spaces and could directly address 16- megabytes (MiBs) (80286) or more, causing Microsoft to develop extended memory (XMS) which did not require additional hardware."Expanded" and "extended" memory have incompatible interfaces, so anyone writing software that used more than one megabyte had to provide for both systems for the greatest compatibility until MS-DOS began including EMM386, which simulated EMS memory using XMS memory. A protected mode OS can also be written for the 80286, but DOS application compatibility was more difficult than expected, not only because most DOS applications accessed the hardware directly, bypassing BIOS routines intended to ensure compatibility, but also that most BIOS requests were made by the first 32 interrupt vectors, which were marked as "reserved" for protected mode processor exceptions by Intel.Video cards suffered from their own incompatibilities. Once video cards advanced to SVGA the standard for accessing them was no longer clear. At the time, PC programming used a memory model that had 64 KB memory segments. The most common VGA graphics mode's screen memory fit into a single memory segment. SVGA modes required more memory, so accessing the full screen memory was tricky. Each manufacturer developed their own methods of accessing the screen memory, even going so far as not to number the modes consistently. An attempt at creating a standard named VESA BIOS Extensions (VBE) was made, but not all manufacturers used it.When the 386 was introduced, again a protected mode OS could be written for it. This time, DOS compatibility was much easier because of virtual 8086 mode. Unfortunately programs could not switch directly between them, so eventually, some new memory-model APIs were developed, VCPI and DPMI, the latter becoming the most popular.Because of the great number of third-party adapters and no standard for them, programming the PC could be difficult. Professional developers would operate a large test-suite of various known-to-be-popular hardware combinations.Meanwhile, consumers were overwhelmed by the competing, incompatible standards and many different combinations of hardware on offer. To give them some idea of what sort of PC they would need to operate their software, the Multimedia PC (MPC) standard was set during 1990. A PC that met the minimum MPC standard could be marketed with the MPC logo, giving consumers an easy-to-understand specification to look for. Software that could operate on the most minimally MPC-compliant PC would be guaranteed to operate on any MPC. The MPC level 2 and MPC level 3 standards were set later, but the term "MPC compliant" never became popular. After MPC level 3 during 1996, no further MPC standards were established. Challenges to Wintel domination By the late 1990s, the success of Microsoft Windows, partially a result of Microsoft's questionable business practices, had driven rival commercial operating systems into near-extinction, and had ensured that the “IBM PC compatible” computer was the dominant computing platform. This meant that if a developer made their software only for the Wintel platform, they would still be able to reach the vast majority of computer users. By the late 1980s, the only major competitor to Windows with more than a few percentage points of market share was Apple Inc.'s Macintosh. The Mac started out billed as "the computer for the rest of us" but the Mac's high prices and closed architecture meant the DOS/Windows onslaught quickly drove the Macintosh into an education and desktop publishing niche, from which it only emerged in the mid-2000s. By the mid-1990s the Mac's market share had dwindled to around 5% and introducing a new rival operating system had become too risky a commercial venture. Experience had shown that even if an operating system was technically superior to Windows, it would be a failure in the market and fall victim to Microsoft's illegal tactics (BeOS and OS/2 for example). In 1989 Steve Jobs said of his new NeXT system, "It will either be the last new hardware platform to succeed, or the first to fail." Four years later in 1993 NeXT announced it was ending production of the NeXTcube and porting NeXTSTEP to Intel processors.Very early on in PC history, some companies introduced their own XT-compatible chipsets. For example, Chips and Technologies introduced their 82C100 XT Controller which integrated and replaced six of the original XT circuits: one 8237 DMA controller, one 8253 interrupt timer, one 8255 parallel interface controller, one 8259 interrupt controller, one 8284 clock generator, and one 8288 bus controller. Similar non-Intel chipsets appeared for the AT-compatibles, for example OPTi's 82C206 or 82C495XLC which were found in many 486 and early Pentium systems. The x86 chipset market was very volatile though. In 1993, VLSI Technology had become the dominant market player only to be virtually wiped out by Intel a year later. Intel has been the uncontested leader ever since. As the "Wintel" platform gained dominance Intel gradually abandoned the practice of licensing its technologies to other chipset makers; in 2010 Intel was involved in litigation related to their refusal to license their processor bus and related technologies to other companies like Nvidia.Companies such as AMD and Cyrix developed alternative CPUs that were functionally compatible with Intel's. Towards the end of the 1990s, AMD was taking an increasing share of the CPU market for PCs. AMD even ended up playing a significant role in directing the development of the x86 platform when its Athlon line of processors continued to develop the classic x86 architecture as Intel deviated with its Netburst architecture for the Pentium 4 CPUs and the IA-64 architecture for the Itanium set of server CPUs. AMD developed AMD64, the first major extension not created by Intel, which Intel later adopted as x86-64. During 2006 Intel began abandoning Netburst with the release of their set of "Core" processors that represented a development of the earlier Pentium III.A major alternative to Wintel domination is the rise of mobile computing since the early 2000s, which has been marked as the start of a post-PC era. By mid-2016, Windows-running PCs had a little less than half the market share of all computers; but all such "IBM PC" compatibles with the additional macOS-running computers, that are also capable of running Windows, "PCs" had at that point a slight majority. Mobile computers, running Android and iOS –  Tablets and smartphones –  based on CPUs with the ARM architecture –  represent the bulk of personal computers "PCs", in that sense, while not in the "IBM PC compatible" sense. A version of Windows, Windows RT (that got discontinued, with later versions; namely Windows 10 not only supporting x86 but also), exists for ARM-based computers. The IBM PC compatible today The term "IBM PC compatible" is not commonly used presently because all current mainstream desktop and laptop computers are based on the PC architecture, and IBM no longer makes PCs. The competing hardware architectures have either been discontinued or, like the Amiga, have been relegated to niche, enthusiast markets. In the past, the most successful exception was Apple's Macintosh platform, which used non-Intel processors from its inception. Although Macintosh was initially based on the Motorola 68000 family, then transitioned to the PowerPC architecture, Macintosh computers transitioned to Intel processors beginning in 2006. Today's Macintosh computers share the same system architecture as their Wintel counterparts and can boot Microsoft Windows without delegating to a DOS Compatibility Card.The processor speed and memory capacity of modern PCs are many orders of magnitude greater than they were for the original IBM PC and yet backwards compatibility has been largely maintained –  a 32-bit operating system released during the 2000s can still operate many of the simpler programs written for the OS of the early 1980s without needing an emulator, though an emulator like DOSBox now has near-native functionality at full speed. Additionally, many modern PCs can still run DOS directly, although special options such as USB legacy mode and SATA-to-PATA emulation may need to be set in the BIOS setup utility. Computers using the Extensible Firmware Interface might need to be set at legacy BIOS mode to be able to boot DOS. However, the BIOS/EFI options in most mass-produced consumer-grade computers are very limited and cannot be configured to truly handle OSes such as the original variants of DOS.The recent spread of the x86-64 architecture has further distanced current computers' and operating systems' internal similarity with the original IBM PC by introducing yet another processor mode with an instruction set modified for 64-bit addressing, but x86-64 capable processors also retain standard x86 compatibility. Intel started making x86 CPUs (Intel Atom) for Android, but had a slow start on tablet computers and smartphones. Now Google supports x86 equally for all Android uses. x86 Android hardware is theoretically IBM PC compatible but not practical for running Windows because of driver support and the lack of a mouse and keyboard (although USB On-The-Go or Bluetooth can supply these). Several apps for Android that emulate an old x86 machine benefit from the fact that the real CPU is also x86. See also AT (form factor)ATX form factorBaby AT form factorComputer hardwareComputer softwareComputing platformCustom built PCHistory of computing hardware (1960s–present)Homebuilt computerIBM Personal ComputerInfluence of the IBM PC on the personal computer marketPC speakerPersonal computerx86 architectureMS-DOSCP/M References  External links http://oldcomputers.net/compaqi.html
